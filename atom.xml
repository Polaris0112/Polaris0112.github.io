<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://yoursite.com/"/>
  <updated>2018-04-02T03:21:56.142Z</updated>
  <id>http://yoursite.com/</id>
  
  <author>
    <name>Jayden</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Docker数据卷之进阶篇</title>
    <link href="http://yoursite.com/2018/03/21/docker-data-volume-improve/"/>
    <id>http://yoursite.com/2018/03/21/docker-data-volume-improve/</id>
    <published>2018-03-20T16:00:00.000Z</published>
    <updated>2018-04-02T03:21:56.142Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是Docker数据卷之进阶篇</p><p>在《浅谈Docker基础之数据管理》[<a href="https://polaris0112.github.io/2018/03/20/docker-data-volume/]一文中介绍了" target="_blank" rel="noopener">https://polaris0112.github.io/2018/03/20/docker-data-volume/]一文中介绍了</a> <code>Docker</code> 数据卷(volume) 的基本用法。随着使用的深入，笔者对 <code>Docker</code> 数据卷的理解与认识也在不断的增强。本文将在前文的基础上介绍 <code>Docker</code> 数据卷的原理及一些高级用法。</p><h2 id="为什么需要数据卷？"><a href="#为什么需要数据卷？" class="headerlink" title="为什么需要数据卷？"></a>为什么需要数据卷？</h2><p>这得从 docker 容器的文件系统说起。出于效率等一系列原因，docker 容器的文件系统在宿主机上存在的方式很复杂，这会带来下面几个问题：</p><ul><li>不能在宿主机上很方便地访问容器中的文件。</li><li>无法在多个容器之间共享数据。</li><li>当容器删除时，容器中产生的数据将丢失。</li></ul><p>为了解决这些问题，docker 引入了数据卷(volume) 机制。数据卷是存在于一个或多个容器中的特定文件或文件夹，这个文件或文件夹以独立于 docker 文件系统的形式存在于宿主机中。数据卷的最大特定是：其生存周期独立于容器的生存周期。</p><p>使用数据卷的最佳场景</p><ul><li>在多个容器之间共享数据，多个容器可以同时以只读或者读写的方式挂载同一个数据卷，从而共享数据卷中的数据。</li><li>当宿主机不能保证一定存在某个目录或一些固定路径的文件时，使用数据卷可以规避这种限制带来的问题。</li><li>当你想把容器中的数据存储在宿主机之外的地方时，比如远程主机上或云存储上。</li><li>当你需要把容器数据在不同的宿主机之间备份、恢复或迁移时，数据卷是很好的选择。</li></ul><h2 id="docker-volume-子命令"><a href="#docker-volume-子命令" class="headerlink" title="docker volume 子命令"></a>docker volume 子命令</h2><p><code>docker</code> 专门提供了 volume 子命令来操作数据卷：<br><code>create</code> 创建数据卷<br><code>inspect</code> 显示数据卷的详细信息<br><code>ls</code> 列出所有的数据卷<br><code>prune</code> 删除所有未使用的 volumes，并且有 -f 选项<br><code>rm</code> 删除一个或多个未使用的 volumes，并且有 -f 选项</p><p>先创建一个名称为 hello 的数据卷并通过 ls 命令进行查看：<br><img src="/images/docker-data-volume-1.png" alt="docker-data-volume-1"></p><p>然后可以使用 inspect 命令看看数据卷 hello 的详细信息：<br><img src="/images/docker-data-volume-2.png" alt="docker-data-volume-2"></p><p>在这里我们可以看到创建数据卷的时间；该数据卷使用的驱动程序为默认的 “local”，表示数据卷使用宿主机的本地存储；数据卷的挂载点，默认是本机 /var/lib/docker/volumes 下的一个目录。<br>最后我们可以使用 rm 或 prune 命令删除数据卷，后面笔者会介绍一些实际使用中与数据卷的删除有关的一些实践。</p><h2 id="使用-mount-语法挂载数据卷"><a href="#使用-mount-语法挂载数据卷" class="headerlink" title="使用 mount 语法挂载数据卷"></a>使用 mount 语法挂载数据卷</h2><p>之前我们使用 –volume(-v) 选项来挂载数据卷，现在 docker 提供了更强大的 –mount 选项来管理数据卷。mount 选项可以通过逗号分隔的多个键值对一次提供多个配置项，因此 mount 选项可以提供比 volume 选项更详细的配置。使用 mount 选项的常用配置如下：</p><p><code>type</code> 指定挂载方式，我们这里用到的是 volume，其实还可以有 bind 和 tmpfs。<br><code>volume-driver</code> 指定挂载数据卷的驱动程序，默认值是 local。<br><code>source</code> 指定挂载的源，对于一个命名的数据卷，这里应该指定这个数据卷的名称。在使用时可以写 source，也可以简写为 src。<br><code>destination</code> 指定挂载的数据在容器中的路径。在使用时可以写 destination，也可以简写为 dst 或 target。<br><code>readonly</code> 指定挂载的数据为只读。<br><code>volume-opt</code> 可以指定多次，用来提高更多的 mount 相关的配置。</p><p>下面我们看个具体的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume create hello</span><br><span class="line">$ docker run -id --mount <span class="built_in">type</span>=volume,<span class="built_in">source</span>=hello,target=/world ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>我们创建了名称为 hello 的数据卷，然后把它挂在到容器中的 /world 目录。通过 inspect 命令查看容器的详情中的 “Mounts” 信息可以验证实际的数据卷挂载结果 ：<br><img src="/images/docker-data-volume-3.png" alt="docker-data-volume-3"></p><h2 id="使用-volume-driver-把数据存储到其它地方"><a href="#使用-volume-driver-把数据存储到其它地方" class="headerlink" title="使用 volume driver 把数据存储到其它地方"></a>使用 volume driver 把数据存储到其它地方</h2><p>除了默认的把数据卷中的数据存储在宿主机，docker 还允许我们通过指定 volume driver 的方式把数据卷中的数据存储在其它的地方，比如 Azrue Storge 或 AWS 的 S3。<br>简单起见，我们接下来的 demo 演示如何通过 vieux/sshfs 驱动把数据卷的存储在其它的主机上。<br>docker 默认是不安装 vieux/sshfs 插件的，我们可以通过下面的命令进行安装：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker plugin install --grant-all-permissions vieux/sshfs</span><br></pre></td></tr></table></figure><p>然后通过 vieux/sshfs 驱动创建数据卷，并指定远程主机的登录用户名、密码和数据存放目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume create --driver vieux/sshfs \</span><br><span class="line">    -o sshcmd=nick@10.32.2.134:/home/nick/sshvolume \</span><br><span class="line">    -o password=yourpassword \</span><br><span class="line">    mysshvolume</span><br></pre></td></tr></table></figure><p>注意，请确保你指定的远程主机上的挂载点目录是存在的(demo 中是 /home/nick/sshvolume 目录)，否则在启动容器时会报错。<br>最后在启动容器时指定挂载这个数据卷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -id \</span><br><span class="line">    --name testcon \</span><br><span class="line">    --mount <span class="built_in">type</span>=volume,volume-driver=vieux/sshfs,<span class="built_in">source</span>=mysshvolume,target=/world \</span><br><span class="line">    ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>这就搞定了，你在容器中 /world 目录下操作的文件都存储在远程主机的 /home/nick/sshvolume 目录中。进入容器 testcon 然后在 /world 目录中创建一个文件，然后打开远程主机的  /home/nick/sshvolume 目录进行查看，你新建的文件是不是已经出现在那里了！</p><h2 id="数据卷原理"><a href="#数据卷原理" class="headerlink" title="数据卷原理"></a>数据卷原理</h2><p>下图描述了 docker 容器挂载数据的三种方式：<br><img src="/images/docker-data-volume-4.png" alt="docker-data-volume-4"></p><p>数据卷是完全被 docker 管理的，就像上图中的黄色区域描述的一样，docker 在宿主机的文件系统中找了个文件管理数据卷相关的数据。因此你可能根本不需要知道数据卷文件在宿主机上的存储位置(事实上抱着刨根问底的精神我们还是很想搞清楚它背后的工作原理！)。</p><p>docker 数据卷的本质是容器中的一个特殊目录。在容器创建的过程中，docker 会将宿主机上的指定目录(一个以数据卷 ID 为名称的目录)挂载到容器中指定的目录上。这里使用的挂载方式为绑定挂载(bind mount)，所以挂载完成后的宿主机目录和容器内的目标目录表现一致。<br>比如我们执行下面的命令创建数据卷 hello，并挂载到容器 testcon 的 /world 目录：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker volume create hello</span><br><span class="line">$ docker run -id --name testcon --mount <span class="built_in">type</span>=volume,<span class="built_in">source</span>=hello,target=/world ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>实际上在容器的创建过程中，类似于在容器中执行了下面的代码：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">// 将数据卷 hello 在宿主机上的目录绑定挂载到 rootfs 中指定的挂载点 /world 上</span><br><span class="line">mount(<span class="string">"/var/lib/docker/volumes/hello/_data"</span>, <span class="string">"rootfs/world"</span>, <span class="string">"none"</span>, MS_BIND, NULL)</span><br></pre></td></tr></table></figure><p>在处理完所有的 mount 操作之后(真正需要 docker 容器挂载的除了数据卷目录还包括 rootfs，init-layer 里的内容，/proc 设备等)，docker 只需要通过 chdir 和 pivot_root 切换进程的根目录到 rootfs 中，这样容器内部进程就只能看见以 rootfs 为根的文件系统以及被 mount 到 rootfs 之下的各项目录了。例如我们启动的 testcon 中的文件系统为：</p><p><img src="/images/docker-data-volume-5.png" alt="docker-data-volume-5"></p><p>下面我们介绍几个数据卷在使用中比较常见的问题。</p><h2 id="数据的覆盖问题"><a href="#数据的覆盖问题" class="headerlink" title="数据的覆盖问题"></a>数据的覆盖问题</h2><ul><li><p>如果挂载一个空的数据卷到容器中的一个非空目录中，那么这个目录下的文件会被复制到数据卷中。</p></li><li><p>如果挂载一个非空的数据卷到容器中的一个目录中，那么容器中的目录中会显示数据卷中的数据。如果原来容器中的目录中有数据，那么这些原始数据会被隐藏掉。</p></li></ul><p>这两个规则都非常重要，灵活利用第一个规则可以帮助我们初始化数据卷中的内容。掌握第二个规则可以保证挂载数据卷后的数据总是你期望的结果。</p><h2 id="在-Dockerfile-中添加数据卷"><a href="#在-Dockerfile-中添加数据卷" class="headerlink" title="在 Dockerfile 中添加数据卷"></a>在 Dockerfile 中添加数据卷</h2><p>在 Dockerfile 中我们可以使用 VOLUME 指令向容器添加数据卷：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">VOLUME /data</span><br></pre></td></tr></table></figure></p><p>在使用 docker build 命令生成镜像并且以该镜像启动容器时会挂载一个数据卷到 /data 目录。根据我们已知的数据覆盖规则，如果镜像中存在 /data 目录，这个目录中的内容将全部被复制到宿主机中对应的目录中，并且根据容器中的文件设置合适的权限和所有者。<br><strong>注意，VOLUME 指令不能挂载主机中指定的目录。这是为了保证 Dockerfile 的可一致性，因为不能保证所有的宿主机都有对应的目录。</strong><br>在实际的使用中，这里还有一个陷阱需要大家注意：在 Dockerfile 中使用 VOLUME 指令之后的代码，如果尝试对这个数据卷进行修改，这些修改都<strong>不会生效</strong>！下面是一个这样的例子：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd nick</span><br><span class="line">VOLUME /data</span><br><span class="line">RUN touch /data/test.txt</span><br><span class="line">RUN chown -R nick:nick /data</span><br></pre></td></tr></table></figure><p>通过这个 Dockerfile 创建镜像并启动容器后，该容器中存在用户 nick，并且能够看到 /data 目录挂载的数据卷。但是 /data 目录内并没有文件 test.txt，更别说 test.txt 文件的所有者属性了。要解释这个现象需要我们了解通过 Dockerfile 创建镜像的过程：<br>Dockerfile 中除了 FROM 指令的每一行都是基于上一行生成的临时镜像运行一个容器，执行一条指令并执行类似 docker commit 的命令得到一个新的镜像。这条类似 docker commit 的命令不会对挂载的数据卷进行保存。<br>所以上面的 Dockerfile 最后两行执行时，都会在一个临时的容器上挂载 /data，并对这个临时的数据卷进行操作，但是这一行指令执行并提交后，这个临时的数据卷并没有被保存。因而我们最终通过镜像创建的容器所挂载的数据卷是没有被最后两条指令操作过的。我们姑且叫它 “Dockerfile 中数据卷的初始化问题”。</p><p>下面的写法可以解决 Dockerfile 中数据卷的初始化问题：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd nick</span><br><span class="line">RUN mkdir /data &amp;&amp; touch /data/test.txt</span><br><span class="line">RUN chown -R nick:nick /data</span><br><span class="line">VOLUME /data</span><br></pre></td></tr></table></figure><p>通过这个 Dockerfile 创建镜像并启动容器后，数据卷的初始化是符合预期的。这是由于在挂载数据卷时，/data 已经存在，/data 中的文件以及它们的权限和所有者设置会被复制到数据卷中。<br>还有另外一种方法可以解决 Dockerfile 中数据卷的初始化问题。就是利用 CMD 指令和 ENTRYPOINT 指令的执行特点：与 RUN 指令在镜像构建过程中执行不同，CMD 指令和 ENTRYPOINT 指令是在容器启动时执行。因此使用下面的 Dockerfile 也可以达到对数据卷的初始化目的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu</span><br><span class="line">RUN useradd nick</span><br><span class="line">VOLUME /data</span><br><span class="line">CMD touch /data/test.txt &amp;&amp; chown -R nick:nick /data &amp;&amp; /bin/bash</span><br></pre></td></tr></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>数据卷解决了用户数据的持久化问题，能够让用户在容器中产生的数据超出容器自身的生命周期。因此对于容器技术来说掌握数据卷的使用非常必要。希望本文能够帮助您理解数据卷相关的内容。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是Docker数据卷之进阶篇&lt;/p&gt;
&lt;p&gt;在《浅谈Docker基础之数据管理》[&lt;a href=&quot;https://polaris0112.github.io/2018/03/20/docker-data-volume/]一文中介绍了&quot; target=&quot;_bla
      
    
    </summary>
    
      <category term="Docker数据卷" scheme="http://yoursite.com/categories/Docker%E6%95%B0%E6%8D%AE%E5%8D%B7/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>浅谈Docker基础之数据管理</title>
    <link href="http://yoursite.com/2018/03/20/docker-data-volume/"/>
    <id>http://yoursite.com/2018/03/20/docker-data-volume/</id>
    <published>2018-03-19T16:00:00.000Z</published>
    <updated>2018-04-02T03:22:10.556Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是浅谈Docker基础之数据管理</p><p>用户在使用 Docker 的过程中，往往需要能查看容器内应用产生的数据，或者需要把容器内的数据进行备份，甚至多个容器之间进行数据的共享，这必然涉及容器的数据管理操作。容器中管理数据主要有两种方式：数据卷(Data Volumes)，数据卷容器(Data Volume Containers)。</p><h2 id="数据卷"><a href="#数据卷" class="headerlink" title="数据卷"></a>数据卷</h2><p>数据卷是一个可供容器使用的特殊目录，它绕过文件系统，可以提供很多有用的特性：</p><p>1.数据卷可以在容器之间共享和重用。</p><p>2.对数据卷的更改会立即生效。</p><p>3.对数据卷的更新不会影响镜像。</p><p>4.数据卷会一直存在，直到没有容器使用。</p><p>数据卷的使用，类似于 linux 下对目录或文件进行 mount 操作。</p><h2 id="在容器内创建一个数据卷"><a href="#在容器内创建一个数据卷" class="headerlink" title="在容器内创建一个数据卷"></a>在容器内创建一个数据卷</h2><p>在用 docker run 命令的时候，使用 -v 标记可以在容器内创建一个数据卷。多次使用 -v 标记可以创建多个数据卷。</p><p>下面的例子中我们使用 myimg/webapp 镜像创建一个 web 容器，并创建一个数据卷挂载到容器的 /webdata 目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -d -P --name web -v /webdata myimg/webapp python app.py</span><br></pre></td></tr></table></figure><h2 id="挂载一个主机目录作为数据卷"><a href="#挂载一个主机目录作为数据卷" class="headerlink" title="挂载一个主机目录作为数据卷"></a>挂载一个主机目录作为数据卷</h2><p>使用 -v 标记也可以指定挂载一个本地的已有目录到容器中去作为数据卷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -d -P --name web -v /var/data:/opt/webdata myimg/webapp python app.py</span><br></pre></td></tr></table></figure><p>上面的命令挂载主机的 /var/data 目录到容器的 /opt/webdata 目录。</p><p>这个功能在接下测试的时候特别方便，比如用户可以放置一些程序或数据到本地目录中，然后在容器中使用。另外，本地目录的路径必须是绝对路径，如果目录不存在，Docker 会自动创建。<br>Docker 挂载数据卷的默认权限是可读写(rw)，用户也可以通过 ro 标记指定为只读：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -d -P --name web -v /var/data:/opt/webdata:ro myimg/webapp python app.py</span><br></pre></td></tr></table></figure><p>加了 :ro 之后，容器内挂载的数据卷内的数据就变成只读的了。</p><h2 id="挂载一个本地主机文件作为数据卷"><a href="#挂载一个本地主机文件作为数据卷" class="headerlink" title="挂载一个本地主机文件作为数据卷"></a>挂载一个本地主机文件作为数据卷</h2><p>-v 标记也可以挂载一个主机中的文件到容器中作为数据卷，但是这样做会带来一些问题。建议还是挂载文件所在的目录。</p><h2 id="数据卷容器"><a href="#数据卷容器" class="headerlink" title="数据卷容器"></a>数据卷容器</h2><p>如果用户需要在容器之间共享一些持续更新的数据，最简单的方式是使用数据卷容器。数据卷容器其实就是一个普通的容器，专门用它提供数据卷供其他容器挂载。下面简单介绍其使用方法。</p><p>首先要创建一个数据卷容器 mydata，并在其中创建一个数据卷挂载到 /data 目录。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -it -v /data --name mydata ubuntu</span><br></pre></td></tr></table></figure><p>然后在其他容器中使用 –volumes-from 来挂载 mydata 容器中的数据卷。例如创建两个容器 mycon1 和 mycon2，并从 mydata 容器挂载数据卷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -it --volumes-from mydata --name mycon1 ubuntu</span><br><span class="line">$ sudo docker run -it --volumes-from mydata --name mycon2 ubuntu</span><br></pre></td></tr></table></figure><p>(注意，命令中没有指定数据卷的信息，也就是说新容器中挂载数据卷的目录和源容器中是一样的。)</p><p>此时容器 mycon1 和 mycon2 都挂载同一个数据卷到相同的目录 /data。三个容器任何一个在该目录下写入数据其他容器都能看到。</p><p>可以多次使用 –volumes-from 参数来从多个容器挂载多个数据卷。还可以从其他已经挂载了容器的容器来挂载数据卷。并且使用 –volumes-from 参数所挂载数据卷的容器自身并不需要保持在运行状态。<br>但删除挂载了数据卷的容器时，数据卷并不会被自动删除。如果要删除一个数据卷，必须在删除最后一个还挂载着它的容器时显式的使用 docker rm -v 命令来指定同时删除关联的容器。</p><p>使用数据卷容器可以让用户在容器之间自由的升级和移动数据卷，下面会进行详细的介绍。</p><h2 id="利用数据卷容器迁移数据"><a href="#利用数据卷容器迁移数据" class="headerlink" title="利用数据卷容器迁移数据"></a>利用数据卷容器迁移数据</h2><p>可以利用数据卷容器对其中的数据卷进行备份、恢复，以实现数据的迁移。</p><h3 id="备份"><a href="#备份" class="headerlink" title="备份"></a>备份</h3><p>使用下面的命令来备份 mydata 数据卷容器内的数据卷：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --volumes-from mydata -v $(<span class="built_in">pwd</span>):/backup --name worker ubuntu tar cvf /backup/backup.tar /data</span><br></pre></td></tr></table></figure><p>这个命令首先利用 Ubuntu 镜像创建了一个容器 worker。又使用 –volumes-from mydata 参数来让 worker 容器挂载 mydata 容器的数据卷。接下来使用 -v $(pwd):/backup 参数来挂载本地的当前目录到 worker 容器的 /backup 目录。</p><p>在 worker 容器启动后，使用了 tar cvf /backup/backup.tar /data 命令来将 /data 下内容备份为容器内的 /backup/backup.tar，即宿主主机的当前目录下的backup.tar。</p><h3 id="恢复"><a href="#恢复" class="headerlink" title="恢复"></a>恢复</h3><p>如果要恢复数据到一个容器，可以按照下面的操作。首先创建一个带有数据卷的容器 mydata2:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run -v /data –name mydata2 ubuntu /bin/bash</span><br></pre></td></tr></table></figure><p>然后创建另一个新的容器，挂载 mydata2 的数据卷，并使用 tar 解压缩备份文件到所挂载的容器卷中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo docker run --volumes-from mydata2 -v $(<span class="built_in">pwd</span>):/backup busybox tar xvf /backup/backup.tar</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是浅谈Docker基础之数据管理&lt;/p&gt;
&lt;p&gt;用户在使用 Docker 的过程中，往往需要能查看容器内应用产生的数据，或者需要把容器内的数据进行备份，甚至多个容器之间进行数据的共享，这必然涉及容器的数据管理操作。容器中管理数据主要有两种方式：数据卷(Data 
      
    
    </summary>
    
      <category term="Docker数据卷" scheme="http://yoursite.com/categories/Docker%E6%95%B0%E6%8D%AE%E5%8D%B7/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker容器网络通信原理分析</title>
    <link href="http://yoursite.com/2018/03/19/docker-network/"/>
    <id>http://yoursite.com/2018/03/19/docker-network/</id>
    <published>2018-03-18T16:00:00.000Z</published>
    <updated>2018-03-30T10:22:57.345Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是Docker容器网络通信原理分析</p><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>自从docker容器出现以来，容器的网络通信就一直是大家关注的焦点，也是生产环境的迫切需求。而容器的网络通信又可以分为两大方面：单主机容器上的相互通信和跨主机的容器相互通信。而本文将分别针对这两方面，对容器的通信原理进行简单的分析，帮助大家更好地使用docker。</p><h2 id="docker单主机容器通信"><a href="#docker单主机容器通信" class="headerlink" title="docker单主机容器通信"></a>docker单主机容器通信</h2><p>基于对net namespace的控制，docker可以为在容器创建隔离的网络环境，在隔离的网络环境下，容器具有完全独立的网络栈，与宿主机隔离，也可以使容器共享主机或者其他容器的网络命名空间，基本可以满足开发者在各种场景下的需要。按docker官方的说法，docker容器的网络有五种模式：</p><ul><li><p><code>bridge</code>：docker默认的网络模式，为容器创建独立的网络命名空间，容器具有独立的网卡等所有单独的网络栈，是最常用的使用方式。</p></li><li><p><code>host</code>：直接使用容器宿主机的网络命名空间。</p></li><li><p><code>none</code>：为容器创建独立网络命名空间，但不为它做任何网络配置，容器中只有lo，用户可以在此基础上，对容器网络做任意定制。</p></li><li><p>其他容器：与host模式类似，只是容器将与指定的容器共享网络命名空间。</p></li><li><p>用户自定义：docker 1.9版本以后新增的特性，允许容器使用第三方的网络实现或者创建单独的bridge网络，提供网络隔离能力。</p></li></ul><p>这些网络模式在相互网络通信方面的对比如下所示：</p><table><thead><tr><th>模式</th><th style="text-align:center">是否支持多主机</th><th style="text-align:center">南北向通信机制</th><th style="text-align:center">东西向通信机制</th></tr></thead><tbody><tr><td>bridge</td><td style="text-align:center">否</td><td style="text-align:center">宿主机端口绑定</td><td style="text-align:center">通过Linux bridge</td></tr><tr><td>host</td><td style="text-align:center">是</td><td style="text-align:center">按宿主机网络通信</td><td style="text-align:center">按宿主机网络通信</td></tr><tr><td>none</td><td style="text-align:center">否</td><td style="text-align:center">无法通信</td><td style="text-align:center">只能用link通信</td></tr><tr><td>其他容器</td><td style="text-align:center">否</td><td style="text-align:center">宿主机端口绑定</td><td style="text-align:center">通过link通信</td></tr><tr><td>用户自定义</td><td style="text-align:center">按网络实现而定</td><td style="text-align:center">按网络实现而定</td><td style="text-align:center">按网络实现而定</td></tr></tbody></table><p>南北向通信指容器与宿主机外界的访问机制，东西向流量指同一宿主机上与其他容器相互访问的机制。</p><h3 id="host模式"><a href="#host模式" class="headerlink" title="host模式"></a>host模式</h3><p>由于容器和宿主机共享同一个网络命名空间，换言之，容器的IP地址即为宿主机的IP地址。所以容器可以和宿主机一样，使用宿主机的任意网卡，实现和外界的通信。其网络模型可以参照下图：</p><p><img src="/images/docker-network-host.jpg" alt="docker-network-host"></p><p>采用host模式的容器，可以直接使用宿主机的IP地址与外界进行通信，若宿主机具有公有IP，那么容器也拥有这个公有IP。同时容器内服务的端口也可以使用宿主机的端口，无需额外进行NAT转换，而且由于容器通信时，不再需要通过linuxbridge等方式转发或者数据包的拆封，性能上有很大优势。当然，这种模式有优势，也就有劣势，主要包括以下几个方面：</p><ul><li><p>最明显的就是容器不再拥有隔离、独立的网络栈。容器会与宿主机竞争网络栈的使用，并且容器的崩溃就可能导致宿主机崩溃，在生产环境中，这种问题可能是不被允许的。</p></li><li><p>容器内部将不再拥有所有的端口资源，因为一些端口已经被宿主机服务、bridge模式的容器端口绑定等其他服务占用掉了。</p></li></ul><h3 id="bridge模式"><a href="#bridge模式" class="headerlink" title="bridge模式"></a>bridge模式</h3><p>bridge模式是docker默认的，也是开发者最常使用的网络模式。在这种模式下，docker为容器创建独立的网络栈，保证容器内的进程使用独立的网络环境，实现容器之间、容器与宿主机之间的网络栈隔离。同时，通过宿主机上的docker0网桥，容器可以与宿主机乃至外界进行网络通信。其网络模型可以参考下图：</p><p><img src="/images/docker-network-bridge.jpg" alt="docker-network-bridge"></p><p>从该网络模型可以看出，容器从原理上是可以与宿主机乃至外界的其他机器通信的。同一宿主机上，容器之间都是连接到docker0这个网桥上的，它可以作为虚拟交换机使容器可以相互通信。然而，由于宿主机的IP地址与容器veth pair的 IP地址均不在同一个网段，故仅仅依靠veth pair和namespace的技术，还不足以使宿主机以外的网络主动发现容器的存在。为了使外界可以方位容器中的进程，docker采用了端口绑定的方式，也就是通过iptables的NAT，将宿主机上的端口端口流量转发到容器内的端口上。</p><p>举一个简单的例子，使用下面的命令创建容器，并将宿主机的3306端口绑定到容器的3306端口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -tid –name db -p 3306:3306 MySQL</span><br></pre></td></tr></table></figure><p>在宿主机上，可以通过iptables -t nat -L -n，查到一条DNAT规则：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DNAT tcp — 0.0.0.0/0 0.0.0.0/0 tcp dpt:3306 to:172.17.0.5:3306</span><br></pre></td></tr></table></figure></p><p>上面的172.17.0.5即为bridge模式下，创建的容器IP。</p><p>很明显，bridge模式的容器与外界通信时，必定会占用宿主机上的端口，从而与宿主机竞争端口资源，对宿主机端口的管理会是一个比较大的问题。同时，由于容器与外界通信是基于三层上iptables NAT，性能和效率上的损耗是可以预见的。</p><h3 id="none模式"><a href="#none模式" class="headerlink" title="none模式"></a>none模式</h3><p>在这种模式下，容器有独立的网络栈，但不包含任何网络配置，只具有lo这个loopback网卡用于进程通信。也就是说，none模式为容器做了最少的网络设置，但是俗话说得好“少即是多”，在没有网络配置的情况下，通过第三方工具或者手工的方式，开发这任意定制容器的网络，提供了最高的灵活性。</p><h3 id="其他容器模式"><a href="#其他容器模式" class="headerlink" title="其他容器模式"></a>其他容器模式</h3><p>其他网络模式是docker中一种较为特别的网络的模式。在这个模式下的容器，会使用其他容器的网络命名空间，其网络隔离性会处于bridge桥接模式与host模式之间。当容器共享其他容器的网络命名空间，则在这两个容器之间不存在网络隔离，而她们又与宿主机以及除此之外其他的容器存在网络隔离。其网络模型可以参考下图：</p><p><img src="/images/docker-network-others.jpg" alt="docker-network-others"></p><p>在这种模式下的容器可以通过localhost来同一网络命名空间下的其他容器，传输效率较高。而且这种模式还节约了一定数量的网络资源，但它并没有改变容器与外界通信的方式。在一些特殊的场景中非常有用，例如，kubernetes的pod，kubernetes为pod创建一个基础设施容器，同一pod下的其他容器都以其他容器模式共享这个基础设施容器的网络命名空间，相互之间以localhost访问，构成一个统一的整体。</p><h3 id="用户定义网络模式"><a href="#用户定义网络模式" class="headerlink" title="用户定义网络模式"></a>用户定义网络模式</h3><p>在用户定义网络模式下，开发者可以使用任何docker支持的第三方网络driver来定制容器的网络。并且，docker 1.9以上的版本默认自带了bridge和overlay两种类型的自定义网络driver。可以用于集成calico、weave、openvswitch等第三方厂商的网络实现。</p><p>除了docker自带的bridge driver，其他的几种driver都可以实现容器的跨主机通信。而基于bdrige driver的网络，docker会自动为其创建iptables规则，保证与其他网络之间、与docker0之间的网络隔离。例如，使用下面的命令创建一个基于bridge driver的自定义网络：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker network create bri1</span><br></pre></td></tr></table></figure><p>则docker会自动生成如下的iptables规则，保证不同网络上的容器无法互相通信。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">-A DOCKER-ISOLATION -i br-8dba6df70456 -o docker0 -j DROP</span><br><span class="line">-A DOCKER-ISOLATION -i docker0 -o br-8dba6df70456 -j DROP</span><br></pre></td></tr></table></figure></p><p>除此之外，bridge driver的所有行为都和默认的bridge模式完全一致。而overlay及其他driver，则可以实现容器的跨主机通信。</p><h2 id="docker跨主机容器通信"><a href="#docker跨主机容器通信" class="headerlink" title="docker跨主机容器通信"></a>docker跨主机容器通信</h2><p>早期大家的跨主机通信方案主要有以下几种：</p><ul><li><p>容器使用host模式：容器直接使用宿主机的网络，这样天生就可以支持跨主机通信。虽然可以解决跨主机通信问题，但这种方式应用场景很有限，容易出现端口冲突，也无法做到隔离网络环境，一个容器崩溃很可能引起整个宿主机的崩溃。</p></li><li><p>端口绑定：通过绑定容器端口到宿主机端口，跨主机通信时，使用主机IP+端口的方式访问容器中的服务。显而易见，这种方式仅能支持网络栈的四层及以上的应用，并且容器与宿主机紧耦合，很难灵活的处理，可扩展性不佳。</p></li><li><p>docker外定制容器网络：在容器通过docker创建完成后，然后再通过修改容器的网络命名空间来定义容器网络。典型的就是很久以前的pipework，容器以none模式创建，pipework通过进入容器的网络命名空间为容器重新配置网络，这样容器网络可以是静态IP、vxlan网络等各种方式，非常灵活，容器启动的一段时间内会没有IP，明显无法在大规模场景下使用，只能在实验室中测试使用。</p></li><li><p>第三方SDN定义容器网络：使用Open vSwitch或Flannel等第三方SDN工具，为容器构建可以跨主机通信的网络环境。这些方案一般要求各个主机上的docker0网桥的cidr不同，以避免出现IP冲突的问题，限制了容器在宿主机上的可获取IP范围。并且在容器需要对集群外提供服务时，需要比较复杂的配置，对部署实施人员的网络技能要求比较高。</p></li></ul><p>上面这些方案有各种各样的缺陷，同时也因为跨主机通信的迫切需求，docker 1.9版本时，官方提出了基于vxlan的overlay网络实现，原生支持容器的跨主机通信。同时，还支持通过libnetwork的plugin机制扩展各种第三方实现，从而以不同的方式实现跨主机通信。就目前社区比较流行的方案来说，跨主机通信的基本实现方案有以下几种：</p><ul><li><p>基于隧道的overlay网络：按隧道类型来说，不同的公司或者组织有不同的实现方案。docker原生的overlay网络就是基于vxlan隧道实现的。ovn则需要通过geneve或者stt隧道来实现的。flannel最新版本也开始默认基于vxlan实现overlay网络。</p></li><li><p>基于包封装的overlay网络：基于UDP封装等数据包包装方式，在docker集群上实现跨主机网络。典型实现方案有weave、flannel的早期版本。</p></li><li><p>基于三层实现SDN网络：基于三层协议和路由，直接在三层上实现跨主机网络，并且通过iptables实现网络的安全隔离。典型的方案为Project Calico。同时对不支持三层路由的环境，Project Calico还提供了基于IPIP封装的跨主机网络实现。</p></li></ul><p>下面，本从网络通信模型的角度，对这些方案的通信原理做一个简单的比较，从中可以窥见各种方案在性能上的本质差别。</p><h2 id="docker容器的CNM模型"><a href="#docker容器的CNM模型" class="headerlink" title="docker容器的CNM模型"></a>docker容器的CNM模型</h2><p>首先，科普下docker容器的CNM网络模型，calico、weave等第三方实现都是基于CNM模型与docker集成的。CNM网络模型的结构如下图所示：</p><p><img src="/images/docker-cnm-model.jpg" alt="docker-cnm-model"></p><p>在上面的图中：</p><ul><li><p>Sandbox代表容器的网络命名空间，包含了容器的完整网络栈，不同的容器之间可以完全隔离。在宿主机上，就表现为独立的网络命名空间。</p></li><li><p>Endpoint代表容器接入网络的端点，可以形象地认为一个Endpoint对容器来说，就是一张物理网卡。</p></li><li><p>Network代表一组可以直接相互通信的Endpoint集合，可以基于LinuxBridge或者VLAN实现。在宿主机上，每个网络都是一个独立的网络命名空间，宿主机上同一网络的的容器，都通过veth pair链接到这个网络命名空间上。</p></li></ul><h2 id="docker原生overlay的网络通信模型"><a href="#docker原生overlay的网络通信模型" class="headerlink" title="docker原生overlay的网络通信模型"></a>docker原生overlay的网络通信模型</h2><p>docker官方文档的示例中，overlay网络是在swarm集群中配置的，但实际上，overlay网络可以独立于swarm集群实现，只需要满足以下前提条件即可。</p><ul><li><p>有consul或者etcd，zookeeper的集群key-value存储服务；</p></li><li><p>组成集群的所有主机的主机名不允许重复，因为docker守护进程与consul通信时，以主机名相互区分</p></li><li><p>所有主机都可以访问集群key-value的服务端口，按具体类型需要打开进行配置。例如docker daemon启动时增加参数<br>–cluster-store=etcd://<etcd-ip>:4001 – -cluster-advertise=eth0:2376</etcd-ip></p></li><li><p>overlay网络依赖宿主机三层网络的组播实现，需要在所有宿主机的防火墙上打开下列端口</p></li></ul><table><thead><tr><th>协议</th><th>端口</th><th>说 明</th></tr></thead><tbody><tr><td>udp</td><td>4789</td><td>容器之间流量的vxlan端口</td></tr><tr><td>tcp/udp</td><td>7946</td><td>docker守护进程的控制端口</td></tr></tbody></table><p>宿主机内核版本10以上（1.9版本时，要求3.16以上）</p><p>满足以上条件后，就可以通过docker network命令来创建跨主机的overlay网络了，例如： docker network create -d overlay overlaynet 在集群的不同主机上，使用overlaynet这个网络创建容器，形成如下图所示的网络拓扑：</p><p><img src="/images/docker-network-overlay.jpg" alt="docker-network-overlay"></p><p>由于容器和overlay的网络的网络命名空间文件不再操作系统默认的<code>/var/run/netns</code>下，只能手动通过软连接的方式查看。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ln -s /var/run/docker/netns /var/run/netns</span><br></pre></td></tr></table></figure></p><p>这样就可以通过ip netns查看到容器和网络的网络命名空间了。</p><p>容器的网络命名空间名称可以通过 <code>docker inspect -f &lt;NetworkSettings.SandboxKey&gt; &lt;容器ID&gt;</code> 方式查看到。网络的网络命名空间则是通过docker network ls查看到的网络短ID。</p><p>有时候网络的网络命名空间名称前面会带上1-、2-等序号，有时候不带。但不影响网络的通信和操作。</p><p>从这个通信过程中来看，跨主机通信过程中的步骤如下：</p><ul><li>容器的网络命名空间与overlay网络的网络命名空间通过一对veth pair连接起来，当容器对外通信时，veth pair起到网线的作用，将流量发送到overlay网络的网络命名空间中。</li><li>容器的veth pair对端eth2与vxlan设备通过br0这个Linux bridge桥接在一起，br0在同一宿主机上起到虚拟机交换机的作用，如果目标地址在同一宿主机上，则直接通信，如果不再则通过设置在vxlan1这个vxlan设备进行跨主机通信。</li><li>vxlan1设备上会在创建时，由docker daemon为其分配vxlan隧道ID，起到网络隔离的作用。</li><li>docker主机集群通过key/value存储共享数据，在7946端口上，相互之间通过gossip协议学习各个宿主机上运行了哪些容器。守护进程根据这些数据来在vxlan1设备上生成静态MAC转发表。</li><li>根据静态MAC转发表的设置，通过UDP端口4789，将流量转发到对端宿主机的网卡上。</li><li>根据流量包中的vxlan隧道ID，将流量转发到对端宿主机的overlay网络的网络命名空间中。</li><li>对端宿主机的overlay网络的网络命名空间中br0网桥，起到虚拟交换机的作用，将流量根据MAC地址转发到对应容器内部。</li></ul><p>虽然上面的网络通信模型可以实现容器的跨主机通信，但还是有一些缺陷，造成实际使用上的不便，例如：</p><ul><li>由于vxlan网络与宿主机网络默认不再同一网络环境下，为了解决宿主机与容器的通信问题，docker为overlay网络中的容器额外增加了网卡eth1作为宿主机与容器通信的通道。这样在使用容器服务时，就必须根据访问性质的不同，选择不同的网卡地址，造成使用上的不便。</li><li>容器对外暴露服务仍然只能使用端口绑定的方式，外界无法简单地直接使用容器IP访问容器服务。</li><li>从上面的通信过程中来看，原生的overlay网络通信必须依赖docker守护进程及key/value存储来实现网络通信，约束较多，容器在启动后的一段时间内可能无法跨主机通信，这对一些比较敏感的应用来说是不可靠的。</li></ul><h3 id="weave网络通信模型"><a href="#weave网络通信模型" class="headerlink" title="weave网络通信模型"></a>weave网络通信模型</h3><p>weave通过在docker集群的每个主机上启动虚拟的路由器，将主机作为路由器，形成互联互通的网络拓扑，在此基础上，实现容器的跨主机通信。其主机网络拓扑参见下图：</p><p><img src="/images/docker-network-weave.jpg" alt="docker-network-weave"></p><p>如上图所示，在每一个部署Docker的主机（可能是物理机也可能是虚拟机）上都部署有一个W（即weave router，它本身也可以以一个容器的形式部署）。weave网络是由这些weave routers组成的对等端点（peer）构成，并且可以通过weave命令行定制网络拓扑。</p><p>每个部署了weave router的主机之间都会建立TCP和UDP两个连接，保证weave router之间控制面流量和数据面流量的通过。控制面由weave routers之间建立的TCP连接构成，通过它进行握手和拓扑关系信息的交换通信。控制面的通信可以被配置为加密通信。而数据面由weave routers之间建立的UDP连接构成，这些连接大部分都会加密。这些连接都是全双工的，并且可以穿越防火墙。</p><p>当容器通过weave进行跨主机通信时，其网络通信模型可以参考下图：</p><p><img src="/images/docker-network-weave-model.jpg" alt="docker-network-weave-model"></p><p>从上面的网络模型图中可以看出，对每一个weave网络中的容器，weave都会创建一个网桥，并且在网桥和每个容器之间创建一个veth pair，一端作为容器网卡加入到容器的网络命名空间中，并为容器网卡配置ip和相应的掩码，一端连接在网桥上，最终通过宿主机上weave router将流量转发到对端主机上。其基本过程如下：</p><ul><li>容器流量通过veth pair到达宿主机上weave router网桥上。</li><li>weave router在混杂模式下使用pcap在网桥上截获网络数据包，并排除由内核直接通过网桥转发的数据流量，例如本子网内部、本地容器之间的数据以及宿主机和本地容器之间的流量。捕获的包通过UDP转发到所其他主机的weave router端。</li><li>在接收端，weave router通过pcap将包注入到网桥上的接口，通过网桥的上的veth pair，将流量分发到容器的网卡上。</li></ul><p>weave默认基于UDP承载容器之间的数据包，并且可以完全自定义整个集群的网络拓扑，但从性能和使用角度来看，还是有比较大的缺陷的：</p><ul><li>weave自定义容器数据包的封包解包方式，不够通用，传输效率比较低，性能上的损失也比较大。</li><li>集群配置比较负载，需要通过weave命令行来手工构建网络拓扑，在大规模集群的情况下，加重了管理员的负担。</li></ul><h3 id="calico网络通信模型"><a href="#calico网络通信模型" class="headerlink" title="calico网络通信模型"></a>calico网络通信模型</h3><p>calico是纯三层的SDN 实现，它基于BPG 协议和Linux自身的路由转发机制，不依赖特殊硬件，容器通信也不依赖iptables NAT或Tunnel 等技术。能够方便的部署在物理服务器、虚拟机（如 OpenStack）或者容器环境下。同时calico自带的基于iptables的ACL管理组件非常灵活，能够满足比较复杂的安全隔离需求。</p><p>在主机网络拓扑的组织上，calico的理念与weave类似，都是在主机上启动虚拟机路由器，将每个主机作为路由器使用，组成互联互通的网络拓扑。当安装了calico的主机组成集群后，其拓扑如下图所示：</p><p><img src="/images/docker-network-calico.jpg" alt="docker-network-calico"></p><p>每个主机上都部署了calico/node作为虚拟路由器，并且可以通过calico将宿主机组织成任意的拓扑集群。当集群中的容器需要与外界通信时，就可以通过BGP协议将网关物理路由器加入到集群中，使外界可以直接访问容器IP，而不需要做任何NAT之类的复杂操作。</p><p>当容器通过calico进行跨主机通信时，其网络通信模型如下图所示：</p><p><img src="/images/docker-network-calico-model.jpg" alt="docker-network-calico-model"></p><p>从上图可以看出，当容器创建时，calico为容器生成veth pair，一端作为容器网卡加入到容器的网络命名空间，并设置IP和掩码，一端直接暴露在宿主机上，并通过设置路由规则，将容器IP暴露到宿主机的通信路由上。于此同时，calico为每个主机分配了一段子网作为容器可分配的IP范围，这样就可以根据子网的CIDR为每个主机生成比较固定的路由规则。</p><p>当容器需要跨主机通信时，主要经过下面的简单步骤：</p><ul><li>容器流量通过veth pair到达宿主机的网络命名空间上。</li><li>根据容器要访问的IP所在的子网CIDR和主机上的路由规则，找到下一跳要到达的宿主机IP。</li><li>流量到达下一跳的宿主机后，根据当前宿主机上的路由规则，直接到达对端容器的veth pair插在宿主机的一端，最终进入容器。</li></ul><p>从上面的通信过程来看，跨主机通信时，整个通信路径完全没有使用NAT或者UDP封装，性能上的损耗确实比较低。但正式由于calico的通信机制是完全基于三层的，这种机制也带来了一些缺陷，例如：</p><ul><li>calico目前只支持TCP、UDP、ICMP、ICMPv6协议，如果使用其他四层协议（例如NetBIOS协议），建议使用weave、原生overlay等其他overlay网络实现。</li><li>基于三层实现通信，在二层上没有任何加密包装，因此只能在私有的可靠网络上使用。</li><li>流量隔离基于iptables实现，并且从etcd中获取需要生成的隔离规则，有一些性能上的隐患。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是Docker容器网络通信原理分析&lt;/p&gt;
&lt;h2 id=&quot;概述&quot;&gt;&lt;a href=&quot;#概述&quot; class=&quot;headerlink&quot; title=&quot;概述&quot;&gt;&lt;/a&gt;概述&lt;/h2&gt;&lt;p&gt;自从docker容器出现以来，容器的网络通信就一直是大家关注的焦点，也是生产环
      
    
    </summary>
    
      <category term="Docker容器网络通信原理分析" scheme="http://yoursite.com/categories/Docker%E5%AE%B9%E5%99%A8%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1%E5%8E%9F%E7%90%86%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Docker核心技术与实现原理</title>
    <link href="http://yoursite.com/2018/03/18/docker-tech/"/>
    <id>http://yoursite.com/2018/03/18/docker-tech/</id>
    <published>2018-03-17T16:00:00.000Z</published>
    <updated>2018-03-30T09:32:08.548Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是Docker 核心技术与实现原理</p><p>提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Docker 已经成为了很多公司的标配，也不再是一个只能在开发阶段使用的玩具了。作为在生产环境中广泛应用的产品，Docker 有着非常成熟的社区以及大量的使用者，代码库中的内容也变得非常庞大。</p><p><img src="/images/docker-logo.png" alt="docker-logo"></p><p>同样，由于项目的发展、功能的拆分以及各种奇怪的改名 PR，让我们再次理解 Docker 的的整体架构变得更加困难。</p><p>虽然 Docker 目前的组件较多，并且实现也非常复杂，但是本文不想过多的介绍 Docker 具体的实现细节，我们更想谈一谈 Docker 这种虚拟化技术的出现有哪些核心技术的支撑。</p><p><img src="/images/docker-core-techs.png" alt="docker-core-techs"></p><p>首先，Docker 的出现一定是因为目前的后端在开发和运维阶段确实需要一种虚拟化技术解决开发环境和生产环境环境一致的问题，通过 Docker 我们可以将程序运行的环境也纳入到版本控制中，排除因为环境造成不同运行结果的可能。但是上述需求虽然推动了虚拟化技术的产生，但是如果没有合适的底层技术支撑，那么我们仍然得不到一个完美的产品。本文剩下的内容会介绍几种 Docker 使用的核心技术，如果我们了解它们的使用方法和原理，就能清楚 Docker 的实现原理。</p><h2 id="Namespaces"><a href="#Namespaces" class="headerlink" title="Namespaces"></a>Namespaces</h2><p>命名空间 (namespaces) 是 Linux 为我们提供的用于分离进程树、网络接口、挂载点以及进程间通信等资源的方法。在日常使用 Linux 或者 macOS 时，我们并没有运行多个完全分离的服务器的需要，但是如果我们在服务器上启动了多个服务，这些服务其实会相互影响的，每一个服务都能看到其他服务的进程，也可以访问宿主机器上的任意文件，这是很多时候我们都不愿意看到的，我们更希望运行在同一台机器上的不同服务能做到<strong>完全隔离</strong>，就像运行在多台不同的机器上一样。</p><p><img src="/images/multiple-servers-on-linux.png" alt="multiple-servers-on-linux"></p><p>在这种情况下，一旦服务器上的某一个服务被入侵，那么入侵者就能够访问当前机器上的所有服务和文件，这也是我们不想看到的，而 <code>Docker</code> 其实就通过 <code>Linux</code> 的 <code>Namespaces</code> 对不同的容器实现了隔离。</p><p>Linux 的命名空间机制提供了以下七种不同的命名空间，包括 <code>CLONE_NEWCGROUP</code>、<code>CLONE_NEWIPC</code>、<code>CLONE_NEWNET</code>、<code>CLONE_NEWNS</code>、<code>CLONE_NEWPID</code>、<code>CLONE_NEWUSER</code> 和 <code>CLONE_NEWUTS</code>，通过这七个选项我们能在创建新的进程时设置新进程应该在哪些资源上与宿主机器进行隔离。</p><h2 id="进程"><a href="#进程" class="headerlink" title="进程"></a>进程</h2><p>进程是 Linux 以及现在操作系统中非常重要的概念，它表示一个正在执行的程序，也是在现代分时系统中的一个任务单元。在每一个 *nix 的操作系统上，我们都能够通过 <code>ps</code> 命令打印出当前操作系统中正在执行的进程，比如在 Ubuntu 上，使用该命令就能得到以下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ ps -ef</span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 Apr08 ?        00:00:09 /sbin/init</span><br><span class="line">root         2     0  0 Apr08 ?        00:00:00 [kthreadd]</span><br><span class="line">root         3     2  0 Apr08 ?        00:00:05 [ksoftirqd/0]</span><br><span class="line">root         5     2  0 Apr08 ?        00:00:00 [kworker/0:0H]</span><br><span class="line">root         7     2  0 Apr08 ?        00:07:10 [rcu_sched]</span><br><span class="line">root        39     2  0 Apr08 ?        00:00:00 [migration/0]</span><br><span class="line">root        40     2  0 Apr08 ?        00:01:54 [watchdog/0]</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>当前机器上有很多的进程正在执行，在上述进程中有两个非常特殊，一个是 <code>pid</code> 为 1 的 <code>/sbin/init</code> 进程，另一个是 <code>pid</code> 为 2 的 <code>kthreadd</code> 进程，这两个进程都是被 <code>Linux</code> 中的上帝进程 <code>idle</code> 创建出来的，其中前者负责执行内核的一部分初始化工作和系统配置，也会创建一些类似 <code>getty</code> 的注册进程，而后者负责管理和调度其他的内核进程。</p><p><img src="/images/linux-processes.png" alt="linux-processes"></p><p>如果我们在当前的 Linux 操作系统下运行一个新的 <code>Docker</code> 容器，并通过 <code>exec</code> 进入其内部的 <code>bash</code> 并打印其中的全部进程，我们会得到以下的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">root@iZ255w13cy6Z:~<span class="comment"># docker run -it -d ubuntu</span></span><br><span class="line">b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79</span><br><span class="line">root@iZ255w13cy6Z:~<span class="comment"># docker exec -it b809a2eb3630 /bin/bash</span></span><br><span class="line">root@b809a2eb3630:/<span class="comment"># ps -ef</span></span><br><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root         1     0  0 15:42 pts/0    00:00:00 /bin/bash</span><br><span class="line">root         9     0  0 15:42 pts/1    00:00:00 /bin/bash</span><br><span class="line">root        17     9  0 15:43 pts/1    00:00:00 ps -ef</span><br></pre></td></tr></table></figure><p>在新的容器内部执行 <code>ps</code> 命令打印出了非常干净的进程列表，只有包含当前 <code>ps -ef</code> 在内的三个进程，在宿主机器上的几十个进程都已经消失不见了。</p><p>当前的 Docker 容器成功将容器内的进程与宿主机器中的进程隔离，如果我们在宿主机器上打印当前的全部进程时，会得到下面三条与 <code>Docker</code> 相关的结果：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UID        PID  PPID  C STIME TTY          TIME CMD</span><br><span class="line">root     29407     1  0 Nov16 ?        00:08:38 /usr/bin/dockerd --raw-logs</span><br><span class="line">root      1554 29407  0 Nov19 ?        00:03:28 docker-containerd -l unix:///var/run/docker/libcontainerd/docker-containerd.sock --metrics-interval=0 --start-timeout 2m --state-dir /var/run/docker/libcontainerd/containerd --shim docker-containerd-shim --runtime docker-runc</span><br><span class="line">root      5006  1554  0 08:38 ?        00:00:00 docker-containerd-shim b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 /var/run/docker/libcontainerd/b809a2eb3630e64c581561b08ac46154878ff1c61c6519848b4a29d412215e79 docker-runc</span><br></pre></td></tr></table></figure><p>在当前的宿主机器上，可能就存在由上述的不同进程构成的进程树：</p><p><img src="/images/docker-process-group.png" alt="docker-process-group"></p><p>这就是在使用 <code>clone(2)</code> 创建新进程时传入 <code>CLONE_NEWPID</code> 实现的，也就是使用 <code>Linux</code> 的命名空间实现进程的隔离，<code>Docker</code> 容器内部的任意进程都对宿主机器的进程一无所知。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">containerRouter.postContainersStart</span><br><span class="line">└── daemon.ContainerStart</span><br><span class="line">    └── daemon.createSpec</span><br><span class="line">        └── setNamespaces</span><br><span class="line">            └── setNamespace</span><br></pre></td></tr></table></figure><p><code>Docker</code> 的容器就是使用上述技术实现与宿主机器的进程隔离，当我们每次运行 <code>docker run</code> 或者 <code>docker start</code> 时，都会在下面的方法中创建一个用于设置进程间隔离的 <code>Spec</code>：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="params">(daemon *Daemon)</span> <span class="title">createSpec</span><span class="params">(c *container.Container)</span> <span class="params">(*specs.Spec, error)</span></span> &#123;</span><br><span class="line">s := oci.DefaultSpec()</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">if</span> err := setNamespaces(daemon, &amp;s, c); err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span>, fmt.Errorf(<span class="string">"linux spec namespaces: %v"</span>, err)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> &amp;s, <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在 <code>setNamespaces</code> 方法中不仅会设置进程相关的命名空间，还会设置与用户、网络、<code>IPC</code> 以及 <code>UTS</code> 相关的命名空间：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">func</span> <span class="title">setNamespaces</span><span class="params">(daemon *Daemon, s *specs.Spec, c *container.Container)</span> <span class="title">error</span></span> &#123;</span><br><span class="line"><span class="comment">// user</span></span><br><span class="line"><span class="comment">// network</span></span><br><span class="line"><span class="comment">// ipc</span></span><br><span class="line"><span class="comment">// uts</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// pid</span></span><br><span class="line"><span class="keyword">if</span> c.HostConfig.PidMode.IsContainer() &#123;</span><br><span class="line">ns := specs.LinuxNamespace&#123;Type: <span class="string">"pid"</span>&#125;</span><br><span class="line">pc, err := daemon.getPidContainer(c)</span><br><span class="line"><span class="keyword">if</span> err != <span class="literal">nil</span> &#123;</span><br><span class="line"><span class="keyword">return</span> err</span><br><span class="line">&#125;</span><br><span class="line">ns.Path = fmt.Sprintf(<span class="string">"/proc/%d/ns/pid"</span>, pc.State.GetPID())</span><br><span class="line">setNamespace(s, ns)</span><br><span class="line">&#125; <span class="keyword">else</span> <span class="keyword">if</span> c.HostConfig.PidMode.IsHost() &#123;</span><br><span class="line">oci.RemoveNamespace(s, specs.LinuxNamespaceType(<span class="string">"pid"</span>))</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">ns := specs.LinuxNamespace&#123;Type: <span class="string">"pid"</span>&#125;</span><br><span class="line">setNamespace(s, ns)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">return</span> <span class="literal">nil</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>所有命名空间相关的设置 <code>Spec</code> 最后都会作为 <code>Create</code> 函数的入参在创建新的容器时进行设置：</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">daemon.containerd.Create(context.Background(), container.ID, spec, createOptions)</span><br></pre></td></tr></table></figure><p>所有与命名空间的相关的设置都是在上述的两个函数中完成的，<code>Docker</code> 通过命名空间成功完成了与宿主机进程和网络的隔离。</p><h2 id="网络"><a href="#网络" class="headerlink" title="网络"></a>网络</h2><p>如果 Docker 的容器通过 Linux 的命名空间完成了与宿主机进程的网络隔离，但是却有没有办法通过宿主机的网络与整个互联网相连，就会产生很多限制，所以 Docker 虽然可以通过命名空间创建一个隔离的网络环境，但是 Docker 中的服务仍然需要与外界相连才能发挥作用。</p><p>每一个使用 <code>docker run</code> 启动的容器其实都具有单独的网络命名空间，<code>Docker</code> 为我们提供了四种不同的网络模式，<code>Host</code>、<code>Container</code>、<code>None</code> 和 <code>Bridge</code> 模式。</p><p><img src="/images/docker-network.png" alt="docker-network"></p><p>在这一部分，我们将介绍 Docker 默认的网络设置模式：网桥模式。在这种模式下，除了分配隔离的网络命名空间之外，Docker 还会为所有的容器设置 IP 地址。当 Docker 服务器在主机上启动之后会创建新的虚拟网桥 docker0，随后在该主机上启动的全部服务在默认情况下都与该网桥相连。</p><p><img src="/images/docker-network-topology.png" alt="docker-network-topology"></p><p>在默认情况下，每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在创建的容器中，会加入到名为 docker0 网桥中。我们可以使用如下的命令来查看当前网桥的接口：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ brctl show</span><br><span class="line">bridge namebridge idSTP enabledinterfaces</span><br><span class="line">docker08000.0242a6654980noveth3e84d4f</span><br><span class="line">                     veth9953b75</span><br></pre></td></tr></table></figure><p>docker0 会为每一个容器分配一个新的 IP 地址并将 docker0 的 IP 地址设置为默认的网关。网桥 docker0 通过 iptables 中的配置与宿主机器上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 docker0 并由网桥分发给对应的机器。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ iptables -t nat -L</span><br><span class="line">Chain PREROUTING (policy ACCEPT)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">DOCKER     all  --  anywhere             anywhere             ADDRTYPE match dst-type LOCAL</span><br><span class="line"></span><br><span class="line">Chain DOCKER (2 references)</span><br><span class="line">target     prot opt <span class="built_in">source</span>               destination</span><br><span class="line">RETURN     all  --  anywhere             anywhere</span><br></pre></td></tr></table></figure><p>我们在当前的机器上使用 <code>docker run -d -p 6379:6379 redis</code> 命令启动了一个新的 <code>Redis</code> 容器，在这之后我们再查看当前 <code>iptables</code> 的 <code>NAT</code> 配置就会看到在 <code>DOCKER</code> 的链中出现了一条新的规则：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DNAT       tcp  --  anywhere             anywhere             tcp dpt:6379 to:192.168.0.4:6379</span><br></pre></td></tr></table></figure><p>上述规则会将从任意源发送到当前机器 6379 端口的 <code>TCP</code> 包转发到 <code>192.168.0.4:6379</code> 所在的地址上。</p><p>这个地址其实也是 <code>Docker</code> 为 <code>Redis</code> 服务分配的 <code>IP</code> 地址，如果我们在当前机器上直接 <code>ping</code> 这个 <code>IP</code> 地址就会发现它是可以访问到的：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ping 192.168.0.4</span><br><span class="line">PING 192.168.0.4 (192.168.0.4) 56(84) bytes of data.</span><br><span class="line">64 bytes from 192.168.0.4: icmp_seq=1 ttl=64 time=0.069 ms</span><br><span class="line">64 bytes from 192.168.0.4: icmp_seq=2 ttl=64 time=0.043 ms</span><br><span class="line"></span><br><span class="line">--- 192.168.0.4 ping statistics ---</span><br><span class="line">2 packets transmitted, 2 received, 0% packet loss, time 999ms</span><br><span class="line">rtt min/avg/max/mdev = 0.043/0.056/0.069/0.013 ms</span><br></pre></td></tr></table></figure><p>从上述的一系列现象，我们就可以推测出 <code>Docker</code> 是如何将容器的内部的端口暴露出来并对数据包进行转发的了；当有 <code>Docker</code> 的容器需要将服务暴露给宿主机器，就会为容器分配一个 <code>IP</code> 地址，同时向 <code>iptables</code> 中追加一条新的规则。</p><p><img src="/images/docker-network-forward.png" alt="docker-network-forward"></p><p>当我们使用 <code>redis-cli</code> 在宿主机器的命令行中访问 <code>127.0.0.1:6379</code> 的地址时，经过 <code>iptables</code> 的 <code>NAT PREROUTING</code> 将 <code>ip</code> 地址定向到了 <code>192.168.0.4</code>，重定向过的数据包就可以通过 <code>iptables</code> 中的 <code>FILTER</code> 配置，最终在 <code>NAT POSTROUTING</code> 阶段将 <code>ip</code> 地址伪装成 <code>127.0.0.1</code>，到这里虽然从外面看起来我们请求的是 <code>127.0.0.1:6379</code>，但是实际上请求的已经是 <code>Docker</code> 容器暴露出的端口了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ redis-cli -h 127.0.0.1 -p 6379 ping</span><br><span class="line">PONG</span><br></pre></td></tr></table></figure><p>Docker 通过 Linux 的命名空间实现了网络的隔离，又通过 iptables 进行数据包转发，让 Docker 容器能够优雅地为宿主机器或者其他容器提供服务。</p><h2 id="libnetwork"><a href="#libnetwork" class="headerlink" title="libnetwork"></a>libnetwork</h2><p>整个网络部分的功能都是通过 Docker 拆分出来的 libnetwork 实现的，它提供了一个连接不同容器的实现，同时也能够为应用给出一个能够提供一致的编程接口和网络层抽象的<strong>容器网络模型</strong>。</p><blockquote><p>The goal of libnetwork is to deliver a robust Container Network Model that provides a consistent programming interface and the required network abstractions for applications.</p></blockquote><p>libnetwork 中最重要的概念，容器网络模型由以下的几个主要组件组成，分别是 Sandbox、Endpoint 和 Network：</p><p><img src="/images/container-network-model.png" alt="container-network-model"></p><p>在容器网络模型中，每一个容器内部都包含一个 Sandbox，其中存储着当前容器的网络栈配置，包括容器的接口、路由表和 DNS 设置，Linux 使用网络命名空间实现这个 Sandbox，每一个 Sandbox 中都可能会有一个或多个 Endpoint，在 Linux 上就是一个虚拟的网卡 veth，Sandbox 通过 Endpoint 加入到对应的网络中，这里的网络可能就是我们在上面提到的 Linux 网桥或者 VLAN。</p><blockquote><p>想要获得更多与 libnetwork 或者容器网络模型相关的信息，可以阅读<a href="https://github.com/docker/libnetwork/blob/master/docs/design.md" target="_blank" rel="noopener">Design · libnetwork</a>了解更多信息，当然也可以阅读源代码了解不同 OS 对容器网络模型的不同实现。</p></blockquote><h2 id="挂载点"><a href="#挂载点" class="headerlink" title="挂载点"></a>挂载点</h2><p>虽然我们已经通过 Linux 的命名空间解决了进程和网络隔离的问题，在 Docker 进程中我们已经没有办法访问宿主机器上的其他进程并且限制了网络的访问，但是 Docker 容器中的进程仍然能够访问或者修改宿主机器上的其他目录，这是我们不希望看到的。</p><p>在新的进程中创建隔离的挂载点命名空间需要在 <code>clone</code> 函数中传入 <code>CLONE_NEWNS</code>，这样子进程就能得到父进程挂载点的拷贝，如果不传入这个参数<strong>子进程对文件系统的读写都会同步回父进程以及整个主机的文件系统</strong>。</p><p>如果一个容器需要启动，那么它一定需要提供一个根文件系统（rootfs），容器需要使用这个文件系统来创建一个新的进程，所有二进制的执行都必须在这个根文件系统中。</p><p><img src="/images/libcontainer-filesystem.png" alt="libcontainer-filesystem"></p><p>想要正常启动一个容器就需要在 rootfs 中挂载以上的几个特定的目录，除了上述的几个目录需要挂载之外我们还需要建立一些符号链接保证系统 IO 不会出现问题。</p><p><img src="/images/libcontainer-symlinks-and-io.png" alt="libcontainer-symlinks-and-io"></p><p>为了保证当前的容器进程没有办法访问宿主机器上其他目录，我们在这里还需要通过 <code>libcontainer</code> 提供的 <code>pivot_root</code> 或者 <code>chroot</code> 函数改变进程能够访问个文件目录的根节点。</p><figure class="highlight go"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// pivor_root</span></span><br><span class="line">put_old = mkdir(...);</span><br><span class="line">pivot_root(rootfs, put_old);</span><br><span class="line">chdir(<span class="string">"/"</span>);</span><br><span class="line">unmount(put_old, MS_DETACH);</span><br><span class="line">rmdir(put_old);</span><br><span class="line"></span><br><span class="line"><span class="comment">// chroot</span></span><br><span class="line">mount(rootfs, <span class="string">"/"</span>, NULL, MS_MOVE, NULL);</span><br><span class="line">chroot(<span class="string">"."</span>);</span><br><span class="line">chdir(<span class="string">"/"</span>);</span><br></pre></td></tr></table></figure><p>到这里我们就将容器需要的目录挂载到了容器中，同时也禁止当前的容器进程访问宿主机器上的其他目录，保证了不同文件系统的隔离。</p><blockquote><p>这一部分的内容是作者在 <code>libcontainer</code> 中的 <a href="https://github.com/opencontainers/runc/blob/master/libcontainer/SPEC.md" target="_blank" rel="noopener">SPEC.md</a> 文件中找到的，其中包含了 <code>Docker</code> 使用的文件系统的说明，对于 <code>Docker</code> 是否真的使用 <code>chroot</code> 来确保当前的进程无法访问宿主机器的目录，作者其实也<strong>没有确切的答案</strong>，一是 <code>Docker</code> 项目的代码太多庞大，不知道该从何入手，作者尝试通过 <code>Google</code> 查找相关的结果，但是既找到了无人回答的问题，也得到了与 <code>SPEC</code> 中的描述有冲突的答案 ，如果各位读者有明确的答案可以在博客下面留言，非常感谢。</p></blockquote><h3 id="chroot"><a href="#chroot" class="headerlink" title="chroot"></a>chroot</h3><p>在这里不得不简单介绍一下 <code>chroot</code>（change root），在 Linux 系统中，系统默认的目录就都是以 <code>/</code> 也就是根目录开头的，<code>chroot</code> 的使用能够改变当前的系统根目录结构，通过改变当前系统的根目录，我们能够限制用户的权利，在新的根目录下并不能够访问旧系统根目录的结构个文件，也就建立了一个与原系统完全隔离的目录结构。</p><blockquote><p>与 <code>chroot</code> 的相关内容部分来自 理解 <a href="https://www.ibm.com/developerworks/cn/linux/l-cn-chroot/index.html" target="_blank" rel="noopener">chroot</a> 一文，各位读者可以阅读这篇文章获得更详细的信息。</p></blockquote><h3 id="CGroups"><a href="#CGroups" class="headerlink" title="CGroups"></a>CGroups</h3><p>我们通过 Linux 的命名空间为新创建的进程隔离了文件系统、网络并与宿主机器之间的进程相互隔离，但是命名空间并不能够为我们提供物理资源上的隔离，比如 CPU 或者内存，如果在同一台机器上运行了多个对彼此以及宿主机器一无所知的『容器』，这些容器却共同占用了宿主机器的物理资源。</p><p><img src="/images/docker-shared-resources.png" alt="docker-shared-resources"></p><p>如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题，而 Control Groups（简称 CGroups）就是能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 和网络带宽。</p><p>每一个 CGroup 都是一组被相同的标准和参数限制的进程，不同的 CGroup 之间是有层级关系的，也就是说它们之间可以从父类继承一些用于限制资源使用的标准和参数。</p><p><img src="/images/cgroups-inheritance.png" alt="cgroups-inheritance"></p><p>Linux 的 CGroup 能够为一组进程分配资源，也就是我们在上面提到的 CPU、内存、网络带宽等资源，通过对资源的分配，CGroup 能够提供以下的几种功能：</p><p><img src="/images/cgroups-features.png" alt="cgroups-features"></p><blockquote><p>在 CGroup 中，所有的任务就是一个系统的一个进程，而 CGroup 就是一组按照某种标准划分的进程，在 CGroup 这种机制中，所有的资源控制都是以 CGroup 作为单位实现的，每一个进程都可以随时加入一个 CGroup 也可以随时退出一个 CGroup。<br><a href="https://www.ibm.com/developerworks/cn/linux/1506_cgroup/index.html" target="_blank" rel="noopener">– CGroup 介绍、应用实例及原理描述</a></p></blockquote><p>Linux 使用文件系统来实现 CGroup，我们可以直接使用下面的命令查看当前的 CGroup 中有哪些子系统：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ lssubsys -m</span><br><span class="line">cpuset /sys/fs/cgroup/cpuset</span><br><span class="line">cpu /sys/fs/cgroup/cpu</span><br><span class="line">cpuacct /sys/fs/cgroup/cpuacct</span><br><span class="line">memory /sys/fs/cgroup/memory</span><br><span class="line">devices /sys/fs/cgroup/devices</span><br><span class="line">freezer /sys/fs/cgroup/freezer</span><br><span class="line">blkio /sys/fs/cgroup/blkio</span><br><span class="line">perf_event /sys/fs/cgroup/perf_event</span><br><span class="line">hugetlb /sys/fs/cgroup/hugetlb</span><br></pre></td></tr></table></figure><p>大多数 Linux 的发行版都有着非常相似的子系统，而之所以将上面的 cpuset、cpu 等东西称作子系统，是因为它们能够为对应的控制组分配资源并限制资源的使用。</p><p>如果我们想要创建一个新的 cgroup 只需要在想要分配或者限制资源的子系统下面创建一个新的文件夹，然后这个文件夹下就会自动出现很多的内容，如果你在 Linux 上安装了 Docker，你就会发现所有子系统的目录下都有一个名为 docker 的文件夹：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">$ ls cpu</span><br><span class="line">cgroup.clone_children  </span><br><span class="line">...</span><br><span class="line">cpu.stat  </span><br><span class="line">docker  </span><br><span class="line">notify_on_release </span><br><span class="line">release_agent </span><br><span class="line">tasks</span><br><span class="line"></span><br><span class="line">$ ls cpu/docker/</span><br><span class="line">9c3057f1291b53fd54a3d12023d2644efe6a7db6ddf330436ae73ac92d401cf1 </span><br><span class="line">cgroup.clone_children  </span><br><span class="line">...</span><br><span class="line">cpu.stat  </span><br><span class="line">notify_on_release </span><br><span class="line">release_agent </span><br><span class="line">tasks</span><br></pre></td></tr></table></figure><p><code>9c3057xxx</code> 其实就是我们运行的一个 <code>Docker</code> 容器，启动这个容器时，<code>Docker</code> 会为这个容器创建一个与容器标识符相同的 <code>CGroup</code>，在当前的主机上 <code>CGroup</code> 就会有以下的层级关系：</p><p><img src="/images/linux-cgroups.png" alt="linux-cgroups"></p><p>每一个 <code>CGroup</code> 下面都有一个 <code>tasks</code> 文件，其中存储着属于当前控制组的所有进程的 <code>pid</code>，作为负责 <code>cpu</code> 的子系统，<code>cpu.cfs_quota_us</code> 文件中的内容能够对 <code>CPU</code> 的使用作出限制，如果当前文件的内容为 <code>50000</code>，那么当前控制组中的全部进程的 <code>CPU</code> 占用率不能超过 50%。</p><p>如果系统管理员想要控制 <code>Docker</code> 某个容器的资源使用率就可以在 <code>docker</code> 这个父控制组下面找到对应的子控制组并且改变它们对应文件的内容，当然我们也可以直接在程序运行时就使用参数，让 <code>Docker</code> 进程去改变相应文件中的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ docker run -it -d --cpu-quota=50000 busybox</span><br><span class="line">53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274</span><br><span class="line">$ <span class="built_in">cd</span> 53861305258ecdd7f5d2a3240af694aec9adb91cd4c7e210b757f71153cdd274/</span><br><span class="line">$ ls</span><br><span class="line">cgroup.clone_children  cgroup.event_control  cgroup.procs  cpu.cfs_period_us  cpu.cfs_quota_us  cpu.shares  cpu.stat  notify_on_release  tasks</span><br><span class="line">$ cat cpu.cfs_quota_us</span><br><span class="line">50000</span><br></pre></td></tr></table></figure><p>当我们使用 Docker 关闭掉正在运行的容器时，Docker 的子控制组对应的文件夹也会被 Docker 进程移除，Docker 在使用 CGroup 时其实也只是做了一些创建文件夹改变文件内容的文件操作，不过 CGroup 的使用也确实解决了我们限制子容器资源占用的问题，系统管理员能够为多个容器合理的分配资源并且不会出现多个容器互相抢占资源的问题。</p><h2 id="UnionFS"><a href="#UnionFS" class="headerlink" title="UnionFS"></a>UnionFS</h2><p>Linux 的命名空间和控制组分别解决了不同资源隔离的问题，前者解决了进程、网络以及文件系统的隔离，后者实现了 CPU、内存等资源的隔离，但是在 Docker 中还有另一个非常重要的问题需要解决 - 也就是镜像。</p><p>镜像到底是什么，它又是如何组成和组织的是作者使用 Docker 以来的一段时间内一直比较让作者感到困惑的问题，我们可以使用 docker run 非常轻松地从远程下载 Docker 的镜像并在本地运行。</p><p>Docker 镜像其实本质就是一个压缩包，我们可以使用下面的命令将一个 Docker 镜像中的文件导出：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ docker <span class="built_in">export</span> $(docker create busybox) | tar -C rootfs -xvf -</span><br><span class="line">$ ls</span><br><span class="line">bin  dev  etc  home proc root sys  tmp  usr  var</span><br></pre></td></tr></table></figure><p>你可以看到这个 busybox 镜像中的目录结构与 Linux 操作系统的根目录中的内容并没有太多的区别，可以说 <strong>Docker 镜像就是一个文件</strong>。</p><h2 id="存储驱动"><a href="#存储驱动" class="headerlink" title="存储驱动"></a>存储驱动</h2><p>Docker 使用了一系列不同的存储驱动管理镜像内的文件系统并运行容器，这些存储驱动与 Docker 卷（volume）有些不同，存储引擎管理着能够在多个容器之间共享的存储。</p><p>想要理解 Docker 使用的存储驱动，我们首先需要理解 Docker 是如何构建并且存储镜像的，也需要明白 Docker 的镜像是如何被每一个容器所使用的；Docker 中的每一个镜像都是由一系列只读的层组成的，Dockerfile 中的每一个命令都会在已有的只读层上创建一个新的层：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">FROM ubuntu:15.04</span><br><span class="line">COPY . /app</span><br><span class="line">RUN make /app</span><br><span class="line">CMD python /app/app.py</span><br></pre></td></tr></table></figure><p>容器中的每一层都只对当前容器进行了非常小的修改，上述的 Dockerfile 文件会构建一个拥有四层 layer 的镜像：</p><p><img src="/images/docker-container-layer.png" alt="docker-container-layer"></p><p>当镜像被 <code>docker run</code> 命令创建时就会在镜像的最上层添加一个可写的层，也就是容器层，所有对于运行时容器的修改其实都是对这个容器读写层的修改。</p><p>容器和镜像的区别就在于，所有的镜像都是只读的，而每一个容器其实等于镜像加上一个可读写的层，也就是同一个镜像可以对应多个容器。</p><p><img src="/images/docker-images-and-container.png" alt="docker-images-and-container"></p><h2 id="AUFS"><a href="#AUFS" class="headerlink" title="AUFS"></a>AUFS</h2><p>UnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统『联合』到同一个挂载点的文件系统服务。而 AUFS 即 Advanced UnionFS 其实就是 UnionFS 的升级版，它能够提供更优秀的性能和效率。</p><p>AUFS 作为联合文件系统，它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，这些文件夹在 AUFS 中称作分支，整个『联合』的过程被称为联合挂载（Union Mount）：</p><p><img src="/images/docker-aufs.png" alt="docker-aufs"></p><p>每一个镜像层或者容器层都是 <code>/var/lib/docker/</code> 目录下的一个子文件夹；在 <code>Docker</code> 中，所有镜像层和容器层的内容都存储在 <code>/var/lib/docker/aufs/diff/</code> 目录中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ls /var/lib/docker/aufs/diff/00adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c       93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d8</span><br><span class="line">00adcccc1a55a36a610a6ebb3e07cc35577f2f5a3b671be3dbc0e74db9ca691c-init  93604f232a831b22aeb372d5b11af8c8779feb96590a6dc36a80140e38e764d8-init</span><br><span class="line">019a8283e2ff6fca8d0a07884c78b41662979f848190f0658813bb6a9a464a90       93b06191602b7934fafc984fbacae02911b579769d0debd89cf2a032e7f35cfa</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>而 <code>/var/lib/docker/aufs/layers/</code> 中存储着镜像层的元数据，每一个文件都保存着镜像层的元数据，最后的 <code>/var/lib/docker/aufs/mnt/</code> 包含镜像或者容器层的挂载点，最终会被 <code>Docker</code> 通过联合的方式进行组装。</p><p><img src="/images/docker-filesystems.png" alt="docker-filesystems"></p><p>上面的这张图片非常好的展示了组装的过程，每一个镜像层都是建立在另一个镜像层之上的，同时所有的镜像层都是只读的，只有每个容器最顶层的容器层才可以被用户直接读写，所有的容器都建立在一些底层服务（Kernel）上，包括命名空间、控制组、rootfs 等等，这种容器的组装方式提供了非常大的灵活性，只读的镜像层通过共享也能够减少磁盘的占用。</p><h2 id="其他存储驱动"><a href="#其他存储驱动" class="headerlink" title="其他存储驱动"></a>其他存储驱动</h2><p>AUFS 只是 <code>Docker</code> 使用的存储驱动的一种，除了 AUFS 之外，<code>Docker</code> 还支持了不同的存储驱动，包括 <code>aufs</code>、<code>devicemapper</code>、<code>overlay2</code>、<code>zfs</code> 和 <code>vfs</code> 等等，在最新的 <code>Docker</code> 中，<code>overlay2</code> 取代了 <code>aufs</code> 成为了推荐的存储驱动，但是在没有 <code>overlay2</code> 驱动的机器上仍然会使用 <code>aufs</code> 作为 <code>Docker</code> 的默认驱动。</p><p><img src="/images/docker-storage-driver.png" alt="docker-storage-driver"></p><p>不同的存储驱动在存储镜像和容器文件时也有着完全不同的实现，有兴趣的读者可以在 Docker 的官方文档 <a href="https://docs.docker.com/engine/userguide/storagedriver/selectadriver/" target="_blank" rel="noopener">Select a storage driver</a> 中找到相应的内容。</p><p>想要查看当前系统的 Docker 上使用了哪种存储驱动只需要使用以下的命令就能得到相对应的信息：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ docker info | grep Storage</span><br><span class="line">Storage Driver: aufs</span><br></pre></td></tr></table></figure><p>作者的这台 Ubuntu 上由于没有 overlay2 存储驱动，所以使用 aufs 作为 Docker 的默认存储驱动。</p><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>Docker 目前已经成为了非常主流的技术，已经在很多成熟公司的生产环境中使用，但是 Docker 的核心技术其实已经有很多年的历史了，Linux 命名空间、控制组和 UnionFS 三大技术支撑了目前 Docker 的实现，也是 Docker 能够出现的最重要原因。</p><p>作者在学习 Docker 实现原理的过程中查阅了非常多的资料，从中也学习到了很多与 Linux 操作系统相关的知识，不过由于 Docker 目前的代码库实在是太过庞大，想要从源代码的角度完全理解 Docker 实现的细节已经是非常困难的了，但是如果各位读者真的对其实现细节感兴趣，可以从 <a href="https://github.com/docker/docker-ce" target="_blank" rel="noopener">Docker CE</a> 的源代码开始了解 Docker 的原理。</p><h2 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a><a href="https://draveness.me/docker" target="_blank" rel="noopener">原文链接</a></h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是Docker 核心技术与实现原理&lt;/p&gt;
&lt;p&gt;提到虚拟化技术，我们首先想到的一定是 Docker，经过四年的快速发展 Docker 已经成为了很多公司的标配，也不再是一个只能在开发阶段使用的玩具了。作为在生产环境中广泛应用的产品，Docker 有着非常成熟的
      
    
    </summary>
    
      <category term="Docker核心技术与实现原理" scheme="http://yoursite.com/categories/Docker%E6%A0%B8%E5%BF%83%E6%8A%80%E6%9C%AF%E4%B8%8E%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
  </entry>
  
  <entry>
    <title>Ceph修改OSD和Monitor的网络</title>
    <link href="http://yoursite.com/2018/03/17/ceph-change-subnet/"/>
    <id>http://yoursite.com/2018/03/17/ceph-change-subnet/</id>
    <published>2018-03-16T16:00:00.000Z</published>
    <updated>2018-03-29T07:16:42.976Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph修改OSD和Monitor的网络过程记录。</p><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>随着Ceph的应用越来越深入，我们也遇到了越来越多的需求，本次我们将讲一下如何修改OSD和Monitor的网络。</p><p>Ceph应用中，在部署完ceph集群并运行一段时间后，我们很可能会遇到机房网络变动，或集群网络升级的情况，这时我们都期望能在尽量减少对现有Ceph集群影响的情况下，修改Ceph的OSD和Monitor网络，而不是简单粗暴的重新部署Ceph集群。<br>本文基于Ceph的官方文档和作者的实践，给出了正确修改Ceph OSD和Monitor网络的方法。</p><p>Ceph版本：Hammer 0.94.5<br>测试系统环境：Ubuntu 14.04</p><h2 id="修改OSD的网络"><a href="#修改OSD的网络" class="headerlink" title="修改OSD的网络"></a>修改OSD的网络</h2><p>由于Ceph OSD的网络配置是启动时读取ceph.conf配置动态加载的，所以修改OSD的网络比较简单，步骤如下：</p><p>修改下<code>ceph.conf</code>中<code>pubulic/cluster network</code>信息<br>     例如：<code>public network = 172.16.1.0/24</code><br>           <code>cluster network = 172.16.1.0/24</code></p><p>把<code>ceph.conf</code>更新到所有OSD节点测配合<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy --overwrite-conf config push &lt;node1&gt; &lt;node2&gt; &lt;node3&gt;</span><br></pre></td></tr></table></figure></p><p>重启所有的<code>osd daemon</code><br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ restart ceph-all</span><br></pre></td></tr></table></figure></p><p>检查<code>OSD</code>使用的网络<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ netstat -nap | grep ceph-osd</span><br><span class="line">tcp        0      0 172.16.1.16:844         0.0.0.0:*               LISTEN      23412/ceph-osd</span><br><span class="line">tcp        0      0 172.16.1.16:812         0.0.0.0:*               LISTEN      19423/ceph-osd</span><br><span class="line">tcp        0      0 172.16.1.16:845         0.0.0.0:*               LISTEN      23412/ceph-osd</span><br><span class="line">tcp        0      0 172.16.1.16:813         0.0.0.0:*               LISTEN      19529/ceph-osd</span><br></pre></td></tr></table></figure></p><h2 id="修改Monitor的网络"><a href="#修改Monitor的网络" class="headerlink" title="修改Monitor的网络"></a>修改Monitor的网络</h2><p>因为Ceph的Monitor是集群中非常重要的模块，它们需要维护一组合理有效的Monitor节点信息，这些节点之间彼此能发现，它们通过选举达成一致的状态，来保证整个Ceph系统处于一个可用的一致状态。<br>不同于别的Ceph Daemon通过ceph.conf文件中的配置来与Monitor通信，Monitors之间则通过独立的monitor map来彼此发现，在monitor map中有monitor的ip信息，所以单独通过修改ceph.conf文件的方法来修改Monitor的网络是行不通的。<br>下面给出了两种办法来达到修改monitor网络的目的：</p><ul><li>通过依次添加新的monitor node(使用新的网络)，再删除旧的monitor node的方法</li></ul><p><strong>注意：</strong>新的monitor网络跟旧的monitor的网络必须是互通的，否则该方法失效。</p><p>添加新的monitor节点步骤如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">1). 在新的monitor节点创建默认文件夹</span><br><span class="line">$ ssh &#123;new-mon-host&#125;</span><br><span class="line">$ sudo mkdir /var/lib/ceph/mon/ceph-&#123;mon-id&#125;</span><br><span class="line"></span><br><span class="line">2). 获取monitors的keyring文件，若内部集群没配置认证，该步会出错，忽略即可</span><br><span class="line">$ ceph auth get mon. -o &#123;tmp&#125;/&#123;key-filename&#125;</span><br><span class="line"></span><br><span class="line">3). 获取monitor map文件</span><br><span class="line">$ ceph mon getmap -o &#123;tmp&#125;/&#123;map-filename&#125;</span><br><span class="line"></span><br><span class="line">4). 在新的monitor节点生产monitor所需数据信息</span><br><span class="line">$ sudo ceph-mon -i &#123;mon-id&#125; --mkfs --monmap &#123;tmp&#125;/&#123;map-filename&#125; --keyring &#123;tmp&#125;/&#123;key-filename&#125;</span><br><span class="line"></span><br><span class="line">5). 添加新的monitor到monitor集群，绑定新的ip地址和port</span><br><span class="line">$ ceph-mon -i &#123;mon-id&#125; --public-addr &#123;ip:port&#125;</span><br></pre></td></tr></table></figure><p>删除旧的monitor节点步骤如下：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">1). 在旧的monitor节点停止该monitor</span><br><span class="line">$ service ceph -a stop mon.&#123;mon-id&#125;</span><br><span class="line"></span><br><span class="line">2). 把该monitor从集群中移除</span><br><span class="line">$ ceph mon remove &#123;mon-id&#125;</span><br></pre></td></tr></table></figure></p><p>修改monitor map里的信息，更新后重启所有monitors<br>因为该方法一般是通过整体修改monitor map里的网络信息，然后重启monitors，所以会短暂影响Ceph集群的服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">1). 获取现在的monitor map</span><br><span class="line">$ ceph mon getmap -o &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line"></span><br><span class="line">2). 查看当前的monitor map信息</span><br><span class="line">$ monmaptool --<span class="built_in">print</span> &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">$ monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">epoch 1</span><br><span class="line">fsid 224e376d-c5fe-4504-96bb-ea6332a19e61</span><br><span class="line">last_changed 2012-12-17 02:46:41.591248</span><br><span class="line">created 2012-12-17 02:46:41.591248</span><br><span class="line">0: 10.0.0.1:6789/0 mon.a</span><br><span class="line">1: 10.0.0.2:6789/0 mon.b</span><br><span class="line">2: 10.0.0.3:6789/0 mon.c</span><br><span class="line"></span><br><span class="line">3). 删除现有的monitors信息</span><br><span class="line">$ monmaptool --rm a --rm b --rm c &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">monmaptool: removing a</span><br><span class="line">monmaptool: removing b</span><br><span class="line">monmaptool: removing c</span><br><span class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (0 monitors)</span><br><span class="line"></span><br><span class="line">4). 添加新的monitors信息</span><br><span class="line">$ monmaptool --add a 10.1.0.1:6789 --add b 10.1.0.2:6789 --add c 10.1.0.3:6789 &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line"></span><br><span class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">monmaptool: writing epoch 1 to &#123;tmp&#125;/&#123;filename&#125; (3 monitors)</span><br><span class="line"></span><br><span class="line">5). 检查新的monitors信息</span><br><span class="line">$ monmaptool --<span class="built_in">print</span> &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">monmaptool: monmap file &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line">epoch 1</span><br><span class="line">fsid 224e376d-c5fe-4504-96bb-ea6332a19e61</span><br><span class="line">last_changed 2012-12-17 02:46:41.591248</span><br><span class="line">created 2012-12-17 02:46:41.591248</span><br><span class="line">0: 10.1.0.1:6789/0 mon.a</span><br><span class="line">1: 10.1.0.2:6789/0 mon.b</span><br><span class="line">2: 10.1.0.3:6789/0 mon.c</span><br><span class="line"></span><br><span class="line">6). copy修改后的&#123;tmp&#125;/&#123;filename&#125;文件到所有monitor节点</span><br><span class="line"></span><br><span class="line">7). 停止所有的monitor</span><br><span class="line">$ stop ceph-mon-all</span><br><span class="line"></span><br><span class="line">8). 注入新的monitor map信息</span><br><span class="line">$ ceph-mon -i &#123;mon-id&#125; --inject-monmap &#123;tmp&#125;/&#123;filename&#125;</span><br><span class="line"></span><br><span class="line">9). 启动所有的monitro</span><br><span class="line">$ start ceph-mon-all</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph修改OSD和Monitor的网络过程记录。&lt;/p&gt;
&lt;h2 id=&quot;背景&quot;&gt;&lt;a href=&quot;#背景&quot; class=&quot;headerlink&quot; title=&quot;背景&quot;&gt;&lt;/a&gt;背景&lt;/h2&gt;&lt;p&gt;随着Ceph的应用越来越深入，我们也遇到了越来
      
    
    </summary>
    
      <category term="Ceph修改OSD和Monitor的网络" scheme="http://yoursite.com/categories/Ceph%E4%BF%AE%E6%94%B9OSD%E5%92%8CMonitor%E7%9A%84%E7%BD%91%E7%BB%9C/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph日常命令使用</title>
    <link href="http://yoursite.com/2018/03/16/ceph-commands/"/>
    <id>http://yoursite.com/2018/03/16/ceph-commands/</id>
    <published>2018-03-15T16:00:00.000Z</published>
    <updated>2018-03-28T06:53:55.263Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph的日常命令使用方法。</p><h2 id="Ceph日常操作命令"><a href="#Ceph日常操作命令" class="headerlink" title="Ceph日常操作命令"></a>Ceph日常操作命令</h2><p>查看状态命令：<br>查看ceph集群状态：ceph -s<br>查看mon状态：ceph mon stat<br>查看msd状态：ceph msd stat<br>查看osd状态：ceph osd stat<br>查看osd目录树（可以查看每个osd挂在哪台机，是否已启动）：ceph osd tree</p><p>启动ceph进程命令：<br>需要在对应的节点进行启动（如果对应节点没有该服务，会进行提示）<br>启动mon进程：service ceph start mon.ceph-node1<br>启动msd进程：service ceph start msd.ceoh-node1<br>启动osd进程：service ceph start osd.0（在ceph-node1上）<br>启动osd进程：service ceph start osd.1（在ceph-node2上）<br>启动osd进程：service ceph start osd.2（在ceph-node3上）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看机器的监控状态</span></span><br><span class="line">$ ceph health</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph的实时运行状态</span></span><br><span class="line">$ ceph -w</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 检查信息状态信息</span></span><br><span class="line">$ ceph -s</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph存储空间</span></span><br><span class="line">$ ceph df</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个节点的所有的ceph数据包</span></span><br><span class="line">$ ceph-deploy purge ceph-node1</span><br><span class="line">$ ceph-deploy purgedata ceph-node1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 为ceph创建一个admin用户并为admin用户创建一个密钥，把密钥保存到/etc/ceph目录下：</span></span><br><span class="line">$ ceph auth get-or-create client.admin mds <span class="string">'allow'</span> osd <span class="string">'allow '</span> mon <span class="string">'allow '</span> &gt; /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"><span class="comment">## 或</span></span><br><span class="line">$ ceph auth get-or-create client.admin mds <span class="string">'allow'</span> osd <span class="string">'allow '</span> mon <span class="string">'allow '</span> -o /etc/ceph/ceph.client.admin.keyring</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 为osd.0创建一个用户并创建一个key</span></span><br><span class="line">$ ceph auth get-or-create osd.0 mon <span class="string">'allow rwx'</span> osd <span class="string">'allow *'</span> -o /var/lib/ceph/osd/ceph-0/keyring</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 为mds.node1创建一个用户并创建一个key（ceph-node1是节点名称）</span></span><br><span class="line">$ ceph auth get-or-create mds.node1 mon <span class="string">'allow rwx'</span> osd <span class="string">'allow '</span> mds <span class="string">'allow '</span> -o /var/lib/ceph/mds/ceph-ceph-node1/keyring</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph集群中的认证用户及相关的key</span></span><br><span class="line">$ ceph auth list</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除集群中的一个认证用户</span></span><br><span class="line">$ ceph auth del osd.0</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看集群的详细配置（ceph-node1是节点名称）</span></span><br><span class="line">$ ceph daemon mon.ceph-node1 config show | more</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看集群健康状态细节</span></span><br><span class="line">$ ceph health detail</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph log日志所在的目录</span></span><br><span class="line">$ ceph-conf –name mon.node1 –show-config-value log_file</span><br></pre></td></tr></table></figure><h2 id="关于mon节点的相关操作命令"><a href="#关于mon节点的相关操作命令" class="headerlink" title="关于mon节点的相关操作命令"></a>关于mon节点的相关操作命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看mon的状态信息</span></span><br><span class="line">$ ceph mon <span class="built_in">stat</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看mon的选举状态</span></span><br><span class="line">$ ceph quorum_status</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看mon的映射信息</span></span><br><span class="line">$ ceph mon dump</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个mon节点</span></span><br><span class="line">$ ceph mon remove node1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 获得一个正在运行的mon map，并保存在1.txt文件中</span></span><br><span class="line">$ ceph mon getmap -o 1.txt</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看上面获得的map</span></span><br><span class="line">$ monmaptool --<span class="built_in">print</span> 1.txt</span><br><span class="line"> </span><br><span class="line"><span class="comment">## map注入新加入的节点（如新节点主机名为ceph-node4）</span></span><br><span class="line">$ ceph-mon -i ceph-node4 --inject-monmap 1.txt</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看mon的amin socket</span></span><br><span class="line">$ ceph-conf --name mon.ceph-node1 --show-config-value admin_socket</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看mon的详细状态（ceph-node1为mon节点主机名）</span></span><br><span class="line">$ ceph daemon mon.ceph-node1 mon_status</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个mon节点（ceph-node1为mon节点主机名）</span></span><br><span class="line">$ ceph mon remove ceph-node1</span><br></pre></td></tr></table></figure><h2 id="msd节点相关操作命令"><a href="#msd节点相关操作命令" class="headerlink" title="msd节点相关操作命令"></a>msd节点相关操作命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看msd状态</span></span><br><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看msd的映射信息</span></span><br><span class="line">$ ceph mds dump</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个mds节点</span></span><br><span class="line">$ ceph mds rm 0 mds.ceph-node1</span><br></pre></td></tr></table></figure><h2 id="osd节点相关操作命令"><a href="#osd节点相关操作命令" class="headerlink" title="osd节点相关操作命令"></a>osd节点相关操作命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看ceph osd运行状态</span></span><br><span class="line">$ ceph osd <span class="built_in">stat</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看osd映射信息</span></span><br><span class="line">$ ceph osd dump</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看osd的目录树</span></span><br><span class="line">$ ceph osd tree</span><br><span class="line"> </span><br><span class="line"><span class="comment">## down掉一个osd硬盘(比如down掉osd.0节点磁盘）</span></span><br><span class="line">$ ceph osd down 0    </span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在集群中删除一个osd硬盘</span></span><br><span class="line">$ ceph osd rm 0</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在集群中删除一个osd 硬盘 crush map</span></span><br><span class="line">$ ceph osd crush rm osd.0</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在集群中删除一个osd的host节点</span></span><br><span class="line">$ ceph osd crush rm node1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看最大osd的个数（默认最大是4个osd节点）</span></span><br><span class="line">$ ceph osd getmaxosd</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置最大的osd的个数（当扩大osd节点的时候必须扩大这个值）</span></span><br><span class="line">$ ceph osd setmaxosd 10</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置osd crush的权重为1.0</span></span><br><span class="line">$ ceph osd crush <span class="built_in">set</span> &#123;id&#125; &#123;weight&#125; [&#123;loc1&#125; [&#123;loc2&#125; …]]</span><br><span class="line"></span><br><span class="line"><span class="comment">## 例如：</span></span><br><span class="line">$ ceph osd crush <span class="built_in">set</span> 3 3.0 host=ceph-node4</span><br><span class="line"><span class="built_in">set</span> item id 3 name <span class="string">'osd.3'</span> weight 3 at location &#123;host=node4&#125; to crush map</span><br><span class="line">$ ceph osd tree</span><br><span class="line"><span class="comment"># id weight type name up/down reweight</span></span><br><span class="line">-1 6 root default</span><br><span class="line">-2 1 host ceph-node1</span><br><span class="line">0 1 osd.0 up 1</span><br><span class="line">-3 1 host ceph-node2</span><br><span class="line">1 1 osd.1 up 1</span><br><span class="line">-4 1 host ceph-node3</span><br><span class="line">2 1 osd.2 up 1</span><br><span class="line">-5 3 host ceph-node4</span><br><span class="line">3 3 osd.3 up 0.5</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 或者用下面的方式</span></span><br><span class="line">$ ceph osd crush reweight osd.3 1.0</span><br><span class="line">reweighted item id 3 name <span class="string">'osd.3'</span> to 1 <span class="keyword">in</span> crush map</span><br><span class="line">$ ceph osd tree</span><br><span class="line"><span class="comment"># id weight type name up/down reweight</span></span><br><span class="line">-1 4 root default</span><br><span class="line">-2 1 host node1</span><br><span class="line">0 1 osd.0 up 1</span><br><span class="line">-3 1 host node2</span><br><span class="line">1 1 osd.1 up 1</span><br><span class="line">-4 1 host node3</span><br><span class="line">2 1 osd.2 up 1</span><br><span class="line">-5 1 host node4</span><br><span class="line">3 1 osd.3 up 0.5</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置osd的权重</span></span><br><span class="line"> </span><br><span class="line">$ ceph osd reweight 3 0.5</span><br><span class="line">reweighted osd.3 to 0.5 (8327682)</span><br><span class="line">$ ceph osd tree</span><br><span class="line"><span class="comment"># id weight type name up/down reweight</span></span><br><span class="line">-1 4 root default</span><br><span class="line">-2 1 host node1</span><br><span class="line">0 1 osd.0 up 1</span><br><span class="line">-3 1 host node2</span><br><span class="line">1 1 osd.1 up 1</span><br><span class="line">-4 1 host node3</span><br><span class="line">2 1 osd.2 up 1</span><br><span class="line">-5 1 host node4</span><br><span class="line">3 1 osd.3 up 0.5</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 把一个osd节点逐出集群</span></span><br><span class="line"> </span><br><span class="line">$ ceph osd out osd.3</span><br><span class="line">marked out osd.3.</span><br><span class="line">$ ceph osd tree</span><br><span class="line"><span class="comment"># id weight type name up/down reweight</span></span><br><span class="line">-1 4 root default</span><br><span class="line">-2 1 host node1</span><br><span class="line">0 1 osd.0 up 1</span><br><span class="line">-3 1 host node2</span><br><span class="line">1 1 osd.1 up 1</span><br><span class="line">-4 1 host node3</span><br><span class="line">2 1 osd.2 up 1</span><br><span class="line">-5 1 host node4</span><br><span class="line">3 1 osd.3 up 0</span><br><span class="line"><span class="comment">## osd.3的reweight变为0了就不再分配数据，但是设备还是存活的</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 把逐出的osd加入集群</span></span><br><span class="line"> </span><br><span class="line">$ ceph osd <span class="keyword">in</span> osd.3</span><br><span class="line">marked <span class="keyword">in</span> osd.3.</span><br><span class="line">$ ceph osd tree</span><br><span class="line"><span class="comment"># id weight type name up/down reweight</span></span><br><span class="line">-1 4 root default</span><br><span class="line">-2 1 host node1</span><br><span class="line">0 1 osd.0 up 1</span><br><span class="line">-3 1 host node2</span><br><span class="line">1 1 osd.1 up 1</span><br><span class="line">-4 1 host node3</span><br><span class="line">2 1 osd.2 up 1</span><br><span class="line">-5 1 host node4</span><br><span class="line">3 1 osd.3 up 1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 暂停osd （暂停后整个集群不再接收数据）</span></span><br><span class="line">$ ceph osd pause</span><br><span class="line"><span class="comment"># set pauserd,pausewr</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 再次开启osd （开启后再次接收数据）</span></span><br><span class="line">$ ceph osd unpause</span><br><span class="line"><span class="comment"># unset pauserd,pausewr</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看一个集群osd.2参数的配置</span></span><br><span class="line">$ ceph –admin-daemon /var/run/ceph/ceph-osd.2.asok config show | less</span><br></pre></td></tr></table></figure><h2 id="PG组相关操作命令"><a href="#PG组相关操作命令" class="headerlink" title="PG组相关操作命令"></a>PG组相关操作命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看pg组的映射信息</span></span><br><span class="line">$ ceph pg dump</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看一个PG的map</span></span><br><span class="line">$ ceph pg map 0.3f</span><br><span class="line">osdmap e88 pg 0.3f (0.3f) -&gt; up [0,2] acting [0,2]</span><br><span class="line"><span class="comment">##其中的[0,2]代表存储在osd.0、osd.2节点，osd.0代表主副本的存储位置</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看PG状态</span></span><br><span class="line">$ ceph pg <span class="built_in">stat</span></span><br><span class="line">v1164: 448 pgs: 448 active+clean; 10003 MB data, 23617 MB used, 37792 MB / 61410 MB avail</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查询一个pg的详细信息</span></span><br><span class="line">$ ceph pg 0.26 query</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看pg中stuck的状态</span></span><br><span class="line">$ ceph pg dump_stuck unclean</span><br><span class="line">ok</span><br><span class="line">$ ceph pg dump_stuck inactive</span><br><span class="line">ok</span><br><span class="line">$ ceph pg dump_stuck stale</span><br><span class="line">ok</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 显示一个集群中的所有的pg统计</span></span><br><span class="line">$ ceph pg dump –format plain</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 恢复一个丢失的pg</span></span><br><span class="line">$ ceph pg &#123;pg-id&#125; mark_unfound_lost revert</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 显示非正常状态的pg</span></span><br><span class="line">$ ceph pg dump_stuck inactive|unclean|stale</span><br></pre></td></tr></table></figure><h2 id="pool相关操作命令"><a href="#pool相关操作命令" class="headerlink" title="pool相关操作命令"></a>pool相关操作命令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">ph集群中的pool数量</span><br><span class="line">$ ceph osd lspools</span><br><span class="line">0 data,1 metadata,2 rbd,</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在ceph集群中创建一个pool</span></span><br><span class="line">$ ceph osd pool create kevin 100</span><br><span class="line"><span class="comment">## 这里的100指的是PG组，kevin是集群名称</span></span><br><span class="line"> </span><br><span class="line"><span class="comment">## 为一个ceph pool配置配额</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span>-quota data max_objects 10000</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在集群中删除一个pool（集群名字需要重复两次）</span></span><br><span class="line">$ ceph osd pool delete kevin kevin --yes-i-really-really-mean-it</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment">## 显示集群中pool的详细信息</span></span><br><span class="line">$ rados df</span><br><span class="line">pool name category KB objects clones degraded unfound rd rd KB wr wr KB</span><br><span class="line">data - 475764704 116155 0 0 0 0 0 116379 475764704</span><br><span class="line">metadata - 5606 21 0 0 0 0 0 314 5833</span><br><span class="line">rbd - 0 0 0 0 0 0 0 0 0</span><br><span class="line">total used 955852448 116176</span><br><span class="line">total avail 639497596</span><br><span class="line">total space 1595350044</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 给一个pool创建一个快照</span></span><br><span class="line">$ ceph osd pool mksnap data date-snap</span><br><span class="line">created pool data snap date-snap</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除pool的快照</span></span><br><span class="line">$  ceph osd pool rmsnap data date-snap</span><br><span class="line">removed pool data snap date-snap</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看data池的pg数量</span></span><br><span class="line">$  ceph osd pool get data pg_num</span><br><span class="line">pg_num: 64</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置data池的最大存储空间为100T（默认是1T)</span></span><br><span class="line">$  ceph osd pool <span class="built_in">set</span> data target_max_bytes 100000000000000</span><br><span class="line"><span class="built_in">set</span> pool 0 target_max_bytes to 100000000000000</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置data池的副本数是3</span></span><br><span class="line">$  ceph osd pool <span class="built_in">set</span> data size 3</span><br><span class="line"><span class="built_in">set</span> pool 0 size to 3</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置data池能接受写操作的最小副本为2</span></span><br><span class="line">$  ceph osd pool <span class="built_in">set</span> data min_size 2</span><br><span class="line"><span class="built_in">set</span> pool 0 min_size to 2</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看集群中所有pool的副本尺寸</span></span><br><span class="line">$ ceph osd dump | grep <span class="string">'replicated size'</span></span><br><span class="line">pool 0 <span class="string">'data'</span> replicated size 3 min_size 2 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 26 owner 0 flags hashpspool crash_replay_interval 45 target_bytes 100000000000000 stripe_width 0</span><br><span class="line">pool 1 <span class="string">'metadata'</span> replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 1 owner 0 flags hashpspool stripe_width 0</span><br><span class="line">pool 2 <span class="string">'rbd'</span> replicated size 2 min_size 1 crush_ruleset 0 object_hash rjenkins pg_num 64 pgp_num 64 last_change 1 owner 0 flags hashpspool stripe_width 0</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置一个pool的pg数量</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> data pg_num 100</span><br><span class="line"><span class="built_in">set</span> pool 0 pg_num to 100</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 设置一个pool的pgp数量</span></span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> data pgp_num 100</span><br><span class="line"><span class="built_in">set</span> pool 0 pgp_num to 100</span><br></pre></td></tr></table></figure><h2 id="rados指令"><a href="#rados指令" class="headerlink" title="rados指令"></a>rados指令</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">$ rados lspools</span><br><span class="line">rbd</span><br><span class="line">cephfs_data</span><br><span class="line">cephfs_metadata</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph集群中有多少个pool,并且每个pool容量及利用情况</span></span><br><span class="line">$ rados df</span><br><span class="line">pool name                 KB      objects       clones     degraded      unfound           rd        rd KB           wr        wr KB</span><br><span class="line">cephfs_data                0            0            0            0           0            0            0            0            0</span><br><span class="line">cephfs_metadata           21           20            0            0           0            0            0           45           36</span><br><span class="line">rbd                        0            0            0            0           0            0            0            0            0</span><br><span class="line">  total used        15833064           20</span><br><span class="line">  total avail       47044632</span><br><span class="line">  total space       62877696</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 创建一个pool,名称为kevin</span></span><br><span class="line">$ rados mkpool kevin</span><br><span class="line">successfully created pool kevin</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph pool中的ceph object （这里的object是以块形式存储的）</span></span><br><span class="line">$ rados ls -p kevin|more</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 创建一个对象object（下面的kevin是pool名称）</span></span><br><span class="line">$ rados create kevin-object -p kevin</span><br><span class="line">$ rados -p kevin ls</span><br><span class="line">kevin-object</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个对象</span></span><br><span class="line">$ rados rm kevin-object -p kevin</span><br><span class="line">$ rados -p kevin ls</span><br></pre></td></tr></table></figure><h2 id="rbd命令的用法"><a href="#rbd命令的用法" class="headerlink" title="rbd命令的用法"></a>rbd命令的用法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 查看ceph中一个pool里的所有镜像</span></span><br><span class="line">$ rbd ls kevin     <span class="comment">##kevin是一个pool名</span></span><br><span class="line"><span class="comment">## 或者</span></span><br><span class="line">$ rbd list kevin</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看ceph pool中一个镜像的信息（kevin是pool名，wangshibo是镜像名）</span></span><br><span class="line">$ rbd info -p kevin --image wangshibo</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 在kevin池中创建一个命名为wangshibo的10000M的镜像</span></span><br><span class="line">$ rbd create -p kevin --size 10000 wangshibo</span><br><span class="line">$ rbd -p kevin info wangshibo                 <span class="comment">## 查看新建的镜像的信息</span></span><br><span class="line">rbd image <span class="string">'wangshibo'</span>:</span><br><span class="line">   size 10000 MB <span class="keyword">in</span> 2500 objects</span><br><span class="line">   order 22 (4096 kB objects)</span><br><span class="line">   block_name_prefix: rb.0.1079.2ae8944a</span><br><span class="line">   format: 1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个镜像</span></span><br><span class="line">$ rbd rm -p kevin wangshibo</span><br><span class="line">Removing image: 100% complete...done.</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 调整一个镜像的尺寸（前提是wangshibo镜像已经创建并没有被删除）</span></span><br><span class="line">$ rbd resize -p kevin --size 20000 wangshibo</span><br><span class="line">Resizing image: 100% complete...done.</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看调整后的wangshibo镜像大小</span></span><br><span class="line">$ rbd -p kevin info wangshibo</span><br><span class="line">rbd image <span class="string">'wangshibo'</span>:</span><br><span class="line">   size 20000 MB <span class="keyword">in</span> 5000 objects</span><br><span class="line">   order 22 (4096 kB objects)</span><br><span class="line">   block_name_prefix: rb.0.107d.2ae8944a</span><br><span class="line">   format: 1</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 给一个镜像创建一个快照（如下，池/镜像@快照 ）</span></span><br><span class="line">$ rbd snap create kevin/wangshibo@wangshibo123</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看快照</span></span><br><span class="line">$ rbd info kevin/wangshibo@wangshibo123</span><br><span class="line">rbd image <span class="string">'wangshibo'</span>:</span><br><span class="line">   size 20000 MB <span class="keyword">in</span> 5000 objects</span><br><span class="line">   order 22 (4096 kB objects)</span><br><span class="line">   block_name_prefix: rb.0.107d.2ae8944a</span><br><span class="line">   format: 1</span><br><span class="line">   protected: False</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 查看一个镜像文件的快照</span></span><br><span class="line">$ rbd snap ls -p kevin wangshibo</span><br><span class="line">SNAPID NAME             SIZE</span><br><span class="line">     4 wangshibo123 20000 MB</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个镜像文件的一个快照</span></span><br><span class="line">$ rbd snap rm kevin/wangshibo@wangshibo123</span><br><span class="line">$ rbd snap ls -p kevin wangshibo        //wangshibo123快照已经被删除</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 如果发现不能删除显示的报错信息是此快照备写保护了，下面命令是删除写保护后再进行删除。</span></span><br><span class="line">$ rbd snap unprotect kevin/wangshibo@wangshibo123</span><br><span class="line">$ rbd snap rm kevin/wangshibo@wangshibo123</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 删除一个镜像文件的所有快照</span></span><br><span class="line">$ rbd snap purge -p kevin wangshibo</span><br><span class="line">Removing all snapshots: 100% complete...done.</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 把ceph pool中的一个镜像导出</span></span><br><span class="line">$ rbd <span class="built_in">export</span> -p kevin --image wangshibo</span><br><span class="line">Exporting image: 100% complete...done.</span><br><span class="line"> </span><br><span class="line"><span class="comment">## 把一个镜像导入ceph中 （但是直接导入是不能用的，因为没有经过openstack,openstack是看不到的）</span></span><br><span class="line">$ rbd import /root/ceph_test.img -p kevin --image wangshibo</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph的日常命令使用方法。&lt;/p&gt;
&lt;h2 id=&quot;Ceph日常操作命令&quot;&gt;&lt;a href=&quot;#Ceph日常操作命令&quot; class=&quot;headerlink&quot; title=&quot;Ceph日常操作命令&quot;&gt;&lt;/a&gt;Ceph日常操作命令&lt;/h2&gt;&lt;p&gt;查看
      
    
    </summary>
    
      <category term="Ceph日常命令使用" scheme="http://yoursite.com/categories/Ceph%E6%97%A5%E5%B8%B8%E5%91%BD%E4%BB%A4%E4%BD%BF%E7%94%A8/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph 优化总结</title>
    <link href="http://yoursite.com/2018/03/15/ceph-optimization-summary/"/>
    <id>http://yoursite.com/2018/03/15/ceph-optimization-summary/</id>
    <published>2018-03-14T16:00:00.000Z</published>
    <updated>2018-03-27T07:47:33.558Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph的优化点。</p><h2 id="硬件层面"><a href="#硬件层面" class="headerlink" title="硬件层面"></a>硬件层面</h2><ul><li><p>硬件规划</p></li><li><p>SSD选择</p></li><li><p>BIOS设置</p></li></ul><h2 id="软件层面"><a href="#软件层面" class="headerlink" title="软件层面"></a>软件层面</h2><ul><li><p>Linux OS</p></li><li><p>Ceph Configurations</p></li><li><p>PG Number调整</p></li><li><p>CRUSH Map</p></li><li><p>其他因素</p></li></ul><h2 id="硬件优化"><a href="#硬件优化" class="headerlink" title="硬件优化"></a>硬件优化</h2><h3 id="硬件规划"><a href="#硬件规划" class="headerlink" title="硬件规划"></a>硬件规划</h3><ul><li>Processor<br>ceph-osd进程在运行过程中会消耗CPU资源，所以一般会为每一个ceph-osd进程绑定一个CPU核上。当然如果你使用EC方式，可能需要更多的CPU资源。</li></ul><p>ceph-mon进程并不十分消耗CPU资源，所以不必为ceph-mon进程预留过多的CPU资源。</p><p>ceph-msd也是非常消耗CPU资源的，所以需要提供更多的CPU资源。</p><ul><li><p>内存<br>ceph-mon和ceph-mds需要2G内存，每个ceph-osd进程需要1G内存，当然2G更好。</p></li><li><p>网络规划<br>万兆网络现在基本上是跑Ceph必备的，网络规划上，也尽量考虑分离cilent和cluster网络。</p></li></ul><h3 id="SSD选择"><a href="#SSD选择" class="headerlink" title="SSD选择"></a>SSD选择</h3><p>硬件的选择也直接决定了Ceph集群的性能，从成本考虑，一般选择SATA SSD作为Journal，<a href="https://www.intel.com/content/www/us/en/products/memory-storage/solid-state-drives/data-center-ssds/dc-s3500-series.html" target="_blank" rel="noopener">Intel® SSD DC S3500 Series</a>基本是目前看到的方案中的首选。400G的规格4K随机写可以达到11000 IOPS。如果在预算足够的情况下，推荐使用PCIE SSD，性能会得到进一步提升，但是由于Journal在向数据盘写入数据时Block后续请求，所以Journal的加入并未呈现出想象中的性能提升，但是的确会对Latency有很大的改善。</p><p>如何确定你的SSD是否适合作为SSD Journal，可以参考<a href="http://www.sebastien-han.fr/blog/2014/10/10/ceph-how-to-test-if-your-ssd-is-suitable-as-a-journal-device/" target="_blank" rel="noopener">SÉBASTIEN HAN的Ceph: How to Test if Your SSD Is Suitable as a Journal Device?</a>，这里面他也列出了常见的SSD的测试结果，从结果来看SATA SSD中，Intel S3500性能表现最好。</p><h3 id="BIOS设置"><a href="#BIOS设置" class="headerlink" title="BIOS设置"></a>BIOS设置</h3><ul><li><p>Hyper-Threading(HT)<br>基本做云平台的，VT和HT打开都是必须的，超线程技术(HT)就是利用特殊的硬件指令，把两个逻辑内核模拟成两个物理芯片，让单个处理器都能使用线程级并行计算，进而兼容多线程操作系统和软件，减少了CPU的闲置时间，提高的CPU的运行效率。</p></li><li><p>关闭节能<br>关闭节能后，对性能还是有所提升的，所以坚决调整成性能型(Performance)。当然也可以在操作系统级别进行调整，详细的调整过程请<a href="http://www.servernoobs.com/avoiding-cpu-speed-scaling-in-modern-linux-distributions-running-cpu-at-full-speed-tips/" target="_blank" rel="noopener">参考链接</a>，但是不知道是不是由于BIOS已经调整的缘故，所以在CentOS 6.6上并没有发现相关的设置。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> CPUFREQ <span class="keyword">in</span> /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor; <span class="keyword">do</span> [ -f <span class="variable">$CPUFREQ</span> ] || <span class="built_in">continue</span>; <span class="built_in">echo</span> -n performance &gt; <span class="variable">$CPUFREQ</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure></li><li><p><a href="http://www.ibm.com/developerworks/cn/linux/l-numa/" target="_blank" rel="noopener">NUMA</a><br>简单来说，NUMA思路就是将内存和CPU分割为多个区域，每个区域叫做NODE,然后将NODE高速互联。 node内cpu与内存访问速度快于访问其他node的内存，NUMA可能会在<a href="http://lists.ceph.com/pipermail/ceph-users-ceph.com/2013-December/036211.html" target="_blank" rel="noopener">某些情况</a>下影响ceph-osd。解决的方案，一种是通过BIOS关闭NUMA，另外一种就是通过cgroup将ceph-osd进程与某一个CPU Core以及同一NODE下的内存进行绑定。但是第二种看起来更麻烦，所以一般部署的时候可以在系统层面关闭NUMA。CentOS系统下，通过修改/etc/grub.conf文件，添加numa=off来关闭NUMA。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">kernel /vmlinuz-2.6.32-504.12.2.el6.x86_64 ro root=UUID=870d47f8-0357-4a32-909f-74173a9f0633 rd_NO_LUKS rd_NO_LVM LANG=en_US.UTF-8 rd_NO_MD SYSFONT=latarcyrheb-sun16 crashkernel=auto  KEYBOARDTYPE=pc KEYTABLE=us rd_NO_DM   biosdevname=0 numa=off</span><br></pre></td></tr></table></figure></li></ul><h2 id="软件优化"><a href="#软件优化" class="headerlink" title="软件优化"></a>软件优化</h2><h3 id="Linux-OS"><a href="#Linux-OS" class="headerlink" title="Linux OS"></a>Linux OS</h3><ul><li><p>Kernel pid max</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> 4194303 &gt; /proc/sys/kernel/pid_max</span><br></pre></td></tr></table></figure></li><li><p>Jumbo frames, 交换机端需要支持该功能，系统网卡设置才有效果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ifconfig eth0 mtu 9000</span><br></pre></td></tr></table></figure></li></ul><p>永久设置<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"MTU=9000"</span> | tee -a /etc/sysconfig/network-script/ifcfg-eth0</span><br><span class="line">$ /etc/init.d/networking restart</span><br></pre></td></tr></table></figure></p><ul><li>read_ahead, 通过数据预读并且记载到随机访问内存方式提高磁盘读操作，查看默认值<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat /sys/block/sda/queue/read_ahead_kb</span><br></pre></td></tr></table></figure></li></ul><p>根据一些Ceph的公开分享，8192是比较理想的值<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"8192"</span> &gt; /sys/block/sda/queue/read_ahead_kb</span><br></pre></td></tr></table></figure></p><ul><li><p>swappiness, 主要控制系统对swap的使用，这个参数的调整最先见于UnitedStack公开的文档中，猜测调整的原因主要是使用swap会影响系统的性能。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"vm.swappiness = 0"</span> | tee -a /etc/sysctl.conf</span><br></pre></td></tr></table></figure></li><li><p>I/O Scheduler，关于I/O Scheculder的调整网上已经有很多资料，这里不再赘述，简单说SSD要用noop，SATA/SAS使用deadline。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">echo</span> <span class="string">"deadline"</span> &gt; /sys/block/sd[x]/queue/scheduler</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"noop"</span> &gt; /sys/block/sd[x]/queue/scheduler</span><br></pre></td></tr></table></figure></li><li><p>cgroup<br>这方面的文章好像比较少，昨天在和Ceph社区交流过程中，Jan Schermer说准备把生产环境中的一些脚本贡献出来，但是暂时还没有，他同时也列举了一些使用cgroup进行隔离的<a href="https://www.mail-archive.com/ceph-users@lists.ceph.com/msg21111.html" target="_blank" rel="noopener">原因</a>。</p><ul><li>不在process和thread在不同的core上移动(更好的缓存利用)</li><li>减少NUMA的影响</li><li>网络和存储控制器影响 - 较小</li><li>通过限制cpuset来限制Linux调度域(不确定是不是重要但是是最佳实践)</li><li>如果开启了HT，可能会造成OSD在thread1上，KVM在thread2上，并且是同一个core。Core的延迟和性能取决于其他一个线程做什么。</li></ul></li></ul><h3 id="Ceph-Configurations"><a href="#Ceph-Configurations" class="headerlink" title="Ceph Configurations"></a>Ceph Configurations</h3><ul><li>[global]</li></ul><table><thead><tr><th>参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td>public network</td><td style="text-align:center">客户端访问网络</td><td style="text-align:center">-</td><td style="text-align:center">192.168.100.0/24</td></tr><tr><td>cluster network</td><td style="text-align:center">集群网络</td><td style="text-align:center">-</td><td style="text-align:center">192.168.1.0/24</td></tr><tr><td>max open files</td><td style="text-align:center">如果设置了该选项，Ceph会设置系统的max open fds</td><td style="text-align:center">0</td><td style="text-align:center">131072</td></tr></tbody></table><p>查看系统最大文件打开数可以使用命令<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cat /proc/sys/fs/file-max</span><br></pre></td></tr></table></figure></p><ul><li>[osd] - filestore</li></ul><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td style="text-align:center">filestore xattr use omap</td><td style="text-align:center">为XATTRS使用object map，EXT4文件系统时使用，XFS或者btrfs也可以使用</td><td style="text-align:center">false</td><td style="text-align:center">true</td></tr><tr><td style="text-align:center">filestore max sync interval</td><td style="text-align:center">从日志到数据盘最大同步间隔(seconds)</td><td style="text-align:center">5</td><td style="text-align:center">15</td></tr><tr><td style="text-align:center">filestore min sync interval</td><td style="text-align:center">从日志到数据盘最小同步间隔(seconds)</td><td style="text-align:center">0.1</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center">filestore queue max ops</td><td style="text-align:center">数据盘最大接受的操作数</td><td style="text-align:center">500</td><td style="text-align:center">25000</td></tr><tr><td style="text-align:center">filestore queue max bytes</td><td style="text-align:center">数据盘一次操作最大字节数(bytes)</td><td style="text-align:center">100 &lt;&lt; 20</td><td style="text-align:center">10485760</td></tr><tr><td style="text-align:center">filestore queue committing max ops</td><td style="text-align:center">数据盘能够commit的操作数</td><td style="text-align:center">500</td><td style="text-align:center">5000</td></tr><tr><td style="text-align:center">filestore queue committing max bytes</td><td style="text-align:center">数据盘能够commit的最大字节数(bytes)</td><td style="text-align:center">100 &lt;&lt; 20</td><td style="text-align:center">10485760000</td></tr><tr><td style="text-align:center">filestore op threads</td><td style="text-align:center">并发文件系统操作数</td><td style="text-align:center">2</td><td style="text-align:center">32</td></tr></tbody></table><ul><li>调整omap的原因主要是EXT4文件系统默认仅有4K</li><li>filestore queue相关的参数对于性能影响很小，参数调整不会对性能优化有本质上提升</li></ul><ul><li>[osd] - journal</li></ul><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td style="text-align:center">osd journal size</td><td style="text-align:center">OSD日志大小(MB)</td><td style="text-align:center">5120</td><td style="text-align:center">20000</td></tr><tr><td style="text-align:center">journal max write bytes</td><td style="text-align:center">journal一次性写入的最大字节数(bytes)</td><td style="text-align:center">10 &lt;&lt; 20</td><td style="text-align:center">1073714824</td></tr><tr><td style="text-align:center">journal max write entries</td><td style="text-align:center">journal一次性写入的最大记录数</td><td style="text-align:center">100</td><td style="text-align:center">10000</td></tr><tr><td style="text-align:center">journal queue max ops</td><td style="text-align:center">journal一次性最大在队列中的操作数</td><td style="text-align:center">500</td><td style="text-align:center">50000</td></tr><tr><td style="text-align:center">journal queue max bytes</td><td style="text-align:center">journal一次性最大在队列中的字节数(bytes)</td><td style="text-align:center">10 &lt;&lt; 20</td><td style="text-align:center">10485760000</td></tr></tbody></table><ul><li>Ceph OSD Daemon stops writes and synchronizes the journal with the filesystem, allowing Ceph OSD Daemons to trim operations from the journal and reuse the space.</li><li>上面这段话的意思就是，Ceph OSD进程在往数据盘上刷数据的过程中，是停止写操作的。</li></ul><ul><li>[osd] - osd config tuning</li></ul><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td style="text-align:center">osd max write size</td><td style="text-align:center">OSD一次可写入的最大值(MB)</td><td style="text-align:center">90</td><td style="text-align:center">512</td></tr><tr><td style="text-align:center">osd client message size cap</td><td style="text-align:center">客户端允许在内存中的最大数据(bytes)</td><td style="text-align:center">524288000</td><td style="text-align:center">2147483648</td></tr><tr><td style="text-align:center">osd deep scrub stride</td><td style="text-align:center">在Deep Scrub时候允许读取的字节数(bytes)</td><td style="text-align:center">524288</td><td style="text-align:center">131072</td></tr><tr><td style="text-align:center">osd op threads</td><td style="text-align:center">OSD进程操作的线程数</td><td style="text-align:center">2</td><td style="text-align:center">8</td></tr><tr><td style="text-align:center">osd disk threads</td><td style="text-align:center">OSD密集型操作例如恢复和Scrubbing时的线程</td><td style="text-align:center">1</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">osd map cache size</td><td style="text-align:center">保留OSD Map的缓存(MB)</td><td style="text-align:center">500</td><td style="text-align:center">1024</td></tr><tr><td style="text-align:center">osd map cache bl size</td><td style="text-align:center">OSD进程在内存中的OSD Map缓存(MB)</td><td style="text-align:center">50</td><td style="text-align:center">128</td></tr><tr><td style="text-align:center">osd mount options xfs</td><td style="text-align:center">Ceph OSD xfs Mount选项</td><td style="text-align:center">rw,noatime,inode64</td><td style="text-align:center">rw,noexec,nodev,noatime,nodiratime,nobarrier</td></tr></tbody></table><ul><li>增加osd op threads和disk threads会带来额外的CPU开销</li></ul><ul><li>[osd] - recovery tuning</li></ul><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td style="text-align:center">osd recovery op priority</td><td style="text-align:center">恢复操作优先级，取值1-63，值越高占用资源越高</td><td style="text-align:center">10</td><td style="text-align:center">4</td></tr><tr><td style="text-align:center">osd recovery max active</td><td style="text-align:center">同一时间内活跃的恢复请求数</td><td style="text-align:center">15</td><td style="text-align:center">10</td></tr><tr><td style="text-align:center">osd max backfills</td><td style="text-align:center">一个OSD允许的最大backfills数</td><td style="text-align:center">10</td><td style="text-align:center">4</td></tr></tbody></table><ul><li>[osd] - client tuning</li></ul><table><thead><tr><th style="text-align:center">参数名</th><th style="text-align:center">描述</th><th style="text-align:center">默认值</th><th style="text-align:center">建议值</th></tr></thead><tbody><tr><td style="text-align:center">rbd cache</td><td style="text-align:center">RBD缓存</td><td style="text-align:center">true</td><td style="text-align:center">true</td></tr><tr><td style="text-align:center">rbd cache size</td><td style="text-align:center">RBD缓存大小(bytes)</td><td style="text-align:center">33554432</td><td style="text-align:center">268435456</td></tr><tr><td style="text-align:center">rbd cache max dirty</td><td style="text-align:center">缓存为write-back时允许的最大dirty字节数(bytes)，如果为0，使用write-through</td><td style="text-align:center">25165824</td><td style="text-align:center">134217728</td></tr><tr><td style="text-align:center">rbd cache max dirty age</td><td style="text-align:center">在被刷新到存储盘前dirty数据存在缓存的时间(seconds)</td><td style="text-align:center">1</td><td style="text-align:center">5</td></tr></tbody></table><p>关闭Debug</p><h3 id="PG-Number"><a href="#PG-Number" class="headerlink" title="PG Number"></a>PG Number</h3><p>PG和PGP数量一定要根据OSD的数量进行调整，计算公式如下，但是最后算出的结果一定要接近或者等于一个2的指数</p><p><img src="/images/ceph-14.png" alt="ceph-14"></p><table><thead><tr><th style="text-align:center">名称</th><th style="text-align:center">说明</th><th style="text-align:left">备注</th></tr></thead><tbody><tr><td style="text-align:center">PoolPGCount</td><td style="text-align:center">单个Pool的PG数量</td><td style="text-align:left"></td></tr><tr><td style="text-align:center">Target PGs per OSD</td><td style="text-align:center">每个OSD的PGS数量</td><td style="text-align:left">1.如果未来集群的OSD数量基本不再增长，Target PGs per OSD=100<br>2.如果未来集群的OSD数量可能增长到目前规模的2倍以内，Target PGs per OSD=200<br>3.如果未来集群的OSD数量可能增长到大于目前规模的2倍且小于3倍，Target PGs per OSD=300。</td></tr><tr><td style="text-align:center">OSD #</td><td style="text-align:center">集群OSD的总数，默认是全部OSD数量</td><td style="text-align:left">如果通过CRUSH rules进行了SSD和SATA设备的规则拆分(比如SSD和SATA划分两个zone)，需要单独填写对应rule的数量</td></tr><tr><td style="text-align:center">%Data</td><td style="text-align:center">Pool占用所在OSD总容量的百分比(预估值)</td><td style="text-align:left">(自定义百分比)</td></tr><tr><td style="text-align:center">Size</td><td style="text-align:center">每个pool的replicas size，默认是3</td><td style="text-align:left">如果使用Erasure Coded Pools简称EC pool，Pool Size=k+m</td></tr></tbody></table><p><strong>结果：</strong><br>１．如果上述计算的值小于（OSD＃）/（Size）的值，则将该值更新为（OSD＃）/（Size）的值。 这是为了通过为每个池分配至少一个主或辅助PG到每个OSD来确保均匀加载/数据分发。</p><p>２．然后将输出值四舍五入为最接近的2的幂。</p><p>提示：最接近的2的幂提供了CRUSH算法的效率的边际改进。</p><p>３．如果最接近的2的幂大于原始值的25％以上，则使用下一个更高的2的幂。</p><p>例如15个OSD，副本数为3的情况下，根据公式计算的结果应该为500，最接近512，所以需要设定该pool(volumes)的pg_num和pgp_num都为512.<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes pg_num 512</span><br><span class="line">$ ceph osd pool <span class="built_in">set</span> volumes pgp_num 512</span><br></pre></td></tr></table></figure></p><h3 id="CRUSH-Map"><a href="#CRUSH-Map" class="headerlink" title="CRUSH Map"></a>CRUSH Map</h3><p>CRUSH是一个非常灵活的方式，CRUSH MAP的调整取决于部署的具体环境，这个可能需要根据具体情况进行分析，这里面就不再赘述了。</p><h3 id="其他因素的影响"><a href="#其他因素的影响" class="headerlink" title="其他因素的影响"></a>其他因素的影响</h3><p>在2015年的Ceph Day上，海云捷迅在调优过程中分享过一个由于在集群中存在一个性能不好的磁盘，导致整个集群性能下降的case。通过osd perf可以提供磁盘latency的状况，同时在运维过程中也可以作为监控的一个重要指标，很明显在下面的例子中，OSD 8的磁盘延时较长，所以需要考虑将该OSD剔除出集群：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd perf</span><br><span class="line">osd fs_commit_latency(ms) fs_apply_latency(ms)</span><br><span class="line">  0                    14                   17</span><br><span class="line">  1                    14                   16</span><br><span class="line">  2                    10                   11</span><br><span class="line">  3                     4                    5</span><br><span class="line">  4                    13                   15</span><br><span class="line">  5                    17                   20</span><br><span class="line">  6                    15                   18</span><br><span class="line">  7                    14                   16</span><br><span class="line">  8                   299                  329</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ cat /etc/ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = 059f27e8-a23f-4587-9033-3e3679d03b31</span><br><span class="line">mon_host = 10.10.20.102, 10.10.20.101, 10.10.20.100</span><br><span class="line">auth cluster required = cephx</span><br><span class="line">auth service required = cephx</span><br><span class="line">auth client required = cephx</span><br><span class="line">osd pool default size = 3</span><br><span class="line">osd pool default min size = 1</span><br><span class="line"></span><br><span class="line">public network = 10.10.20.0/24</span><br><span class="line">cluster network = 10.10.20.0/24</span><br><span class="line"></span><br><span class="line">max open files = 131072</span><br><span class="line"></span><br><span class="line">[mon]</span><br><span class="line">mon data = /var/lib/ceph/mon/ceph-<span class="variable">$id</span></span><br><span class="line"></span><br><span class="line">[osd]</span><br><span class="line">osd data = /var/lib/ceph/osd/ceph-<span class="variable">$id</span></span><br><span class="line">osd journal size = 20000</span><br><span class="line">osd mkfs <span class="built_in">type</span> = xfs</span><br><span class="line">osd mkfs options xfs = -f</span><br><span class="line"></span><br><span class="line">filestore xattr use omap = <span class="literal">true</span></span><br><span class="line">filestore min sync interval = 10</span><br><span class="line">filestore max sync interval = 15</span><br><span class="line">filestore queue max ops = 25000</span><br><span class="line">filestore queue max bytes = 10485760</span><br><span class="line">filestore queue committing max ops = 5000</span><br><span class="line">filestore queue committing max bytes = 10485760000</span><br><span class="line"></span><br><span class="line">journal max write bytes = 1073714824</span><br><span class="line">journal max write entries = 10000</span><br><span class="line">journal queue max ops = 50000</span><br><span class="line">journal queue max bytes = 10485760000</span><br><span class="line"></span><br><span class="line">osd max write size = 512</span><br><span class="line">osd client message size <span class="built_in">cap</span> = 2147483648</span><br><span class="line">osd deep scrub stride = 131072</span><br><span class="line">osd op threads = 8</span><br><span class="line">osd disk threads = 4</span><br><span class="line">osd map cache size = 1024</span><br><span class="line">osd map cache bl size = 128</span><br><span class="line">osd mount options xfs = <span class="string">"rw,noexec,nodev,noatime,nodiratime,nobarrier"</span></span><br><span class="line">osd recovery op priority = 4</span><br><span class="line">osd recovery max active = 10</span><br><span class="line">osd max backfills = 4</span><br><span class="line"></span><br><span class="line">[client]</span><br><span class="line">rbd cache = <span class="literal">true</span></span><br><span class="line">rbd cache size = 268435456</span><br><span class="line">rbd cache max dirty = 134217728</span><br><span class="line">rbd cache max dirty age = 5</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph的优化点。&lt;/p&gt;
&lt;h2 id=&quot;硬件层面&quot;&gt;&lt;a href=&quot;#硬件层面&quot; class=&quot;headerlink&quot; title=&quot;硬件层面&quot;&gt;&lt;/a&gt;硬件层面&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;硬件规划&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;
      
    
    </summary>
    
      <category term="Ceph 优化总结" scheme="http://yoursite.com/categories/Ceph-%E4%BC%98%E5%8C%96%E6%80%BB%E7%BB%93/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Ceph Client挂载方法</title>
    <link href="http://yoursite.com/2018/03/14/ceph-client-mount/"/>
    <id>http://yoursite.com/2018/03/14/ceph-client-mount/</id>
    <published>2018-03-13T16:00:00.000Z</published>
    <updated>2018-03-28T06:37:05.059Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph的客户端挂载记录。</p><h2 id="Client挂载"><a href="#Client挂载" class="headerlink" title="Client挂载"></a>Client挂载</h2><p>client挂载方式有两种：内核kernal方式和fuse方式。</p><p>特别需要注意的是：<br>client挂载前需要在client节点安装ceph，详细请参考<a href="abc">abc</a></p><ul><li>系统环境：CentOS 7</li></ul><h3 id="安装ceph-fuse"><a href="#安装ceph-fuse" class="headerlink" title="安装ceph-fuse"></a>安装<code>ceph-fuse</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y epel-release</span><br><span class="line">$ yum install -y ceph-fuse</span><br></pre></td></tr></table></figure><h3 id="创建挂载目录"><a href="#创建挂载目录" class="headerlink" title="创建挂载目录"></a>创建挂载目录</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir /cephfs</span><br></pre></td></tr></table></figure><h3 id="复制配置文件"><a href="#复制配置文件" class="headerlink" title="复制配置文件"></a>复制配置文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 将ceph配置文件ceph.conf从管理节点copy到client节点</span></span><br><span class="line">$ rsync -e <span class="string">"ssh -p22"</span> -avp root@192.168.0.200:/etc/ceph/ceph.conf /etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 将ceph的ceph.client.admin.keyring从管理节点copy到client节点</span></span><br><span class="line">$ rsync -e <span class="string">"ssh -p22"</span> -avp root@192.168.0.200:/etc/ceph/ceph.client.admin.keyring /etc/ceph/</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看ceph授权</span></span><br><span class="line">$ ceph auth list</span><br><span class="line">installed auth entries:</span><br><span class="line"> </span><br><span class="line">mds.ceph-node1</span><br><span class="line">   key: AQDV/3lasqfVARAAa/eOiCbWQ/ccFLkVD/9UCQ==</span><br><span class="line">   caps: [mds] allow</span><br><span class="line">   caps: [mon] allow profile mds</span><br><span class="line">   caps: [osd] allow rwx</span><br><span class="line">osd.0</span><br><span class="line">   key: AQAB9XlapzMfOxAAfKLo3Z6FZMyqWUW9F5FU4Q==</span><br><span class="line">   caps: [mon] allow profile osd</span><br><span class="line">   caps: [osd] allow *</span><br><span class="line">osd.1</span><br><span class="line">   key: AQAL9XlaT06zLRAABQr9kweLPiHL2Icdgj8YsA==</span><br><span class="line">   caps: [mon] allow profile osd</span><br><span class="line">   caps: [osd] allow *</span><br><span class="line">osd.2</span><br><span class="line">   key: AQAV9XladRLjORAATbxZHpD34ztqJJ3vFGkVOg==</span><br><span class="line">   caps: [mon] allow profile osd</span><br><span class="line">   caps: [osd] allow *</span><br><span class="line">client.admin</span><br><span class="line">   key: AQCnH3lalDbeHhAAC6y7YERqDDGL+f8S+sP1bw==</span><br><span class="line">   caps: [mds] allow</span><br><span class="line">   caps: [mon] allow *</span><br><span class="line">   caps: [osd] allow *</span><br><span class="line">client.bootstrap-mds</span><br><span class="line">   key: AQCoH3laUqT5JRAAiVMRF8ueWQXfgQjIPalNMQ==</span><br><span class="line">   caps: [mon] allow profile bootstrap-mds</span><br><span class="line">client.bootstrap-osd</span><br><span class="line">   key: AQCnH3laBzU7MBAAnreE1I/0Egh8gvMseoy93w==</span><br><span class="line">   caps: [mon] allow profile bootstrap-osd</span><br><span class="line">client.bootstrap-rgw</span><br><span class="line">   key: AQCoH3laziwXDxAAenU6cqw0gRcQ3Y0JtygtPQ==</span><br><span class="line">   caps: [mon] allow profile bootstrap-rgw</span><br></pre></td></tr></table></figure><p>若出现<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"/usr/bin/ceph"</span>, line 118, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    import rados</span><br><span class="line">ImportError: No module named rados</span><br></pre></td></tr></table></figure></p><p>类似缺少包的报错，请用<code>root</code>用户修改<code>/usr/bin/ceph</code>的文件头<code>#!/usr/bin/env python</code>改为<code>#!/usr/bin/python2.7</code> (CentOS 6请改为<code>#!/usr/bin/python2.6</code>)</p><p>挂载到指定目录<br>将ceph挂载到/cephfs</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-fuse -m 192.168.0.200:6789 /cephfs</span><br><span class="line"></span><br><span class="line">$ df -h</span><br><span class="line">Filesystem      Size  Used Avail Use% Mounted on</span><br><span class="line">/dev/vda2        48G  1.5G   45G   4% /</span><br><span class="line">tmpfs           1.9G     0  1.9G   0% /dev/shm</span><br><span class="line">/dev/vda1       190M   36M  145M   20% /boot</span><br><span class="line">/dev/vda5        47G   52M   44G   1% /home</span><br><span class="line">ceph-fuse       300G   16G  285G   6% /cephfs</span><br></pre></td></tr></table></figure><p>如上可以看到，clinet节点已经将三个osd存储节点（每个20G，一共30G）的ceph存储挂载上了，总共60G！<br>可以在/cephfs下存放数据，当ceph的部分osd节点挂掉后，不会影响client对ceph存储数据的读取操作！</p><p>取消挂载<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ umount /cephfs</span><br></pre></td></tr></table></figure></p><h2 id="直接挂载"><a href="#直接挂载" class="headerlink" title="直接挂载"></a>直接挂载</h2><p>首先也是要安装好ceph，详细参考上述过程</p><p>从管理节点中查看ceph client keyring信息<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -iu ceph</span><br><span class="line">$ <span class="built_in">cd</span> /home/ceph/ceph-cluster</span><br><span class="line">$ cat ceph.client.admin.keyring</span><br><span class="line">[client.admin]</span><br><span class="line">key = AQBETrlakTy4LBAA4yMZc1q3nQHbUBD+X2KTmg==</span><br><span class="line">caps mds = <span class="string">"allow *"</span></span><br><span class="line">caps mon = <span class="string">"allow *"</span></span><br><span class="line">caps osd = <span class="string">"allow *"</span></span><br></pre></td></tr></table></figure></p><p>获取<code>ceph.client.admin.keyring</code>的<code>key</code>的值<code>AQBETrlakTy4LBAA4yMZc1q3nQHbUBD+X2KTmg==</code></p><p>复制到客户端中<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ cat /root/admin.secret</span><br><span class="line">AQBETrlakTy4LBAA4yMZc1q3nQHbUBD+X2KTmg==</span><br><span class="line"></span><br><span class="line"><span class="comment">## 开始挂载</span></span><br><span class="line">$ mount -t ceph 192.168.0.200:6789:/ /cephfs -o name=admin,secretfile=/root/admin.secret  </span><br><span class="line">$ df -h</span><br><span class="line">Filesystem                  Size  Used  Avail Use% Mounted on</span><br><span class="line">/dev/vda2                   48G   1.5G  45G   4%   / </span><br><span class="line">tmpfs                       1.9G  0     1.9G  0%   /dev/shm</span><br><span class="line">/dev/vda1                   190M  36M   145M  20%  /boot</span><br><span class="line">/dev/vda5                   47G   52M   44G   1%   /home</span><br><span class="line">192.168.0.200:6789:/        300G   16G   285G 6%  /cephfs</span><br><span class="line"></span><br><span class="line"><span class="comment">## 取消挂载</span></span><br><span class="line">$ umount /cephfs</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph的客户端挂载记录。&lt;/p&gt;
&lt;h2 id=&quot;Client挂载&quot;&gt;&lt;a href=&quot;#Client挂载&quot; class=&quot;headerlink&quot; title=&quot;Client挂载&quot;&gt;&lt;/a&gt;Client挂载&lt;/h2&gt;&lt;p&gt;client挂载方式有
      
    
    </summary>
    
      <category term="Ceph Client挂载方法" scheme="http://yoursite.com/categories/Ceph-Client%E6%8C%82%E8%BD%BD%E6%96%B9%E6%B3%95/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>分布式存储系统Ceph部署过程</title>
    <link href="http://yoursite.com/2018/03/14/ceph-installation/"/>
    <id>http://yoursite.com/2018/03/14/ceph-installation/</id>
    <published>2018-03-13T16:00:00.000Z</published>
    <updated>2018-03-28T05:09:56.250Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph部署过程记录。</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><p>这里我只是测试环境，所以使用一个监控节点，两个存储节点，具体如下：</p><table><thead><tr><th style="text-align:center">ip地址(双网卡)</th><th style="text-align:center">主机名</th><th style="text-align:center">ceph磁盘</th><th style="text-align:center">系统</th><th style="text-align:center">备注</th></tr></thead><tbody><tr><td style="text-align:center">192.168.0.200/172.16.0.100</td><td style="text-align:center">ceph-node1</td><td style="text-align:center">100G</td><td style="text-align:center">CentOS Linux release 7.4.1708 (Core)</td><td style="text-align:center">作为mds、mon、osd0</td></tr><tr><td style="text-align:center">192.168.0.201/172.16.0.101</td><td style="text-align:center">ceph-node2</td><td style="text-align:center">100G</td><td style="text-align:center">CentOS Linux release 7.4.1708 (Core)</td><td style="text-align:center">作为osd1</td></tr><tr><td style="text-align:center">192.168.0.202/172.16.0.102</td><td style="text-align:center">ceph-node3</td><td style="text-align:center">100G</td><td style="text-align:center">CentOS Linux release 7.4.1708 (Core)</td><td style="text-align:center">作为osd2</td></tr><tr><td style="text-align:center">192.168.0.203/172.16.0.103</td><td style="text-align:center">ceph-client</td><td style="text-align:center">挂载点：/cephfs</td><td style="text-align:center">CentOS Linux release 7.4.1708 (Core)</td><td style="text-align:center">ceph客户端</td></tr></tbody></table><p>Ceph的文件系统作为一个目录挂载到客户端cephclient的/cephfs目录下，可以像操作普通目录一样对此目录进行操作。</p><h2 id="安装前准备-每个节点都需要做以下操作"><a href="#安装前准备-每个节点都需要做以下操作" class="headerlink" title="安装前准备(每个节点都需要做以下操作)"></a>安装前准备(每个节点都需要做以下操作)</h2><p>分别在ceph的三个节点机（ceph-node1、ceph-node2、ceph-node3）上添加hosts<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/hosts</span><br><span class="line">127.0.0.1   localhost localhost.localdomain localhost4 localhost4.localdomain4</span><br><span class="line">::1         localhost localhost.localdomain localhost6 localhost6.localdomain6</span><br><span class="line">192.168.0.200 ceph-node1</span><br><span class="line">192.168.0.201 ceph-node2</span><br><span class="line">192.168.0.202 ceph-node3</span><br><span class="line">192.168.0.203 ceph-client</span><br></pre></td></tr></table></figure></p><h3 id="关闭防火墙和selinux"><a href="#关闭防火墙和selinux" class="headerlink" title="关闭防火墙和selinux"></a>关闭防火墙和<code>selinux</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sed -i <span class="string">'s/SELINUX=.*/SELINUX=disabled/'</span> /etc/selinux/config</span><br><span class="line">$ setenforce 0</span><br><span class="line">$ systemctl stop firewalld</span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld</span><br></pre></td></tr></table></figure><h3 id="创建用户ceph"><a href="#创建用户ceph" class="headerlink" title="创建用户ceph"></a>创建用户<code>ceph</code></h3><p>分别在ceph的三个节点机（ceph-node1、ceph-node2、ceph-node3）上创建用户ceph，密码统一设置为ceph</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ adduser ceph</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"ceph"</span>|passwd --stdin ceph</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"ceph ALL = (root) NOPASSWD:ALL"</span> | sudo tee /etc/sudoers.d/ceph</span><br><span class="line">$ chmod 0440 /etc/sudoers.d/ceph</span><br></pre></td></tr></table></figure><h3 id="配置NTP服务"><a href="#配置NTP服务" class="headerlink" title="配置NTP服务"></a>配置<code>NTP</code>服务</h3><p>由于Ceph对时间比较敏感，所以建议使用ntp服务保持三台服务器的时间保持同步</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y ntp</span><br><span class="line">$ vi /etc/ntp.conf</span><br><span class="line"><span class="comment"># For more information about this file, see the man pages</span></span><br><span class="line"><span class="comment"># ntp.conf(5), ntp_acc(5), ntp_auth(5), ntp_clock(5), ntp_misc(5), ntp_mon(5).</span></span><br><span class="line"> </span><br><span class="line">driftfile /var/lib/ntp/drift</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Permit time synchronization with our time source, but do not</span></span><br><span class="line"><span class="comment"># permit the source to query or modify the service on this system.</span></span><br><span class="line">restrict default nomodify notrap nopeer noquery</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Permit all access over the loopback interface.  This could</span></span><br><span class="line"><span class="comment"># be tightened as well, but to do so would effect some of</span></span><br><span class="line"><span class="comment"># the administrative functions.</span></span><br><span class="line">restrict 127.0.0.1</span><br><span class="line">restrict ::1</span><br><span class="line"> </span><br><span class="line"><span class="comment"># 允许1-4网段的服务器来校时，不允许客户端来修改，登录ntp服务器 </span></span><br><span class="line">restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">restrict 192.168.2.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">restrict 192.168.3.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line">restrict 192.168.4.0 mask 255.255.255.0 nomodify notrap</span><br><span class="line"> </span><br><span class="line"> </span><br><span class="line"><span class="comment"># Hosts on local network are less restricted.</span></span><br><span class="line"><span class="comment">#restrict 192.168.1.0 mask 255.255.255.0 nomodify notrap</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Use public servers from the pool.ntp.org project.</span></span><br><span class="line"><span class="comment"># Please consider joining the pool (http://www.pool.ntp.org/join.html).</span></span><br><span class="line"><span class="comment">#server 0.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 1.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 2.centos.pool.ntp.org iburst</span></span><br><span class="line"><span class="comment">#server 3.centos.pool.ntp.org iburst</span></span><br><span class="line"> </span><br><span class="line">server time1.aliyun.com</span><br><span class="line">server time2.aliyun.com</span><br><span class="line">server time3.aliyun.com</span><br><span class="line">server time4.aliyun.com</span><br><span class="line">server time5.aliyun.com</span><br><span class="line">server time6.aliyun.com</span><br><span class="line">server time7.aliyun.com</span><br><span class="line"> </span><br><span class="line"><span class="comment">#broadcast 192.168.1.255 autokey        # broadcast server</span></span><br><span class="line"><span class="comment">#broadcastclient                        # broadcast client</span></span><br><span class="line"><span class="comment">#broadcast 224.0.1.1 autokey            # multicast server</span></span><br><span class="line"><span class="comment">#multicastclient 224.0.1.1              # multicast client</span></span><br><span class="line"><span class="comment">#manycastserver 239.255.254.254         # manycast server</span></span><br><span class="line"><span class="comment">#manycastclient 239.255.254.254 autokey # manycast client</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Enable public key cryptography.</span></span><br><span class="line"><span class="comment">#crypto</span></span><br><span class="line"> </span><br><span class="line">includefile /etc/ntp/crypto/pw</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Key file containing the keys and key identifiers used when operating</span></span><br><span class="line"><span class="comment"># with symmetric key cryptography. </span></span><br><span class="line">keys /etc/ntp/keys</span><br><span class="line"> </span><br><span class="line"><span class="comment"># Specify the key identifiers which are trusted.</span></span><br><span class="line"><span class="comment">#trustedkey 4 8 42</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Specify the key identifier to use with the ntpdc utility.</span></span><br><span class="line"><span class="comment">#requestkey 8</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Specify the key identifier to use with the ntpq utility.</span></span><br><span class="line"><span class="comment">#controlkey 8</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Enable writing of statistics records.</span></span><br><span class="line"><span class="comment">#statistics clockstats cryptostats loopstats peerstats</span></span><br><span class="line"> </span><br><span class="line"><span class="comment"># Disable the monitoring facility to prevent amplification attacks using ntpdc</span></span><br><span class="line"><span class="comment"># monlist command when default restrict does not include the noquery flag. See</span></span><br><span class="line"><span class="comment"># CVE-2013-5211 for more details.</span></span><br><span class="line"><span class="comment"># Note: Monitoring will not be disabled with the limited restriction flag.</span></span><br><span class="line"><span class="built_in">disable</span> monitor</span><br><span class="line"></span><br><span class="line"><span class="comment"># Enable Logfile</span></span><br><span class="line">logfile /var/<span class="built_in">log</span>/ntp.log</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ systemctl start ntpd</span><br><span class="line">$ systemctl <span class="built_in">enable</span> ntpd</span><br></pre></td></tr></table></figure><h2 id="安装并配置"><a href="#安装并配置" class="headerlink" title="安装并配置"></a>安装并配置</h2><h3 id="更换系统源"><a href="#更换系统源" class="headerlink" title="更换系统源"></a>更换系统源</h3><p>把系统默认源更换为阿里云源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rm -rf /etc/yum.repos.d/*.repo</span><br><span class="line">$ wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line">$ wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line">$ sed -i <span class="string">'/aliyuncs/d'</span> /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line">$ sed -i <span class="string">'s/$releasever/7/g'</span> /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line">$ sed -i <span class="string">'/aliyuncs/d'</span> /etc/yum.repos.d/epel.repo</span><br><span class="line">$ yum clean all</span><br><span class="line">$ yum makecache fast</span><br></pre></td></tr></table></figure><h2 id="安装Ceph-Deploy-主节点-管理节点"><a href="#安装Ceph-Deploy-主节点-管理节点" class="headerlink" title="安装Ceph-Deploy(主节点/管理节点)"></a>安装Ceph-Deploy(主节点/管理节点)</h2><p>以下操作仅在主节点/管理节点中进行，<strong>不需要所有节点进行以下操作</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ yum install http://mirrors.163.com/ceph/rpm-jewel/el7/noarch/ceph-deploy-1.5.38-0.noarch.rpm</span><br><span class="line">$ ceph-deploy --version</span><br><span class="line">1.5.39</span><br></pre></td></tr></table></figure><h3 id="创建CEPH集群"><a href="#创建CEPH集群" class="headerlink" title="创建CEPH集群"></a>创建<code>CEPH</code>集群</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -iu ceph</span><br><span class="line">$ mkdir -p ceph-cluster</span><br><span class="line">$ <span class="built_in">cd</span> /home/ceph/ceph-cluster</span><br><span class="line">$ ceph-deploy new ceph-node-1 ceph-node-2 ceph-node-3</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO  ] Invoked (1.5.37): /usr/bin/ceph-deploy new ceph-node1</span><br><span class="line">[ceph_deploy.cli][INFO  ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO  ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  func                          : &lt;<span class="keyword">function</span> new at 0x112b500&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0xf26e18&gt;</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ssh_copykey                   : True</span><br><span class="line">[ceph_deploy.cli][INFO  ]  mon                           : [<span class="string">'ceph-node1'</span>,<span class="string">'ceph-node2'</span>,<span class="string">'ceph-node3'</span>]</span><br><span class="line">[ceph_deploy.cli][INFO  ]  public_network                : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  cluster_network               : None</span><br><span class="line">[ceph_deploy.cli][INFO  ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO  ]  fsid                          : None</span><br><span class="line">[ceph_deploy.new][DEBUG ] Creating new cluster named ceph</span><br><span class="line">···</span><br><span class="line">···</span><br></pre></td></tr></table></figure><h3 id="编辑CEPH配置文件"><a href="#编辑CEPH配置文件" class="headerlink" title="编辑CEPH配置文件"></a>编辑<code>CEPH</code>配置文件</h3><p>在<code>global</code>下添加以下配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ vi ceph.conf</span><br><span class="line">[global]</span><br><span class="line">mon_clock_drift_allowed = 5</span><br><span class="line">osd_journal_size = 20480</span><br><span class="line">public_network=172.16.0.0/24</span><br></pre></td></tr></table></figure><h3 id="安装CEPH-所有节点都需要安装"><a href="#安装CEPH-所有节点都需要安装" class="headerlink" title="安装CEPH(所有节点都需要安装)"></a>安装<code>CEPH</code>(所有节点都需要安装)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy install --release jewel --repo-url http://mirrors.163.com/ceph/rpm-jewel/el7 --gpg-url http://mirrors.163.com/ceph/keys/release.asc ceph-client ceph-node-1 ceph-node-2 ceph-node-3</span><br><span class="line"></span><br><span class="line"><span class="comment">## 检查CEPH版本</span></span><br><span class="line">$ ceph -v</span><br><span class="line">ceph version 10.2.10 (5dc1e4c05cb68dbf62ae6fce3f0700e4654fdbbe)</span><br></pre></td></tr></table></figure><h3 id="初始化MOM节点-主节点-管理节点"><a href="#初始化MOM节点-主节点-管理节点" class="headerlink" title="初始化MOM节点(主节点/管理节点)"></a>初始化MOM节点(主节点/管理节点)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy mon create-initial</span><br><span class="line">···</span><br><span class="line">···</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.client.admin.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-mds.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] keyring <span class="string">'ceph.mon.keyring'</span> already exists</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-osd.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Storing ceph.bootstrap-rgw.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO  ] Destroy temp directory /tmp/tmp8sI0aU</span><br></pre></td></tr></table></figure><h3 id="查看集群状态（在OSD节点上查看）"><a href="#查看集群状态（在OSD节点上查看）" class="headerlink" title="查看集群状态（在OSD节点上查看）"></a>查看集群状态（在OSD节点上查看）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">$ ceph -s</span><br><span class="line">     cluster 60597e53-ad29-44bd-8dcd-db6aeae6f580</span><br><span class="line">     health HEALTH_ERR</span><br><span class="line">            no osds</span><br><span class="line">     monmap e2: 3 mons at &#123;ceph-node-1=172.16.0.100:6789/0,ceph-node-2=172.16.0.101:6789/0,ceph-node-3=172.16.0.102:6789/0&#125;</span><br><span class="line">            election epoch 6, quorum 0,1,2 ceph-node-1,ceph-node-2,ceph-node-3</span><br><span class="line">     osdmap e1: 0 osds: 0 up, 0 <span class="keyword">in</span></span><br><span class="line">            flags sortbitwise,require_jewel_osds</span><br><span class="line">      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">            0 kB used, 0 kB / 0 kB avail</span><br><span class="line">                  64 creating</span><br></pre></td></tr></table></figure><h3 id="配置管理节点ceph-master"><a href="#配置管理节点ceph-master" class="headerlink" title="配置管理节点ceph-master"></a>配置管理节点<code>ceph-master</code></h3><p>用<code>ceph-deploy</code>把配置文件和<code>admin</code>密钥拷贝到<code>Master</code>节点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy admin ceph-master</span><br><span class="line"></span><br><span class="line"><span class="comment">## 确保你对ceph.client.admin.keyring有正确的操作权限</span></span><br><span class="line">$ chmod +r /etc/ceph/ceph.client.admin.keyring</span><br></pre></td></tr></table></figure><h3 id="添加数据节点-在节点上用root进行操作"><a href="#添加数据节点-在节点上用root进行操作" class="headerlink" title="添加数据节点(在节点上用root进行操作)"></a>添加数据节点(在节点上用<code>root</code>进行操作)</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-node1 ~]<span class="comment"># mkdir /data</span></span><br><span class="line">[root@ceph-node1 ~]<span class="comment"># mkdir /data/osd0</span></span><br><span class="line">[root@ceph-node1 ~]<span class="comment"># chmod -R 777 /data/</span></span><br><span class="line"> </span><br><span class="line">[root@ceph-node2 ~]<span class="comment"># mkdir /data</span></span><br><span class="line">[root@ceph-node2 ~]<span class="comment"># mkdir /data/osd1</span></span><br><span class="line">[root@ceph-node2 ~]<span class="comment"># chmod -R 777 /data/</span></span><br><span class="line"> </span><br><span class="line">[root@ceph-node3 ~]<span class="comment"># mkdir /data</span></span><br><span class="line">[root@ceph-node3 ~]<span class="comment"># mkdir /data/osd2</span></span><br><span class="line">[root@ceph-node3 ~]<span class="comment"># chmod -R 777 /data/</span></span><br></pre></td></tr></table></figure><p>分别挂载磁盘</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-node1 ~]<span class="comment"># mount /dev/vdb1 /data/osd0</span></span><br><span class="line"> </span><br><span class="line">[root@ceph-node2 ~]<span class="comment"># mount /dev/vdb1 /data/osd1</span></span><br><span class="line"> </span><br><span class="line">[root@ceph-node3 ~]<span class="comment"># mount /dev/vdb1 /data/osd2</span></span><br></pre></td></tr></table></figure><h3 id="在主节点-管理节点准备OSD"><a href="#在主节点-管理节点准备OSD" class="headerlink" title="在主节点/管理节点准备OSD"></a>在主节点/管理节点准备OSD</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/ceph/ceph-cluster/</span><br><span class="line">$ ceph-deploy osd prepare ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2</span><br><span class="line">······</span><br><span class="line">······</span><br><span class="line">[ceph-node3][INFO  ] Running <span class="built_in">command</span>: sudo /usr/sbin/ceph-disk -v prepare --cluster ceph --fs-type xfs -- /data/osd2</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-osd --check-allows-journal -i 0 --<span class="built_in">log</span>-file <span class="variable">$run_dir</span>/<span class="variable">$cluster</span>-osd-check.log --cluster ceph --setuser ceph --setgroup ceph</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-osd --check-wants-journal -i 0 --<span class="built_in">log</span>-file <span class="variable">$run_dir</span>/<span class="variable">$cluster</span>-osd-check.log --cluster ceph --setuser ceph --setgroup ceph</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-osd --check-needs-journal -i 0 --<span class="built_in">log</span>-file <span class="variable">$run_dir</span>/<span class="variable">$cluster</span>-osd-check.log --cluster ceph --setuser ceph --setgroup ceph</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size</span><br><span class="line">[ceph-node3][WARNIN] populate_data_path: Preparing osd data dir /data/osd2</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /sbin/restorecon -R /data/osd2/ceph_fsid.5285.tmp</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/chown -R ceph:ceph /data/osd2/ceph_fsid.5285.tmp</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /sbin/restorecon -R /data/osd2/fsid.5285.tmp</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/chown -R ceph:ceph /data/osd2/fsid.5285.tmp</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /sbin/restorecon -R /data/osd2/magic.5285.tmp</span><br><span class="line">[ceph-node3][WARNIN] <span class="built_in">command</span>: Running <span class="built_in">command</span>: /usr/bin/chown -R ceph:ceph /data/osd2/magic.5285.tmp</span><br><span class="line">[ceph-node3][INFO  ] checking OSD status...</span><br><span class="line">[ceph-node3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph-node3][INFO  ] Running <span class="built_in">command</span>: sudo /bin/ceph --cluster=ceph osd <span class="built_in">stat</span> --format=json</span><br><span class="line">[ceph-node3][WARNIN] there are 2 OSDs down</span><br><span class="line">[ceph-node3][WARNIN] there are 2 OSDs out</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host ceph-node3 is now ready <span class="keyword">for</span> osd use.</span><br></pre></td></tr></table></figure><h3 id="激活OSD（仍然在主节点-管理节点上操作）"><a href="#激活OSD（仍然在主节点-管理节点上操作）" class="headerlink" title="激活OSD（仍然在主节点/管理节点上操作）"></a>激活OSD（仍然在主节点/管理节点上操作）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy osd activate ceph-node1:/data/osd0 ceph-node2:/data/osd1 ceph-node3:/data/osd2</span><br><span class="line">······</span><br><span class="line">······</span><br><span class="line">[ceph-node3][WARNIN] start_daemon: Starting ceph osd.4...</span><br><span class="line">[ceph-node3][WARNIN] command_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl <span class="built_in">disable</span> ceph-osd@4</span><br><span class="line">[ceph-node3][WARNIN] command_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl <span class="built_in">disable</span> ceph-osd@4 --runtime</span><br><span class="line">[ceph-node3][WARNIN] command_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl <span class="built_in">enable</span> ceph-osd@4</span><br><span class="line">[ceph-node3][WARNIN] Created symlink from /etc/systemd/system/ceph-osd.target.wants/ceph-osd@4.service to /usr/lib/systemd/system/ceph-osd@.service.</span><br><span class="line">[ceph-node3][WARNIN] command_check_call: Running <span class="built_in">command</span>: /usr/bin/systemctl start ceph-osd@4</span><br><span class="line">[ceph-node3][INFO  ] checking OSD status...</span><br><span class="line">[ceph-node3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph-node3][INFO  ] Running <span class="built_in">command</span>: sudo /bin/ceph --cluster=ceph osd <span class="built_in">stat</span> --format=json</span><br><span class="line">[ceph-node3][WARNIN] there are 2 OSDs down</span><br><span class="line">[ceph-node3][WARNIN] there are 2 OSDs out</span><br><span class="line">[ceph-node3][INFO  ] Running <span class="built_in">command</span>: sudo systemctl <span class="built_in">enable</span> ceph.target</span><br></pre></td></tr></table></figure><h3 id="开机挂载磁盘（所有OSD节点都要操作）"><a href="#开机挂载磁盘（所有OSD节点都要操作）" class="headerlink" title="开机挂载磁盘（所有OSD节点都要操作）"></a>开机挂载磁盘（所有<code>OSD</code>节点都要操作）</h3><p>挂载好的新磁盘会在下一次重启或开机的时候失效，因此需要在每台机上对新磁盘进行挂载，并编辑/etc/fstab文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[root@ceph-node1 ~]<span class="comment"># cat /etc/fstab</span></span><br><span class="line">........</span><br><span class="line">/dev/vdb1               /data/osd0                   xfs     defaults        0 0</span><br><span class="line"> </span><br><span class="line">[root@ceph-node2 ~]<span class="comment"># cat /etc/fstab</span></span><br><span class="line">........</span><br><span class="line">/dev/vdb1               /data/osd1                   xfs     defaults        0 0</span><br><span class="line"> </span><br><span class="line">[root@ceph-node3 ~]<span class="comment"># cat /etc/fstab</span></span><br><span class="line">........</span><br><span class="line">/dev/vdb1               /data/osd2                   xfs     defaults        0 0</span><br></pre></td></tr></table></figure><h3 id="分发配置和密钥（主节点-管理节点上执行）"><a href="#分发配置和密钥（主节点-管理节点上执行）" class="headerlink" title="分发配置和密钥（主节点/管理节点上执行）"></a>分发配置和密钥（主节点/管理节点上执行）</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -iu ceph</span><br><span class="line">$ <span class="built_in">cd</span> /home/ceph/ceph-cluster</span><br><span class="line">$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3</span><br><span class="line">······</span><br><span class="line">······</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node1</span><br><span class="line">[ceph-node1][DEBUG ] connection detected need <span class="keyword">for</span> sudo</span><br><span class="line">[ceph-node1][DEBUG ] connected to host: ceph-node1 </span><br><span class="line">[ceph-node1][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph-node1][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph-node1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node2</span><br><span class="line">ceph@ceph-node2<span class="string">'s password: </span></span><br><span class="line"><span class="string">[ceph-node2][DEBUG ] connection detected need for sudo</span></span><br><span class="line"><span class="string">ceph@ceph-node2'</span>s password: </span><br><span class="line">[ceph-node2][DEBUG ] connected to host: ceph-node2 </span><br><span class="line">[ceph-node2][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph-node2][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph-node2][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to ceph-node3</span><br><span class="line">ceph@ceph-node3<span class="string">'s password: </span></span><br><span class="line"><span class="string">[ceph-node3][DEBUG ] connection detected need for sudo</span></span><br><span class="line"><span class="string">ceph@ceph-node3'</span>s password: </span><br><span class="line">[ceph-node3][DEBUG ] connected to host: ceph-node3 </span><br><span class="line">[ceph-node3][DEBUG ] detect platform information from remote host</span><br><span class="line">[ceph-node3][DEBUG ] detect machine <span class="built_in">type</span></span><br><span class="line">[ceph-node3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br></pre></td></tr></table></figure><h3 id="查看集群状态"><a href="#查看集群状态" class="headerlink" title="查看集群状态"></a>查看集群状态</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ ceph health</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br><span class="line">$ ceph -s</span><br><span class="line">    cluster 12d4f209-69ea-4ad9-9507-b00557b42815</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 3 mons at &#123;ceph-node-1=172.16.0.100:6789/0,ceph-node-2=172.16.0.101:6789/0,ceph-node-3=172.16.0.102:6789/0&#125;</span><br><span class="line">            election epoch 2, quorum 0 ceph-node1</span><br><span class="line">     osdmap e14: 3 osds: 3 up, 3 <span class="keyword">in</span></span><br><span class="line">      pgmap v26: 64 pgs, 1 pools, 0 bytes data, 0 objects</span><br><span class="line">            15459 MB used, 45944 MB / 61404 MB avail</span><br><span class="line">                  64 active+clean</span><br></pre></td></tr></table></figure><h3 id="重新部署（温馨提示）"><a href="#重新部署（温馨提示）" class="headerlink" title="重新部署（温馨提示）"></a>重新部署（温馨提示）</h3><p><strong>部署过程中如果出现任何奇怪的问题无法解决，可以简单的删除所有节点，一切从头再来：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ceph-deploy purge ceph-mon1 ceph-mon2 ceph-mon3 ceph-osd1 ceph-osd2</span><br><span class="line">$ ceph-deploy purgedata ceph-mon1 ceph-mon2 ceph-mon3 ceph-osd1 ceph-osd2</span><br><span class="line">$ ceph-deploy forgetkey</span><br></pre></td></tr></table></figure><h3 id="创建文件系统"><a href="#创建文件系统" class="headerlink" title="创建文件系统"></a>创建文件系统</h3><p>创建文件系统的步骤参考<a href="http://docs.ceph.com/docs/master/cephfs/createfs/" target="_blank" rel="noopener">官网</a><br>对于一个刚创建的MDS服务，虽然服务是运行的，但是它的状态直到创建 pools 以及文件系统的时候才会变为Active.</p><p>还没有创建时候的状态<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line">e1: 0/0/0 up</span><br></pre></td></tr></table></figure></p><p>创建管理节点（ceph-node1节点上）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">$ sudo -iu ceph</span><br><span class="line">$ <span class="built_in">cd</span> /home/ceph/ceph-cluster</span><br><span class="line">$ ceph-deploy mds create ceph-node1 ceph-node2 ceph-node3</span><br><span class="line">······</span><br><span class="line">······</span><br><span class="line">[ceph-node2][INFO  ] Running <span class="built_in">command</span>: service ceph start mds.ceph-node2</span><br><span class="line">[ceph-node2][DEBUG ] === mds.ceph-node2 === </span><br><span class="line">[ceph-node2][DEBUG ] Starting Ceph mds.ceph-node2 on ceph-node2...</span><br><span class="line">[ceph-node2][WARNIN] Running as unit ceph-mds.ceph-node2.1496648453.865988091.service.</span><br><span class="line">[ceph-node2][INFO  ] Running <span class="built_in">command</span>: systemctl <span class="built_in">enable</span> ceph</span><br><span class="line">[ceph-node2][WARNIN] ceph.service is not a native service, redirecting to /sbin/chkconfig.</span><br><span class="line">[ceph-node2][WARNIN] Executing /sbin/chkconfig ceph on</span><br><span class="line">[ceph_deploy][ERROR ] GenericError: Failed to create 1 MDSs</span><br></pre></td></tr></table></figure><p><strong>注意：如果不创建mds管理节点，client客户端将不能正常挂载到ceph集群！！</strong></p><h3 id="创建pool"><a href="#创建pool" class="headerlink" title="创建pool"></a>创建<code>pool</code></h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd lspools</span><br><span class="line">0 rbd,</span><br></pre></td></tr></table></figure><p>新创建的ceph集群只有rdb一个pool。这时需要创建一个新的pool<br>新建pool，可参考<a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="noopener">官网</a></p><p>创建新的pool<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line">$ ceph osd pool create cephfs_data 32                //用于数据的pool。后面的数字是PG的数量</span><br><span class="line">pool <span class="string">'cephfs_data'</span> created</span><br><span class="line">$ ceph osd pool create cephfs_metadata 32            //用于元数据的pool</span><br><span class="line">pool <span class="string">'cephfs_metadata'</span> created</span><br><span class="line"></span><br><span class="line">$ ceph fs new myceph  cephfs_metadata cephfs_data</span><br><span class="line">new fs with metadata pool 2 and data pool 1</span><br><span class="line"></span><br><span class="line">$ ceph osd lspools</span><br><span class="line">0 rbd,1 cephfs_data,2 cephfs_metadata,</span><br><span class="line"></span><br><span class="line"><span class="comment">## 检验</span></span><br><span class="line">$ ceph mds <span class="built_in">stat</span></span><br><span class="line">e5: 1/1/1 up &#123;0=ceph-node1=up:active&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">$ ceph -s</span><br><span class="line">    cluster 12d4f209-69ea-4ad9-9507-b00557b42815</span><br><span class="line">     health HEALTH_OK</span><br><span class="line">     monmap e1: 1 mons at &#123;ceph-node-1=172.16.0.100:6789/0,ceph-node-2=172.16.0.101:6789/0,ceph-node-3=172.16.0.102:6789/0&#125;</span><br><span class="line">            election epoch 2, quorum 0 ceph-node1</span><br><span class="line">     mdsmap e5: 1/1/1 up &#123;0=ceph-node1=up:active&#125;</span><br><span class="line">     osdmap e19: 3 osds: 3 up, 3 <span class="keyword">in</span></span><br><span class="line">      pgmap v37: 84 pgs, 3 pools, 1962 bytes data, 20 objects</span><br><span class="line">            15460 MB used, 45943 MB / 61404 MB avail</span><br><span class="line">                  84 active+clean</span><br><span class="line"></span><br><span class="line">$ ceph osd tree</span><br><span class="line">ID WEIGHT  TYPE NAME      UP/DOWN REWEIGHT PRIMARY-AFFINITY </span><br><span class="line">-1 0.29279 root default                                     </span><br><span class="line">-2 0.09760     host ceph6                                   </span><br><span class="line"> 2 0.09760         osd.2       up  1.00000          1.00000 </span><br><span class="line">-3 0.09760     host ceph7                                   </span><br><span class="line"> 3 0.09760         osd.3       up  1.00000          1.00000 </span><br><span class="line">-4 0.09760     host ceph8                                   </span><br><span class="line"> 4 0.09760         osd.4       up  1.00000          1.00000</span><br></pre></td></tr></table></figure></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph部署过程记录。&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;p&gt;这里我只是测试环境，所以使用一个监控节点，两个存储节点，具
      
    
    </summary>
    
      <category term="分布式存储系统Ceph部署过程记录" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9FCeph%E9%83%A8%E7%BD%B2%E8%BF%87%E7%A8%8B%E8%AE%B0%E5%BD%95/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>分布式存储系统Ceph原理</title>
    <link href="http://yoursite.com/2018/03/13/ceph-introduction/"/>
    <id>http://yoursite.com/2018/03/13/ceph-introduction/</id>
    <published>2018-03-12T16:00:00.000Z</published>
    <updated>2018-03-26T03:04:59.609Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是分布式存储系统Ceph的原理介绍。</p><h2 id="Ceph简单概述"><a href="#Ceph简单概述" class="headerlink" title="Ceph简单概述"></a>Ceph简单概述</h2><p>Ceph是一个分布式存储系统，诞生于2004年，最早致力于开发下一代高性能分布式文件系统的项目。<a href="http://ceph.com/download/" target="_blank" rel="noopener">Ceph源码下载</a>。随着云计算的发展，ceph乘上了OpenStack的春风，进而成为了开源社区受关注较高的项目之一。Ceph可以将多台服务器组成一个超大集群，把这些机器中的磁盘资源整合到一块儿，形成一个大的资源池（PB级别），然后按需分配给应用使用。Ceph分布式存储的优势：</p><ul><li><p>CRUSH算法<br>CRUSH算法是ceph的两大创新之一，简单来说，ceph摒弃了传统的集中式存储元数据寻址的方案，转而使用CRUSH算法完成数据的寻址操作。<br>CRUSH在一致性哈希基础上很好的考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等。<br>CRUSH算法有相当强大的扩展性，理论上支持数千个存储节点。</p></li><li><p>高可用<br>Ceph中的数据副本数量可以由管理员自行定义，并可以通过CRUSH算法指定副本的物理存储位置以分隔故障域，支持数据强一致性；<br>Ceph可以忍受多种故障场景并自动尝试并行修复；<br>Ceph支持多份强一致性副本，副本能够垮主机、机架、机房、数据中心存放。所以安全可靠。<br>Ceph存储节点可以自管理、自动修复。无单点故障，容错性强。</p></li><li><p>高性能<br>因为是多个副本，因此在读写操作时候能够做到高度并行化。理论上，节点越多，整个集群的IOPS和吞吐量越高。<br>另外一点ceph客户端读写数据直接与存储设备(osd) 交互。在块存储和对象存储中无需元数据服务器。</p></li><li><p>高扩展性<br>Ceph不同于swift，客户端所有的读写操作都要经过代理节点。一旦集群并发量增大时，代理节点很容易成为单点瓶颈。<br>Ceph本身并没有主控节点，扩展起来比较容易，并且理论上，它的性能会随着磁盘数量的增加而线性增长。<br>Ceph扩容方便、容量大。能够管理上千台服务器、EB级的容量。</p></li><li><p>特性丰富<br>Ceph支持三种调用接口：对象存储，块存储，文件系统挂载。三种方式可以一同使用。<br>在国内一些公司的云环境中，通常会采用Ceph作为openstack的唯一后端存储来提升数据转发效率。<br>Ceph统一存储，虽然Ceph底层是一个分布式文件系统，但由于在上层开发了支持对象和块的接口。所以在开源存储软件中，能够一统江湖。至于能不能千秋万代，就不知了。</p></li></ul><p>Ceph提供3种存储方式分别是对象存储，块存储和文件系统，一般我们主要关心的还是块存储，推荐将虚拟机后端存储从SAN过渡到Ceph。Ceph 现在是云计算、虚拟机部署的最火开源存储解决方案，据说有20%的OpenStack部署存储用的都是Ceph的block storage。</p><h2 id="Ceph分布式存储的基本架构"><a href="#Ceph分布式存储的基本架构" class="headerlink" title="Ceph分布式存储的基本架构"></a>Ceph分布式存储的基本架构</h2><p><img src="/images/ceph-1.png" alt="ceph-1"></p><p>Ceph的底层是RADOS，RADOS本身也是分布式存储系统，CEPH所有的存储功能都是基于RADOS实现。RADOS采用C++开发，所提供的原生Librados API包括C和C++两种。Ceph的上层应用调用本机上的librados API，再由后者通过socket与RADOS集群中的其他节点通信并完成各种操作。</p><p>RADOS向外界暴露了调用接口，即LibRADOS,应用程序只需要调用LibRADOS的接口，就可以操纵Ceph了。这其中，RADOS GW用于对象存储，RBD用于块存储，它们都属于LibRADOS;CephFS是内核态程序，向外界提供了POSIX接口，用户可以通过客户端直接挂载使用。</p><p>RADOS GateWay、RBD其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中，RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。这两种方式目前在云计算中应用的比较多。</p><p>CEPHFS则提供了POSIX接口，用户可直接通过客户端挂载使用。它是内核态的程序，所以无需调用用户空间的librados库。它通过内核中的net模块来与Rados进行交互。</p><h2 id="Ceph之RADOS浅析"><a href="#Ceph之RADOS浅析" class="headerlink" title="Ceph之RADOS浅析"></a>Ceph之RADOS浅析</h2><p>RADOS (Reliable, Autonomic Distributed Object Store) 是Ceph的核心之一，作为Ceph分布式文件系统的一个子项目，特别为Ceph的需求设计，能够在动态变化和异质结构的存储设备机群之上提供一种稳定、可扩展、高性能的单一逻辑对象(Object)存储接口和能够实现节点的自适应和自管理的存储系统。</p><p>在传统分布式存储架构中，存储节点往往仅作为被动查询对象来使用，随着存储规模的增加，数据一致性的管理会出现很多问题。</p><p>而新型的存储架构倾向于将基本的块分配决策和安全保证等操作交给存储节点来做，然后通过提倡客户端和存储节点直接交互来简化数据布局并减小io瓶颈。</p><p>RADOS就是这样一个可用于PB级规模数据存储集群的可伸缩的、可靠的对象存储服务。它包含两类节点：存储节点、管理节点。它通过利用存储设备的智能性，将诸如一致性数据访问、冗余存储、错误检测、错误恢复分布到包含了上千存储节点的集群中，而不是仅仅依靠少数管理节点来处理。</p><p>RADOS中的存储节点被称为OSD(object storage device)，它可以仅由很普通的组件来构成，只需要包含CPU、网卡、本地缓存和一个磁盘或者RAID，并将传统的块存储方式替换成面向对象的存储。</p><p>在PB级的存储规模下，存储系统一定是动态的：系统会随着新设备的部署和旧设备的淘汰而增长或收缩，系统内的设备会持续地崩溃和恢复，大量的数据被创建或者删除。RADOS通过 cluster map来实现这些，cluster map会被复制到集群中的所有部分（存储节点、控制节点，甚至是客户端），并且通过怠惰地传播小增量更新而更新。cluster map中存储了整个集群的数据的分布以及成员。</p><p>通过在每个存储节点存储完整的cluster map，存储设备可以表现的半自动化，通过peer-to-peer的方式（比如定义协议）来进行数据备份、更新，错误检测、数据迁移等等操作。这无疑减轻了占少数的monitor cluster（管理节点组成的集群）的负担。</p><h3 id="设计如下："><a href="#设计如下：" class="headerlink" title="设计如下："></a>设计如下：</h3><p><img src="/images/ceph-2.png" alt="ceph-2"></p><p>一个RADOS系统包含大量的OSDs 和 很少的用于管理OSD集群成员的monitors。OSD的组成如简介所说。而monitor是一些独立的进程，以及少量的本地存储，monitor之间通过一致性算法保证数据的一致性。</p><h3 id="Cluster-Map"><a href="#Cluster-Map" class="headerlink" title="Cluster Map"></a>Cluster Map</h3><p>存储节点集群通过monitor集群操作cluster map来实现成员的管理。cluster map 描述了哪些OSD被包含进存储集群以及所有数据在存储集群中的分布。<br><img src="/images/ceph-3.png" alt="ceph-3"></p><p>cluster map不仅存储在monitor节点，它被复制到集群中的每一个存储节点，以及和集群交互的client。</p><p>当因为一些原因，比如设备崩溃、数据迁移等，cluster map的内容需要改变时，cluster map的版本号被增加，map的版本号可以使通信的双方确认自己的map是否是最新的，版本旧的一方会先将map更新成对方的map，然后才会进行后续操作。</p><h3 id="Data-Placement"><a href="#Data-Placement" class="headerlink" title="Data Placement"></a>Data Placement</h3><p>首先，如下图，总体说下RADOS的存储层次，RADOS中基本的存储单位是对象，一般为2MB或4MB，当一个文件要存入RADOS时，首先会被切分成大小固定的对象（最后一个对象大小可能不同），然后将对象分配到一个PG（Placement Group）中，然后PG会复制几份，伪随机地派给不同的存储节点。当新的存储节点被加入集群，会在已有数据中随机抽取一部分数据迁移到新节点。这种概率平衡的分布方式可以保证设备在潜在的高负载下正常工作。更重要的是，数据的分布过程仅需要做几次随机映射，不需要大型的集中式分配表。</p><p><img src="/images/ceph-4.png" alt="ceph-4"></p><p><strong>对于每个层次的详细说明：</strong></p><ul><li><p>File—— 用户需要存储或者访问的文件。</p></li><li><p>Object—— RADOS的基本存储单元。Object与上面提到的file的区别是，object的最大size由RADOS限定（通常为2MB或4MB），以便实现底层存储的组织管理。因此，当上层应用向RADOS存入size很大的file时，需要将file切分成统一大小的一系列object（最后一个的大小可以不同）进行存储。</p></li><li><p>PG（Placement Group）—— 对object的存储进行组织和位置映射。具体而言，一个PG负责组织若干个object（可以为数千个甚至更多），但一个object只能被映射到一个PG中，即，PG和object之间是“一对多”映射关系。同时，一个PG会被映射到n个OSD上，而每个OSD上都会承载大量的PG，即，PG和OSD之间是“多对多”映射关系。在实践当中，n至少为2（n代表冗余的份数），如果用于生产环境，则至少为3。一个OSD上的PG则可达到数百个。事实上，PG数量的设置牵扯到数据分布的均匀性问题。</p></li><li><p>OSD—— 即object storage device，前文已经详细介绍，此处不再展开。唯一需要说明的是，OSD的数量事实上也关系到系统的数据分布均匀性，因此其数量不应太少。在实践当中，至少也应该是数十上百个的量级才有助于Ceph系统的设计发挥其应有的优势。</p></li></ul><p><strong>各层次之间的映射关系：</strong></p><ul><li><p>file -&gt; object<br>object的最大size是由RADOS配置的，当用户要存储一个file，需要将file切分成几个object。</p></li><li><p>object -&gt; PG<br>每个object都会被映射到一个PG中，然后以PG为单位进行备份以及进一步映射到具体的OSD上。</p></li><li><p>PG -&gt; OSD<br>根据用户设置的冗余存储的个数r，PG会最终存储到r个OSD上，这个映射是通过一种伪随机的映射算法 CRUSH 来实现的，这个算法的特点是可以进行配置。</p></li></ul><h3 id="Ceph集群维护"><a href="#Ceph集群维护" class="headerlink" title="Ceph集群维护"></a>Ceph集群维护</h3><p>前面已经介绍了，由若干个monitor共同负责整个RADOS集群中所有OSD状态的发现与记录，并且共同形成cluster map的master版本，然后扩散至全体OSD以及client。OSD使用cluster map进行数据的维护，而client使用cluster map进行数据的寻址。</p><p>monitor并不主动轮询各个OSD的当前状态。相反，OSD需要向monitor上报状态信息。常见的上报有两种情况：一是新的OSD被加入集群，二是某个OSD发现自身或者其他OSD发生异常。在收到这些上报信息后，monitor将更新cluster map信息并加以扩散。其细节将在下文中加以介绍。</p><p><strong>Cluster map的实际内容包括：</strong></p><ul><li><p>Epoch，即版本号。cluster map的epoch是一个单调递增序列。epoch越大，则cluster map版本越新。因此，持有不同版本cluster map的OSD或client可以简单地通过比较epoch决定应该遵从谁手中的版本。而monitor手中必定有epoch最大、版本最新的cluster map。当任意两方在通信时发现彼此epoch值不同时，将默认先将cluster map同步至高版本一方的状态，再进行后续操作。</p></li><li><p>各个OSD的网络地址。</p></li><li><p>各个OSD的状态。OSD状态的描述分为两个维度：up或者down（表明OSD是否正常工作），in或者out（表明OSD是否在至少一个PG中）。因此，对于任意一个OSD，共有四种可能的状态：<br>up且in：说明该OSD正常运行，且已经承载至少一个PG的数据。这是一个OSD的标准工作状态；</p></li></ul><p>up且out：说明该OSD正常运行，但并未承载任何PG，其中也没有数据。一个新的OSD刚刚被加入Ceph集群后，便会处于这一状态。而一个出现故障的OSD被修复后，重新加入Ceph集群时，也是处于这一状态；</p><p>down且in：说明该OSD发生异常，但仍然承载着至少一个PG，其中仍然存储着数据。这种状态下的OSD刚刚被发现存在异常，可能仍能恢复正常，也可能会彻底无法工作；</p><p>down且out：说明该OSD已经彻底发生故障，且已经不再承载任何PG。</p><ul><li>CRUSH算法配置参数。表明了Ceph集群的物理层级关系（cluster hierarchy），位置映射规则（placement rules）。<br>根据cluster map的定义可以看出，其版本变化通常只会由（3）和（4）两项信息的变化触发。而这两者相比，（3）发生变化的概率更高一些。这可以通过下面对OSD工作状态变化过程的介绍加以反映。</li></ul><p>一个新的OSD上线后，首先根据配置信息与monitor通信。Monitor将其加入cluster map，并设置为up且out状态，再将最新版本的cluster map发给这个新OSD。</p><p>收到monitor发来的cluster map之后，这个新OSD计算出自己所承载的PG（为简化讨论，此处我们假定这个新的OSD开始只承载一个PG），以及和自己承载同一个PG的其他OSD。然后，新OSD将与这些OSD取得联系。如果这个PG目前处于降级状态（即承载该PG的OSD个数少于正常值，如正常应该是3个，此时只有2个或1个。这种情况通常是OSD故障所致），则其他OSD将把这个PG内的所有对象和元数据复制给新OSD。数据复制完成后，新OSD被置为up且in状态。而cluster map内容也将据此更新。这事实上是一个自动化的failure recovery过程。当然，即便没有新的OSD加入，降级的PG也将计算出其他OSD实现failure recovery。</p><p>如果该PG目前一切正常，则这个新OSD将替换掉现有OSD中的一个（PG内将重新选出Primary OSD），并承担其数据。在数据复制完成后，新OSD被置为up且in状态，而被替换的OSD将退出该PG（但状态通常仍然为up且in，因为还要承载其他PG）。而cluster map内容也将据此更新。这事实上是一个自动化的数据re-balancing过程。</p><p>如果一个OSD发现和自己共同承载一个PG的另一个OSD无法联通，则会将这一情况上报monitor。此外，如果一个OSD deamon发现自身工作状态异常，也将把异常情况主动上报给monitor。在上述情况下，monitor将把出现问题的OSD的状态设为down且in。如果超过某一预订时间期限，该OSD仍然无法恢复正常，则其状态将被设置为down且out。反之，如果该OSD能够恢复正常，则其状态会恢复为up且in。在上述这些状态变化发生之后，monitor都将更新cluster map并进行扩散。这事实上是自动化的failure detection过程。</p><p>对于一个RADOS集群而言，即便由数千个甚至更多OSD组成，cluster map的数据结构大小也并不惊人。同时，cluster map的状态更新并不会频繁发生。即便如此，Ceph依然对cluster map信息的扩散机制进行了优化，以便减轻相关计算和通信压力：</p><p>首先，cluster map信息是以增量形式扩散的。如果任意一次通信的双方发现其epoch不一致，则版本更新的一方将把二者所拥有的cluster map的差异发送给另外一方。</p><p>其次，cluster map信息是以异步且lazy的形式扩散的。也即，monitor并不会在每一次cluster map版本更新后都将新版本广播至全体OSD，而是在有OSD向自己上报信息时，将更新回复给对方。类似的，各个OSD也是在和其他OSD通信时，将更新发送给版本低于自己的对方。</p><p>基于上述机制，Ceph避免了由于cluster map版本更新而引起的广播风暴。这虽然是一种异步且lazy的机制，但对于一个由n个OSD组成的Ceph集群，任何一次版本更新能够在O(log(n))时间复杂度内扩散到集群中的任何一个OSD上。</p><p>一个可能被问到的问题是：既然这是一种异步和lazy的扩散机制，则在版本扩散过程中，系统必定出现各个OSD看到的cluster map不一致的情况，这是否会导致问题？答案是：不会。事实上，如果一个client和它要访问çAPG内部的各个OSD看到的cluster map状态一致，则访问操作就可以正确进行。而如果这个client或者PG中的某个OSD和其他几方的cluster map不一致，则根据Ceph的机制设计，这几方将首先同步cluster map至最新状态，并进行必要的数据re-balancing操作，然后即可继续正常访问。</p><h2 id="Ceph内部架构"><a href="#Ceph内部架构" class="headerlink" title="Ceph内部架构"></a>Ceph内部架构</h2><p><img src="/images/ceph-4.png" alt="ceph-4"></p><p>每台服务器都有好几块磁盘（sda,sdb,sdc等），磁盘又可以进一步分区（sda1,sda2等）。CEPH中最基本的进程就是OSD(对象存储设备)，每个磁盘对应一个OSD。</p><p>如果用户通过客户端想要存储一个文件，那么在RADOS中，该文件实际上会分为一个个4M块大小的对象。每个文件都一个文件ID(例如A),那么这些对象的ID就是（A0,A1,A2等）。然而在分布式储存系统中，有成千上万个对象，光遍历就要花很长的时间，所以对象会先通过hash-取模运算，存放到一个PG（Place Group）中，PG相当于数据库中的索引（PG的数量是固定的，不会随着OSD的增加或者删除而改变），这样一来，只需要首先定位到PG的位置，然后在PG中查询对象即可，大大提高了查询的效率。之后，PG中的对象又会根据设置的副本数量进行复制，并根据Crush算法存储到OSD节点上。</p><p>无论使用哪种存储方式（对象、块、挂载），存储的数据都会被切分成对象（Objects）。Objects size大小可以由管理员调整，通常为2M或4M。每个对象都会有一个唯一的OID，由<br>ino与ono生成。ino即是文件的File ID，用于在全局唯一标示每一个文件，而ono则是分片的编号。比如：一个文件FileID为A，它被切成了两个对象，一个对象编号0，另一个编号1，那么这两个文件的oid则为A0与A1。Oid的好处是可以唯一标示每个不同的对象，并且存储了对象与文件的从属关系。</p><p>但是对象并不会直接存储进OSD中，因为对象的size很小，在一个大规模的集群中可能有几百到几千万个对象。这么多对象光是遍历寻址，速度都是很缓慢的；并且如果将对象直接通过某种固定映射的哈希算法映射到osd上，当这个osd损坏时，对象无法自动迁移至其他osd上面（因为映射函数不允许）。为了解决这些问题，ceph引入了归置组的概念，即PG。</p><p>PG是一个逻辑概念，我们linux系统中可以直接看到对象，但是无法直接看到PG。它在数据寻址时类似于数据库中的索引：每个对象都会固定映射进一个PG中，所以当我们要寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。而且在数据迁移时，也是以PG作为基本单位进行迁移，ceph不会直接操作对象。</p><p>对象映射进PG的方式：使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量去模，得出PGID。<br>最后PG会根据管理员设置的副本数量进行复制，然后通过crush算法存储到不同的OSD节点上。</p><h2 id="Ceph的基本组件"><a href="#Ceph的基本组件" class="headerlink" title="Ceph的基本组件"></a>Ceph的基本组件</h2><p><img src="/images/ceph-5.png" alt="ceph-5"></p><p>如上图所示，Ceph主要有三个基本进程</p><ul><li><p>Osd<br>用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再均衡。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。<br>当Ceph存储集群设定数据有两个副本时（一共存两份），则至少需要两个OSD守护进程即两个OSD节点，集群才能达到active+clean状态。</p></li><li><p>MDS(可选)<br>为Ceph文件系统提供元数据计算、缓存与同步。在ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。<br>MDS进程并不是必须的进程，只有需要使用CEPHFS时，才需要配置MDS节点。</p></li><li><p>Monitor<br>监控整个集群Cluster map的状态，维护集群的cluster MAP二进制表，保证集群数据的一致性。ClusterMAP描述了对象块存储的物理位置，以及一个将设备<br>聚合到物理位置的桶列表。</p></li></ul><p><strong>Ceph要求必须是奇数个Monitor监控节点，而且至少是3个！</strong>（如果是自己私下测试玩玩的话，可以是1个，但是生产环境绝不建议1个）用于维护和监控整个集群的状态，每个Monitor都有一个Cluster Map，只要有这个Map，就能够清楚知道每个对象存储在什么位置了。客户端会先tcp连接到Monitor，从中获取Cluster Map，并在客户端进行计算，当知道对象的位置后，再直接与OSD通信（去中心化的思想）。OSD节点平常会向Monitor节点发送简单心跳，只有当添加、删除或者出现异常状况时，才会自动上报信息给Monitor。</p><p>MDS是可选的，只有需要使用Ceph FS的时候才需要配置MDS节点。在Ceph中，元数据也是存放在OSD中的，MDS只相当于元数据的缓存服务器。</p><p>在Ceph中，如果要写数据，只能向主OSD写，然后再由主OSD向从OSD同步地写，只有当从OSD返回结果给主OSD后，主OSD才会向客户端报告写入完成的消息。如果要读数据，不会使用读写分离，而是也需要先向主OSD发请求，以保证数据的强一致性。</p><h3 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h3><p>首先描述一下ceph数据的存储过程，如下图：</p><p><img src="/images/ceph-4.png" alt="ceph-4"></p><p>无论使用哪种存储方式（对象、块、挂载），存储的数据都会被切分成对象（Objects）。Objects size大小可以由管理员调整，通常为2M或4M。<br>每个对象都会有一个唯一的OID，由ino与ono生成，虽然这些名词看上去很复杂，其实相当简单。ino即是文件的File ID，用于在全局唯一标示每一个文件，<br>而ono则是分片的编号。比如：一个文件FileID为A，它被切成了两个对象，一个对象编号0，另一个编号1，那么这两个文件的oid则为A0与A1。<br>Oid的好处是可以唯一标示每个不同的对象，并且存储了对象与文件的从属关系。由于ceph的所有数据都虚拟成了整齐划一的对象，所以在读写时效率都会比较高。</p><p>但是对象并不会直接存储进OSD中，因为对象的size很小，在一个大规模的集群中可能有几百到几千万个对象。这么多对象光是遍历寻址，速度都是很缓慢的；<br>并且如果将对象直接通过某种固定映射的哈希算法映射到osd上，当这个osd损坏时，对象无法自动迁移至其他osd上面（因为映射函数不允许）。<br>为了解决这些问题，ceph引入了归置组的概念，即PG。</p><p>PG是一个逻辑概念，我们linux系统中可以直接看到对象，但是无法直接看到PG。它在数据寻址时类似于数据库中的索引：每个对象都会固定映射进一个PG中，<br>所以当我们要寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。而且在数据迁移时，也是以PG作为基本单位进行迁移，<br>ceph不会直接操作对象。</p><p>对象时如何映射进PG的？还记得OID么？首先使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量去模，得到的序号则是PGID。由于这种设计方式，<br>PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好的提升CEPH集群的性能并使数据均匀分布。</p><p>最后PG会根据管理员设置的副本数量进行复制，然后通过crush算法存储到不同的OSD节点上（其实是把PG中的所有对象存储到节点上），第一个osd节点即为主节点，<br>其余均为从节点。</p><p>下面是一段ceph中的伪代码,简要描述了ceph的数据存储流程</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">locator = object_name</span><br><span class="line">obj_hash =  <span class="built_in">hash</span>(locator)</span><br><span class="line">pg = obj_hash % num_pg</span><br><span class="line">osds_for_pg = crush(pg)    <span class="comment"># returns a list of osds</span></span><br><span class="line">primary = osds_for_pg[0]</span><br><span class="line">replicas = osds_for_pg[1:]</span><br></pre></td></tr></table></figure><p><img src="/images/ceph-6.png" alt="ceph-6"></p><p>上图中更好的诠释了ceph数据流的存储过程，数据无论是从三中接口哪一种写入的，最终都要切分成对象存储到底层的RADOS中。逻辑上通过算法先映射到PG上，最终存储近OSD节点里。图中除了之前介绍过的概念之外多了一个pools的概念。</p><p><img src="/images/ceph-7.png" alt="ceph-7"></p><p>Pool是管理员自定义的命名空间，像其他的命名空间一样，用来隔离对象与PG。我们在调用API存储即使用对象存储时，需要指定对象要存储进哪一个POOL中。除了隔离数据，我们也可以分别对不同的POOL设置不同的优化策略，比如副本数、数据清洗次数、数据块及对象大小等。</p><p>OSD是强一致性的分布式存储，它的读写流程如下图：<br><img src="/images/ceph-8.png" alt="ceph-8"></p><p>Ceph的读写操作采用主从模型，客户端要读写数据时，只能向对象所对应的主osd节点发起请求。主节点在接受到写请求时，会同步的向从OSD中写入数据。当所有的OSD节点都写入完成后，主节点才会向客户端报告写入完成的信息。因此保证了主从节点数据的高度一致性。而读取的时候，客户端也只会向主osd节点发起读请求，并不会有类似于数据库中的读写分离的情况出现，这也是出于强一致性的考虑。由于所有写操作都要交给主osd节点来处理，所以在数据量很大时，性能可能会比较慢，为了克服这个问题以及让ceph能支持事物，每个osd节点都包含了一个journal文件，稍后介绍。</p><p>数据流向介绍到这里就告一段落了，现在终于回到正题：osd进程。在ceph中，每一个osd进程都可称作是一个osd节点，也就是说，每台存储服务器上可能包含了众多的osd节点，每个osd节点监听不同的端口，类似于在同一台服务器上跑多个mysql或redis。每个osd节点可以设置一个目录作为实际存储区域，也可以是一个分区，一整块硬盘。如下图，当前这台机器上跑了两个osd进程，每个osd监听4个端口，分别用于接收客户请求、传输数据、发送心跳、同步数据等操作。</p><p><img src="/images/ceph-9.png" alt="ceph-9"></p><p>如上图所示，osd节点默认监听tcp的6800到6803端口，如果同一台服务器上有多个osd节点，则依次往后排序。</p><p>在生产环境中的osd最少可能都有上百个，所以每个osd都有一个全局的编号，类似osd0，osd1，osd2……..序号根据osd诞生的顺序排列，并且是全局唯一的。存储了相同PG的osd节点除了向mon节点发送心跳外，还会互相发送心跳信息以检测pg数据副本是否正常。</p><p><img src="/images/ceph-10.png" alt="ceph-10"></p><p>默认大小为5G，也就说每创建一个osd节点，还没使用就要被journal占走5G的空间。这个值是可以调整的，具体大小要依osd的总大小而定。</p><p>Journal的作用类似于mysql innodb引擎中的事物日志系统。当有突发的大量写入操作时，ceph可以先把一些零散的，随机的IO请求保存到缓存中进行合并，然后再统一向内核发起IO请求。这样做效率会比较高，但是一旦osd节点崩溃，缓存中的数据就会丢失，所以数据在还未写进硬盘中时，都会记录到journal中，当osd崩溃后重新启动时，会自动尝试从journal恢复因崩溃丢失的缓存数据。因此journal的io是非常密集的，而且由于一个数据要io两次，很大程度上也损耗了硬件的io性能，所以通常在生产环境中，使用ssd来单独存储journal文件以提高ceph读写性能。</p><h3 id="monitor节点"><a href="#monitor节点" class="headerlink" title="monitor节点"></a>monitor节点</h3><p>Mon节点监控着整个ceph集群的状态信息，监听于tcp的6789端口。每一个ceph集群中至少要有一个Mon节点，官方推荐每个集群至少部署三台。Mon节点中保存了最新的版本集群数据分布图（cluster map）的主副本。客户端在使用时，需要挂载mon节点的6789端口，下载最新的cluster map，通过crush算法获得集群中各osd的IP地址，然后再与osd节点直接建立连接来传输数据。所以对于ceph来说，并不需要有集中式的主节点用于计算与寻址，客户端分摊了这部分工作。而且客户端也可以直接和osd通信，省去了中间代理服务器的额外开销。</p><p>Mon节点之间使用Paxos算法来保持各节点cluster map的一致性；各mon节点的功能总体上是一样的，相互间的关系可以被简单理解为主备关系。如果主mon节点损坏，其他mon存活节点超过半数时，集群还可以正常运行。当故障mon节点恢复时，会主动向其他mon节点拉取最新的cluster map。</p><p>Mon节点并不会主动轮询各个osd的当前状态，相反，osd只有在一些特殊情况才会上报自己的信息，平常只会简单的发送心跳。特殊情况包括：1、新的OSD被加入集群；2、某个OSD发现自身或其他OSD发生异常。Mon节点在收到这些上报信息时，则会更新cluster map信息并加以扩散。</p><p>cluster map信息是以异步且lazy的形式扩散的。monitor并不会在每一次cluster map版本更新后都将新版本广播至全体OSD，而是在有OSD向自己上报信息时，将更新回复给对方。类似的，各个OSD也是在和其他OSD通信时，如果发现对方的osd中持有的cluster map版本较低，则把自己更新的版本发送给对方。</p><p>推荐使用以下的架构：<br><img src="/images/ceph-11.png" alt="ceph-11.png"></p><p>这里的ceph除了管理网段外，设了两个网段，一个用于客户端读写传输数据。另一个用于各OSD节点之间同步数据和发送心跳信息等。这样做的好处是可以分担网卡的IO压力。否则在数据清洗时，客户端的读写速度会变得极为缓慢。</p><h3 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h3><p>Mds是ceph集群中的元数据服务器，而通常它都不是必须的，因为只有在使用cephfs的时候才需要它，而目在云计算中用的更广泛的是另外两种存储方式。<br>Mds虽然是元数据服务器，但是它不负责存储元数据，元数据也是被切成对象存在各个osd节点中的，如下图：</p><p><img src="/images/ceph-12.png" alt="ceph-12"></p><p>在创建CEPHFS时，要至少创建两个POOL，一个用于存放数据，另一个用于存放元数据。Mds只是负责接受用户的元数据查询请求，然后从osd中把数据取出来映射进自己的内存中供客户访问。所以mds其实类似一个代理缓存服务器，替osd分担了用户的访问压力,如下图：</p><p><img src="/images/ceph-13.png" alt="ceph-13"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是分布式存储系统Ceph的原理介绍。&lt;/p&gt;
&lt;h2 id=&quot;Ceph简单概述&quot;&gt;&lt;a href=&quot;#Ceph简单概述&quot; class=&quot;headerlink&quot; title=&quot;Ceph简单概述&quot;&gt;&lt;/a&gt;Ceph简单概述&lt;/h2&gt;&lt;p&gt;Ceph是一个分布式存储系统
      
    
    </summary>
    
      <category term="分布式存储系统Ceph原理介绍" scheme="http://yoursite.com/categories/%E5%88%86%E5%B8%83%E5%BC%8F%E5%AD%98%E5%82%A8%E7%B3%BB%E7%BB%9FCeph%E5%8E%9F%E7%90%86%E4%BB%8B%E7%BB%8D/"/>
    
    
      <category term="Ceph" scheme="http://yoursite.com/tags/Ceph/"/>
    
  </entry>
  
  <entry>
    <title>Docker镜像仓库Harbor搭建</title>
    <link href="http://yoursite.com/2018/03/12/harbor-installation/"/>
    <id>http://yoursite.com/2018/03/12/harbor-installation/</id>
    <published>2018-03-11T16:00:00.000Z</published>
    <updated>2018-03-23T03:46:26.193Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是Docker镜像仓库Harbor在CentOS 7环境下搭建全过程</p><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p><img src="/images/harbor-logo.png" alt="harbor-logo"><br>Harbor 是一个企业级的 Docker Registry，可以实现 images 的私有存储和日志统计权限控制等功能，并支持创建多项目(Harbor 提出的概念)，基于官方 Registry V2 实现。官方提供了两种版本：<a href="https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-online-installer-v1.4.0.tgz" target="_blank" rel="noopener">在线版v1.4.0</a>和<a href="https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-offline-installer-v1.4.0.tgz" target="_blank" rel="noopener">离线版v1.4.0</a>。<br>更多版本请看，<a href="https://storage.googleapis.com/harbor-releases" target="_blank" rel="noopener">官方release</a></p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li><p>CentOS 7</p></li><li><p>Docker Version: 17.12.1</p></li><li><p>Docker-Compose Version: 1.18.0</p></li></ul><p>Docker环境请参考<a href="https://polaris0112.github.io/2018/03/11/docker-install/" target="_blank" rel="noopener">《Docker安装部署教程》</a></p><p><code>Docker-Compose</code>可以使用以下命令进行安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ curl -L https://github.com/docker/compose/releases/download/1.18.0/docker-compose-`uname -s`-`uname -m` -o /usr/<span class="built_in">local</span>/bin/docker-compose</span><br><span class="line">$ chmod +x /usr/<span class="built_in">local</span>/bin/docker-compose</span><br></pre></td></tr></table></figure><h2 id="Harbor服务搭建"><a href="#Harbor服务搭建" class="headerlink" title="Harbor服务搭建"></a>Harbor服务搭建</h2><h3 id="从官网下载安装包-推荐离线安装包"><a href="#从官网下载安装包-推荐离线安装包" class="headerlink" title="从官网下载安装包(推荐离线安装包)"></a>从官网下载安装包(推荐离线安装包)</h3><p>因为国内连国外网经常出现慢或者断线的情况，导致安装某些依赖镜像的时候卡住，镜像又不支持断点续传，所以最好使用离线安装包。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 下载harbor-v1.4.0离线安装包</span></span><br><span class="line">$ wget https://storage.googleapis.com/harbor-releases/release-1.4.0/harbor-offline-installer-v1.4.0.tgz</span><br><span class="line">$ tar xf harbor-offline-installer-v1.4.0.tgz</span><br></pre></td></tr></table></figure><h3 id="配置Harbor"><a href="#配置Harbor" class="headerlink" title="配置Harbor"></a>配置Harbor</h3><p>解压缩之后，会有一个<code>Harbor</code>的文件夹，里面会有一个<code>harbor.cfg</code>配置文件，我们需要对里面的参数进行更改。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br></pre></td><td class="code"><pre><span class="line">$ vim harbor.cfg</span><br><span class="line"><span class="comment">## 配置域名或者直接IP地址</span></span><br><span class="line">hostname = harbor-test.com                              <span class="comment">##需要修改</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 访问协议，默认是http，也可以设置https，如果设置https，则nginx ssl需要设置on</span></span><br><span class="line">ui_url_protocol = https</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置进程数</span></span><br><span class="line">max_job_workers = 3 </span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置自定义ssl认证</span></span><br><span class="line">customize_crt = on</span><br><span class="line">ssl_cert = /data/cert/harbor-test.com.crt               <span class="comment">##需要修改</span></span><br><span class="line">ssl_cert_key = /data/cert/harbor-test.com.key           <span class="comment">##需要修改</span></span><br><span class="line">secretkey_path = /data</span><br><span class="line">admiral_url = NA</span><br><span class="line">log_rotate_count = 50</span><br><span class="line">log_rotate_size = 200M</span><br><span class="line"></span><br><span class="line"><span class="comment">## 邮件设置，发送重置密码邮件时使用</span></span><br><span class="line">email_identity = </span><br><span class="line">email_server = smtp.exmail.qq.com                       <span class="comment">##需要修改</span></span><br><span class="line">email_server_port = 645                                 <span class="comment">##需要修改</span></span><br><span class="line">email_username =                                        <span class="comment">##需要修改</span></span><br><span class="line">email_password =                                        <span class="comment">##需要修改</span></span><br><span class="line">email_from = Harbor &lt;xxxx@xxx.com&gt;                      <span class="comment">##需要修改</span></span><br><span class="line">email_ssl = <span class="literal">false</span></span><br><span class="line">email_insecure = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## Harbor界面登录密码</span></span><br><span class="line">harbor_admin_password = Harbor123                       <span class="comment">##需要修改</span></span><br><span class="line">auth_mode = db_auth</span><br><span class="line"></span><br><span class="line"><span class="comment">## LDAP认证时配置项，如果上方auth_mode选择db_auth的话，下方其实没用到，但是注释的话会有报错。</span></span><br><span class="line">ldap_url = ldaps://ldap.mydomain.com</span><br><span class="line">ldap_searchdn = uid=searchuser,ou=people,dc=mydomain,dc=com</span><br><span class="line">ldap_search_pwd = password</span><br><span class="line">ldap_basedn = ou=people,dc=mydomain,dc=com</span><br><span class="line">ldap_uid = uid </span><br><span class="line">ldap_scope = 2 </span><br><span class="line">ldap_timeout = 5</span><br><span class="line">ldap_verify_cert = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 是否开启自注册</span></span><br><span class="line">self_registration = on</span><br><span class="line"></span><br><span class="line"><span class="comment">## Token有效时间，默认30分钟</span></span><br><span class="line">token_expiration = 30</span><br><span class="line"></span><br><span class="line"><span class="comment">## 用户创建项目权限控制，默认是everyone（所有人），也可以设置为adminonly（只能管理员）</span></span><br><span class="line">project_creation_restriction = everyone</span><br><span class="line"></span><br><span class="line"><span class="comment">## 数据库配置相关</span></span><br><span class="line">db_host = mysql</span><br><span class="line">db_password = </span><br><span class="line">db_port = 3306</span><br><span class="line">db_user = root</span><br><span class="line"></span><br><span class="line"><span class="comment">## redis配置，如果有需要，没有的话可以留空</span></span><br><span class="line">redis_url =</span><br><span class="line"></span><br><span class="line"><span class="comment">## clair镜像扫描功能，可以检查镜像操作系统以及上面安装包是否与已知不安全的包版本相匹配，从而提高镜像安全性。</span></span><br><span class="line">clair_db_host = postgres</span><br><span class="line">clair_db_password = password</span><br><span class="line">clair_db_port = 5432</span><br><span class="line">clair_db_username = postgres</span><br><span class="line">clair_db = postgres</span><br><span class="line"></span><br><span class="line"><span class="comment">## 同理，以下设置的是uaa_auth登录方式</span></span><br><span class="line">uaa_endpoint = uaa.mydomain.org</span><br><span class="line">uaa_clientid = id</span><br><span class="line">uaa_clientsecret = secret</span><br><span class="line">uaa_verify_cert = <span class="literal">true</span></span><br><span class="line">uaa_ca_cert = /path/to/ca.pem</span><br><span class="line"></span><br><span class="line"><span class="comment">## 此处的registry后端使用filesystem</span></span><br><span class="line">registry_storage_provider_name = filesystem</span><br><span class="line">registry_storage_provider_config =</span><br></pre></td></tr></table></figure><p>由于上述配置文件中涉及到ssl证书，生成在<code>/data/cert</code>目录下，所以需要执行以下生成命令：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p /data/cert</span><br><span class="line">$ <span class="built_in">cd</span> /data/cert</span><br><span class="line">$ localdomain=harbor-test.com   <span class="comment">##修改为自己的域名</span></span><br><span class="line">$ openssl req -nodes -subj <span class="string">"/C=CN/ST=GuangDong/L=GuangZhou/CN=<span class="variable">$localdomain</span>"</span> -newkey rsa:2048 -keyout <span class="variable">$localdomain</span>.key -out <span class="variable">$localdomain</span>.csr</span><br><span class="line">$ openssl x509 -req -days 3650 -<span class="keyword">in</span> <span class="variable">$localdomain</span>.csr -signkey <span class="variable">$localdomain</span>.key -out <span class="variable">$localdomain</span>.crt</span><br><span class="line">$ openssl x509 -req -<span class="keyword">in</span> <span class="variable">$localdomain</span>.csr -CA <span class="variable">$localdomain</span>.crt -CAkey <span class="variable">$localdomain</span>.key -CAcreateserial -out <span class="variable">$localdomain</span>.crt -days 10000</span><br></pre></td></tr></table></figure></p><p>命令正常完成后应该会有四个文件，后缀名分别是.crt，.csr，.key，.srl，文件名就是对应的域名</p><p>修改完配置文件后，在的当前目录执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line">$ ./install.sh</span><br><span class="line"><span class="comment">## 运行结果</span></span><br><span class="line">[Step 0]: checking installation environment ...</span><br><span class="line"></span><br><span class="line">Note: docker version: 17.12.1</span><br><span class="line"></span><br><span class="line">Note: docker-compose version: 1.18.0</span><br><span class="line"></span><br><span class="line">[Step 1]: loading Harbor images ...</span><br><span class="line">651f69aef02c: Loading layer [==================================================&gt;]  135.8MB/135.8MB</span><br><span class="line">40a1aad64343: Loading layer [==================================================&gt;]  23.24MB/23.24MB</span><br><span class="line">3fe2713e4072: Loading layer [==================================================&gt;]  12.16MB/12.16MB</span><br><span class="line">ba3a1eb0e375: Loading layer [==================================================&gt;]   17.3MB/17.3MB</span><br><span class="line">447427ec5e1a: Loading layer [==================================================&gt;]  15.87kB/15.87kB</span><br><span class="line">4ccb4026663c: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">16faa95946a1: Loading layer [==================================================&gt;]  29.46MB/29.46MB</span><br><span class="line">Loaded image: vmware/notary-server-photon:v0.5.1-v1.4.0</span><br><span class="line">fa7ba9fd42c9: Loading layer [==================================================&gt;]  10.95MB/10.95MB</span><br><span class="line">4e400f9ae23e: Loading layer [==================================================&gt;]   17.3MB/17.3MB</span><br><span class="line">2802fb27c88b: Loading layer [==================================================&gt;]  15.87kB/15.87kB</span><br><span class="line">e6367a4e1e1e: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">8ece8dfcdd98: Loading layer [==================================================&gt;]  28.24MB/28.24MB</span><br><span class="line">Loaded image: vmware/notary-signer-photon:v0.5.1-v1.4.0</span><br><span class="line">a7dd1a8afcaf: Loading layer [==================================================&gt;]  396.7MB/396.7MB</span><br><span class="line">05adebbe496f: Loading layer [==================================================&gt;]  9.216kB/9.216kB</span><br><span class="line">86eb534949fa: Loading layer [==================================================&gt;]  9.216kB/9.216kB</span><br><span class="line">d7f127c69380: Loading layer [==================================================&gt;]   7.68kB/7.68kB</span><br><span class="line">5ac1c4dc5ee9: Loading layer [==================================================&gt;]  1.536kB/1.536kB</span><br><span class="line">d0bec56b5b1a: Loading layer [==================================================&gt;]  9.728kB/9.728kB</span><br><span class="line">4bbe83860556: Loading layer [==================================================&gt;]   2.56kB/2.56kB</span><br><span class="line">e526f9e6769f: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">Loaded image: vmware/harbor-db:v1.4.0</span><br><span class="line">1cff102bbda2: Loading layer [==================================================&gt;]  154.1MB/154.1MB</span><br><span class="line">04c9f3e07de1: Loading layer [==================================================&gt;]  10.75MB/10.75MB</span><br><span class="line">7b6c7bf54f5c: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">42f8acdb7fe3: Loading layer [==================================================&gt;]  48.13kB/48.13kB</span><br><span class="line">5b6299d0a1df: Loading layer [==================================================&gt;]   10.8MB/10.8MB</span><br><span class="line">Loaded image: vmware/clair-photon:v2.0.1-v1.4.0</span><br><span class="line">6534131f457c: Loading layer [==================================================&gt;]  94.76MB/94.76MB</span><br><span class="line">73f582101e4b: Loading layer [==================================================&gt;]  6.656kB/6.656kB</span><br><span class="line">86d847823c48: Loading layer [==================================================&gt;]  6.656kB/6.656kB</span><br><span class="line">Loaded image: vmware/postgresql-photon:v1.4.0</span><br><span class="line">5cd250d5a352: Loading layer [==================================================&gt;]  23.24MB/23.24MB</span><br><span class="line">ad3fd52b54f3: Loading layer [==================================================&gt;]  14.99MB/14.99MB</span><br><span class="line">13b1e24cc368: Loading layer [==================================================&gt;]  14.99MB/14.99MB</span><br><span class="line">Loaded image: vmware/harbor-adminserver:v1.4.0</span><br><span class="line">c26c69706710: Loading layer [==================================================&gt;]  23.24MB/23.24MB</span><br><span class="line">223f6fe02cc8: Loading layer [==================================================&gt;]  23.45MB/23.45MB</span><br><span class="line">1fc843c8698a: Loading layer [==================================================&gt;]  7.168kB/7.168kB</span><br><span class="line">e09293610ee7: Loading layer [==================================================&gt;]  10.39MB/10.39MB</span><br><span class="line">d59f9780b1d8: Loading layer [==================================================&gt;]  23.44MB/23.44MB</span><br><span class="line">Loaded image: vmware/harbor-ui:v1.4.0</span><br><span class="line">dd4753242e59: Loading layer [==================================================&gt;]  73.07MB/73.07MB</span><br><span class="line">95aed61ca251: Loading layer [==================================================&gt;]  3.584kB/3.584kB</span><br><span class="line">1864f9818562: Loading layer [==================================================&gt;]  3.072kB/3.072kB</span><br><span class="line">da2a19f80b81: Loading layer [==================================================&gt;]  4.096kB/4.096kB</span><br><span class="line">058531639e75: Loading layer [==================================================&gt;]  3.584kB/3.584kB</span><br><span class="line">a84e69fb619b: Loading layer [==================================================&gt;]  10.24kB/10.24kB</span><br><span class="line">Loaded image: vmware/harbor-log:v1.4.0</span><br><span class="line">b1056051f246: Loading layer [==================================================&gt;]  23.24MB/23.24MB</span><br><span class="line">07678065e08b: Loading layer [==================================================&gt;]  19.19MB/19.19MB</span><br><span class="line">a2d9bdb8f5fb: Loading layer [==================================================&gt;]  19.19MB/19.19MB</span><br><span class="line">Loaded image: vmware/harbor-jobservice:v1.4.0</span><br><span class="line">7f58ce57cd5e: Loading layer [==================================================&gt;]  4.805MB/4.805MB</span><br><span class="line">Loaded image: vmware/nginx-photon:v1.4.0</span><br><span class="line">4c8965978b77: Loading layer [==================================================&gt;]  23.24MB/23.24MB</span><br><span class="line">1466c942edde: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">ac5c17331735: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">86824c7c466a: Loading layer [==================================================&gt;]  2.048kB/2.048kB</span><br><span class="line">fd3bd0e70d67: Loading layer [==================================================&gt;]   22.8MB/22.8MB</span><br><span class="line">b02195d77636: Loading layer [==================================================&gt;]   22.8MB/22.8MB</span><br><span class="line">Loaded image: vmware/registry-photon:v2.6.2-v1.4.0</span><br><span class="line">Loaded image: vmware/photon:1.0</span><br><span class="line">Loaded image: vmware/mariadb-photon:v1.4.0</span><br><span class="line">454c81edbd3b: Loading layer [==================================================&gt;]  135.2MB/135.2MB</span><br><span class="line">e99db1275091: Loading layer [==================================================&gt;]  395.4MB/395.4MB</span><br><span class="line">051e4ee23882: Loading layer [==================================================&gt;]  9.216kB/9.216kB</span><br><span class="line">6cca4437b6f6: Loading layer [==================================================&gt;]  9.216kB/9.216kB</span><br><span class="line">1d48fc08c8bc: Loading layer [==================================================&gt;]   7.68kB/7.68kB</span><br><span class="line">0419724fd942: Loading layer [==================================================&gt;]  1.536kB/1.536kB</span><br><span class="line">526b2156bd7a: Loading layer [==================================================&gt;]  637.8MB/637.8MB</span><br><span class="line">9ebf6900ecbd: Loading layer [==================================================&gt;]  78.34kB/78.34kB</span><br><span class="line">Loaded image: vmware/harbor-db-migrator:1.4</span><br><span class="line"></span><br><span class="line">[Step 2]: preparing environment ...</span><br><span class="line">Clearing the configuration file: ./common/config/adminserver/env</span><br><span class="line">Clearing the configuration file: ./common/config/ui/env</span><br><span class="line">Clearing the configuration file: ./common/config/ui/app.conf</span><br><span class="line">Clearing the configuration file: ./common/config/ui/private_key.pem</span><br><span class="line">Clearing the configuration file: ./common/config/db/env</span><br><span class="line">Clearing the configuration file: ./common/config/jobservice/env</span><br><span class="line">Clearing the configuration file: ./common/config/jobservice/app.conf</span><br><span class="line">Clearing the configuration file: ./common/config/registry/config.yml</span><br><span class="line">Clearing the configuration file: ./common/config/registry/root.crt</span><br><span class="line">Clearing the configuration file: ./common/config/nginx/cert/harbor.qtz.crt</span><br><span class="line">Clearing the configuration file: ./common/config/nginx/cert/harbor.qtz.key</span><br><span class="line">Clearing the configuration file: ./common/config/nginx/nginx.conf</span><br><span class="line">Clearing the configuration file: ./common/config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">loaded secret from file: /data/secretkey</span><br><span class="line">Generated configuration file: ./common/config/nginx/nginx.conf</span><br><span class="line">Generated configuration file: ./common/config/adminserver/env</span><br><span class="line">Generated configuration file: ./common/config/ui/env</span><br><span class="line">Generated configuration file: ./common/config/registry/config.yml</span><br><span class="line">Generated configuration file: ./common/config/db/env</span><br><span class="line">Generated configuration file: ./common/config/jobservice/env</span><br><span class="line">Generated configuration file: ./common/config/<span class="built_in">log</span>/logrotate.conf</span><br><span class="line">Generated configuration file: ./common/config/jobservice/app.conf</span><br><span class="line">Generated configuration file: ./common/config/ui/app.conf</span><br><span class="line">Generated certificate, key file: ./common/config/ui/private_key.pem, cert file: ./common/config/registry/root.crt</span><br><span class="line">The configuration files are ready, please use docker-compose to start the service.</span><br><span class="line"></span><br><span class="line">Creating harbor-log ... <span class="keyword">done</span></span><br><span class="line">[Step 3]: checking existing instance of Harbor ...</span><br><span class="line"></span><br><span class="line">Creating registry ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-ui ... <span class="keyword">done</span></span><br><span class="line">Creating network <span class="string">"harbor_harbor"</span> with the default driver</span><br><span class="line">Creating nginx ... <span class="keyword">done</span></span><br><span class="line">Creating harbor-db ... </span><br><span class="line">Creating harbor-adminserver ... </span><br><span class="line">Creating registry ... </span><br><span class="line">Creating harbor-ui ... </span><br><span class="line">Creating harbor-jobservice ... </span><br><span class="line">Creating nginx ... </span><br><span class="line"></span><br><span class="line">✔ ----Harbor has been installed and started successfully.----</span><br><span class="line"></span><br><span class="line">Now you should be able to visit the admin portal at https://harbor.qtz. </span><br><span class="line">For more details, please visit https://github.com/vmware/harbor .</span><br></pre></td></tr></table></figure><h2 id="查看启动后状态"><a href="#查看启动后状态" class="headerlink" title="查看启动后状态"></a>查看启动后状态</h2><ul><li>Docker</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ docker images</span><br><span class="line">REPOSITORY                                TAG                 IMAGE ID            CREATED             SIZE</span><br><span class="line">vmware/clair-photon                       v2.0.1-v1.4.0       a1df3526fe43        6 weeks ago         300MB</span><br><span class="line">vmware/notary-server-photon               v0.5.1-v1.4.0       3edfddb8ece2        6 weeks ago         211MB</span><br><span class="line">vmware/notary-signer-photon               v0.5.1-v1.4.0       cc70a05cdb6a        6 weeks ago         209MB</span><br><span class="line">vmware/registry-photon                    v2.6.2-v1.4.0       8920f621ddd1        6 weeks ago         198MB</span><br><span class="line">vmware/nginx-photon                       v1.4.0              20c8a01ac6ab        6 weeks ago         135MB</span><br><span class="line">vmware/harbor-log                         v1.4.0              9e818c7a27ab        6 weeks ago         200MB</span><br><span class="line">vmware/harbor-jobservice                  v1.4.0              29c14d91b043        6 weeks ago         191MB</span><br><span class="line">vmware/harbor-ui                          v1.4.0              6cb4318eda6a        6 weeks ago         209MB</span><br><span class="line">vmware/harbor-adminserver                 v1.4.0              8145970fa013        6 weeks ago         182MB</span><br><span class="line">vmware/harbor-db                          v1.4.0              c38da34727f0        6 weeks ago         521MB</span><br><span class="line">vmware/mariadb-photon                     v1.4.0              8457013cf6e3        6 weeks ago         521MB</span><br><span class="line">vmware/postgresql-photon                  v1.4.0              59aa61520094        6 weeks ago         221MB</span><br><span class="line">vmware/harbor-db-migrator                 1.4                 7a4d871b612e        2 months ago        1.15GB</span><br><span class="line">vmware/photon                             1.0                 9b411d78ad9e        2 months ago        130MB</span><br></pre></td></tr></table></figure><ul><li>Docker-Compose</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose ps</span><br><span class="line">       Name                     Command               State                                Ports                              </span><br><span class="line">------------------------------------------------------------------------------------------------------------------------------</span><br><span class="line">harbor-adminserver   /harbor/start.sh                 Up                                                                      </span><br><span class="line">harbor-db            /usr/<span class="built_in">local</span>/bin/docker-entr ...   Up      3306/tcp                                                        </span><br><span class="line">harbor-jobservice    /harbor/start.sh                 Up                                                                      </span><br><span class="line">harbor-log           /bin/sh -c /usr/<span class="built_in">local</span>/bin/ ...   Up      127.0.0.1:1514-&gt;10514/tcp                                       </span><br><span class="line">harbor-ui            /harbor/start.sh                 Up                                                                      </span><br><span class="line">nginx                nginx -g daemon off;             Up      0.0.0.0:443-&gt;443/tcp, 0.0.0.0:4443-&gt;4443/tcp, 0.0.0.0:80-&gt;80/tcp</span><br><span class="line">registry             /entrypoint.sh serve /etc/ ...   Up      5000/tcp</span><br></pre></td></tr></table></figure><h2 id="启动后"><a href="#启动后" class="headerlink" title="启动后"></a>启动后</h2><p>启动完成后，我们访问刚设置的hostname即可，<code>https://harbor-test.com</code></p><p>如果端口占用，我们可以去修改docker-compose.yml文件中，对应服务的端口映射。</p><p>登录<code>https://harbor-test.com</code></p><p>用户名和密码在配置文件中可以查看，这里我设置的是admin/Harbor123</p><p><img src="/images/harbor-login.png" alt="harbor-login"></p><p>创建自己的项目</p><p><img src="/images/harbor-create-project.png" alt="harbor-create-project"></p><p>创建完之后点击对应项目，里面会有教程如何推送/拉取镜像</p><p><img src="/images/harbor-push.png" alt="harbor-push"></p><h2 id="Docker对应相关配置"><a href="#Docker对应相关配置" class="headerlink" title="Docker对应相关配置"></a>Docker对应相关配置</h2><p>以CentOS 7 为例，安装Harbor镜像仓库之后，使用<code>docker login</code>登录会出现certificate signed by unknown authority类似这样的错误，原因是Harbor如果是部署在内网的话没有HTTPS证书。</p><p>对于这种情况，一般有两个解决方法：</p><ul><li><p>部署到外网，然后添加HTTPS证书，如Certbot</p></li><li><p>Docker配置文件中加–insecure-registry参数</p></li></ul><p>这里就说一下怎么添加–insecure-registry参数</p><p>找到Docker的配置文件<code>/usr/lib/systemd/system/docker.service</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ vim /usr/lib/systemd/system/docker.service</span><br><span class="line"></span><br><span class="line"><span class="comment">## 找到对应行ExecStart= 并修改</span></span><br><span class="line">ExecStart=/usr/bin/dockerd --insecure-registry harbor-test.com</span><br><span class="line"></span><br><span class="line"><span class="comment">## 先reload再重启docker服务</span></span><br><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ systemctl restart docker</span><br></pre></td></tr></table></figure><p>再尝试使用<code>docker login harbor-test.com</code>就可以登录成功并使用仓库进行推送/拉取镜像。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是Docker镜像仓库Harbor在CentOS 7环境下搭建全过程&lt;/p&gt;
&lt;h2 id=&quot;简介&quot;&gt;&lt;a href=&quot;#简介&quot; class=&quot;headerlink&quot; title=&quot;简介&quot;&gt;&lt;/a&gt;简介&lt;/h2&gt;&lt;p&gt;&lt;img src=&quot;/images/harb
      
    
    </summary>
    
      <category term="Docker镜像仓库Harbor搭建" scheme="http://yoursite.com/categories/Docker%E9%95%9C%E5%83%8F%E4%BB%93%E5%BA%93Harbor%E6%90%AD%E5%BB%BA/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
      <category term="Registry" scheme="http://yoursite.com/tags/Registry/"/>
    
      <category term="Harbor" scheme="http://yoursite.com/tags/Harbor/"/>
    
  </entry>
  
  <entry>
    <title>Docker安装部署教程</title>
    <link href="http://yoursite.com/2018/03/11/docker-install/"/>
    <id>http://yoursite.com/2018/03/11/docker-install/</id>
    <published>2018-03-10T16:00:00.000Z</published>
    <updated>2018-03-23T08:19:02.486Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是CentOS 7下安装部署Docker教程</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>系统：CentOS 7 最小安装版</li></ul><h2 id="安装依赖"><a href="#安装依赖" class="headerlink" title="安装依赖"></a>安装依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum update</span><br><span class="line">$ yum install -y yum-utils device-mapper-persistent-data lvm2</span><br></pre></td></tr></table></figure><h2 id="安装Docker-CE"><a href="#安装Docker-CE" class="headerlink" title="安装Docker-CE"></a>安装Docker-CE</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 官方方法添加yum源</span></span><br><span class="line">$ yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="comment">## 国内访问以上官方会很慢，所以推荐使用阿里云源</span></span><br><span class="line">$ vim /etc/yum.repo.d/docker.repo</span><br><span class="line">[docker-ce-stable]</span><br><span class="line">name=Docker CE Stable - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-stable-debuginfo]</span><br><span class="line">name=Docker CE Stable - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/debug-<span class="variable">$basearch</span>/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-stable-source]</span><br><span class="line">name=Docker CE Stable - Sources</span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="built_in">source</span>/stable</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-edge]</span><br><span class="line">name=Docker CE Edge - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/edge</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-edge-debuginfo]</span><br><span class="line">name=Docker CE Edge - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/debug-<span class="variable">$basearch</span>/edge</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-edge-source]</span><br><span class="line">name=Docker CE Edge - Sources</span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="built_in">source</span>/edge</span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-test]</span><br><span class="line">name=Docker CE Test - <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="variable">$basearch</span>/<span class="built_in">test</span></span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-test-debuginfo]</span><br><span class="line">name=Docker CE Test - Debuginfo <span class="variable">$basearch</span></span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/debug-<span class="variable">$basearch</span>/<span class="built_in">test</span></span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line">[docker-ce-test-source]</span><br><span class="line">name=Docker CE Test - Sources</span><br><span class="line">baseurl=https://download.docker.com/linux/centos/7/<span class="built_in">source</span>/<span class="built_in">test</span></span><br><span class="line">enabled=0</span><br><span class="line">gpgcheck=1</span><br><span class="line">gpgkey=https://download.docker.com/linux/centos/gpg</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">## 再安装docker-ce</span></span><br><span class="line">$ yum install -y docker-ce</span><br></pre></td></tr></table></figure><p><strong>可选：如果你想安装最新版本的Docker CE，你需要启用这些仓库（默认是关闭的）</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ yum-config-manager --<span class="built_in">enable</span> docker-ce-edge</span><br><span class="line">$ yum-config-manager --<span class="built_in">enable</span> docker-ce-test</span><br></pre></td></tr></table></figure><h2 id="启动服务"><a href="#启动服务" class="headerlink" title="启动服务"></a>启动服务</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl start docker</span><br><span class="line"></span><br><span class="line"><span class="comment">## 设置开机启动</span></span><br><span class="line">$ systemctl <span class="built_in">enable</span> docker</span><br></pre></td></tr></table></figure><h2 id="国内镜像市场"><a href="#国内镜像市场" class="headerlink" title="国内镜像市场"></a>国内镜像市场</h2><p>由于官方的镜像仓库在国外，所以很多时候拉了一半就会断开，每次也不会有断开重连机制，所以每一次的断开就是重新再下载的过程。</p><p>因此这里收集一些常用的国内镜像仓库地址：</p><ul><li><p><a href="http://hub.daocloud.io/" target="_blank" rel="noopener">DaoCloud</a></p></li><li><p><a href="https://hub.tenxcloud.com/" target="_blank" rel="noopener">时速云</a></p></li><li><p><a href="https://hub.alauda.cn/" target="_blank" rel="noopener">灵雀云</a></p></li></ul><p>这几个国内比较常用的镜像市场。</p><h2 id="镜像加速器"><a href="#镜像加速器" class="headerlink" title="镜像加速器"></a>镜像加速器</h2><p>此外，除了使用国内镜像仓库的同时，也可配合镜像加速器来下载国外官方镜像。</p><p>比如阿里云、腾讯云、网易云也有做类似的功能，以下用阿里云为例子：</p><p>关于加速器的地址，你只需要登录容器<a href="https://cr.console.aliyun.com/?spm=a2c4e.11153959.blogcont29941.9.520269d6odTEcW" target="_blank" rel="noopener">Hub服务</a>的控制台，左侧的加速器帮助页面就会显示为你独立分配的加速地址。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#例如：</span></span><br><span class="line"><span class="comment">#公网Mirror：[系统分配前缀].mirror.aliyuncs.com</span></span><br></pre></td></tr></table></figure><h3 id="当你的docker版本较新时"><a href="#当你的docker版本较新时" class="headerlink" title="当你的docker版本较新时"></a>当你的docker版本较新时</h3><p>当你下载安装的Docker Version不低于1.10时，建议直接通过daemon config进行配置。<br>使用配置文件 /etc/docker/daemon.json（没有时新建该文件）</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    "registry-mirrors": ["&lt;your accelerate address&gt;"]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>以CentOS 7为例</strong></p><p>CentOS的配置方式略微复杂，需要先将默认的配置文件复制出来<br>/lib/systemd/system/docker.service -&gt; /etc/systemd/system/docker.service<br>然后再将加速器地址添加到配置文件的启动命令<br>重启Docker就可以了。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ cp -n /lib/systemd/system/docker.service /etc/systemd/system/docker.service</span><br><span class="line">$ sed -i <span class="string">"s|ExecStart=/usr/bin/docker daemon|ExecStart=/usr/bin/docker daemon --registry-mirror=&lt;your accelerate address&gt;|g"</span> /etc/systemd/system/docker.service</span><br><span class="line">$ sed -i <span class="string">"s|ExecStart=/usr/bin/dockerd|ExecStart=/usr/bin/dockerd --registry-mirror=&lt;your accelerate address&gt;|g"</span> /etc/systemd/system/docker.service</span><br><span class="line">$ systemctl daemon-reload</span><br><span class="line">$ service docker restart</span><br></pre></td></tr></table></figure><h3 id="阿里云加速器"><a href="#阿里云加速器" class="headerlink" title="阿里云加速器"></a>阿里云加速器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sudo mkdir -p /etc/docker</span><br><span class="line">sudo tee /etc/docker/daemon.json &lt;&lt;-<span class="string">'EOF'</span></span><br><span class="line">&#123;</span><br><span class="line">  <span class="string">"registry-mirrors"</span>: [<span class="string">"https://sdu7uwdm.mirror.aliyuncs.com"</span>]</span><br><span class="line">&#125;</span><br><span class="line">EOF</span><br><span class="line">sudo systemctl daemon-reload</span><br><span class="line">sudo systemctl restart docker</span><br></pre></td></tr></table></figure><h3 id="DaoCloud加速器"><a href="#DaoCloud加速器" class="headerlink" title="DaoCloud加速器"></a>DaoCloud加速器</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ curl -sSL https://get.daocloud.io/daotools/set_mirror.sh | sh -s http://942c18cb.m.daocloud.io</span><br></pre></td></tr></table></figure><h2 id="一键安装脚本（CentOS-7）"><a href="#一键安装脚本（CentOS-7）" class="headerlink" title="一键安装脚本（CentOS 7）"></a>一键安装脚本（CentOS 7）</h2><p><a href="https://raw.githubusercontent.com/Polaris0112/Ops-Tools/master/docker/docker-install.sh" target="_blank" rel="noopener">下载地址</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是CentOS 7下安装部署Docker教程&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;系统：CentOS 7 最小安装版&lt;/l
      
    
    </summary>
    
      <category term="Docker安装部署教程" scheme="http://yoursite.com/categories/Docker%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="Docker" scheme="http://yoursite.com/tags/Docker/"/>
    
      <category term="CentOS" scheme="http://yoursite.com/tags/CentOS/"/>
    
  </entry>
  
  <entry>
    <title>通过packstack快速安装openstack</title>
    <link href="http://yoursite.com/2018/03/10/openstack-packstack-installation/"/>
    <id>http://yoursite.com/2018/03/10/openstack-packstack-installation/</id>
    <published>2018-03-09T16:00:00.000Z</published>
    <updated>2018-03-16T08:27:50.260Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是通过packstack快速安装openstack</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li><p>系统：CentOS 7 最小安装版</p></li><li><p>CPU：2+</p></li><li><p>内存：6GB+</p></li><li><p>硬盘：50GB+</p></li><li><p>网卡：2个</p></li></ul><h2 id="配置pike版本openstack源（此处采用阿里源）"><a href="#配置pike版本openstack源（此处采用阿里源）" class="headerlink" title="配置pike版本openstack源（此处采用阿里源）"></a>配置pike版本openstack源（此处采用阿里源）</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ cat &lt;&lt;END &gt;/etc/yum.repos.d/pike.repo</span><br><span class="line">[pike]</span><br><span class="line">name=pike</span><br><span class="line">baseurl=http://mirrors.aliyun.com/centos/7.4.1708/cloud/x86_64/openstack-pike/</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line">END</span><br></pre></td></tr></table></figure><h2 id="安装epel源"><a href="#安装epel源" class="headerlink" title="安装epel源"></a>安装epel源</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install epel-release -y</span><br></pre></td></tr></table></figure><h2 id="安装packstack安装工具和依赖"><a href="#安装packstack安装工具和依赖" class="headerlink" title="安装packstack安装工具和依赖"></a>安装packstack安装工具和依赖</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y centos-release-openstack-ocata  centos-release-openstack-mitaka  </span><br><span class="line">$ yum update -y</span><br><span class="line">$ yum -y install openstack-packstack</span><br></pre></td></tr></table></figure><h2 id="安装allinone的的环境"><a href="#安装allinone的的环境" class="headerlink" title="安装allinone的的环境"></a>安装allinone的的环境</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ packstack --allinone</span><br></pre></td></tr></table></figure><p><strong>注意：</strong></p><ul><li><p>安装大概一个小时左右，由于网络问题可能会报错，报错后重复执行即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ packstack --answer-file packstack-answers-20171129-031548.txt</span><br></pre></td></tr></table></figure></li><li><p>自定义安装<br>如果需要自定义参数，需要先生存应答文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ packstack --gen-answer-file=packstack-answers</span><br></pre></td></tr></table></figure></li></ul><p>通过应答文件执行安装<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ packstack --answer-file=packstack-answers</span><br></pre></td></tr></table></figure></p><p><strong>注意：用户名和密码在执行命令的当前目录下的keystonerc_admin中</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> keystonerc_admin</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看用户</span></span><br><span class="line">$ openstack user list </span><br><span class="line">+----------------------------------+------------+</span><br><span class="line">| ID                               | Name       |</span><br><span class="line">+----------------------------------+------------+</span><br><span class="line">| 15b05802627f4f42b4ae3253f5ce6730 | cinder     |</span><br><span class="line">| 2ac16759b27f480da9adff72188c79f3 | glance     |</span><br><span class="line">| 2aced06e7c984399a7cc799de9199244 | aodh       |</span><br><span class="line">| 3fdc02269da74e67be50c1d2f8711f3b | gnocchi    |</span><br><span class="line">| 70a112fdec0842979e8e9e5e9f171791 | nova       |</span><br><span class="line">| 866962fe0ee14c379dc6245a8ee5a707 | placement  |</span><br><span class="line">| a1c35ba084554dd1bca310667a82bdd2 | ceilometer |</span><br><span class="line">| bce952e6b65641aa93aab872d04e7c0c | admin      |</span><br><span class="line">| c2082398028442a7bff898fb6745af57 | swift      |</span><br><span class="line">| e43f38301fed4a44ae070b6e7b0a0026 | neutron    |</span><br><span class="line">| fbbd1aff07fd4b48a0020bddd2c8deea | demo       |</span><br><span class="line">+----------------------------------+------------+</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看项目</span></span><br><span class="line">$ openstack project list </span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| ID                               | Name     |</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line">| 2bbfb410124f4454a45f012a6274eff7 | demo     |</span><br><span class="line">| 400487fe18a541f1a3c4c462b4ff9fa6 | admin    |</span><br><span class="line">| 8b051a62445f4106a17f5521cd713841 | services |</span><br><span class="line">+----------------------------------+----------+</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看服务</span></span><br><span class="line">$ openstack service list </span><br><span class="line">+----------------------------------+------------+--------------+</span><br><span class="line">| ID                               | Name       | Type         |</span><br><span class="line">+----------------------------------+------------+--------------+</span><br><span class="line">| 128e1c41dae648fd84266be576ab6b8e | aodh       | alarming     |</span><br><span class="line">| 1549860212bd444eb96ebe3399e1af05 | cinderv2   | volumev2     |</span><br><span class="line">| 5144ecf07843499aa76a1c5bdeafcfa2 | nova       | compute      |</span><br><span class="line">| 5670a37cdd7e48c4b77b744082dfd5cd | cinder     | volume       |</span><br><span class="line">| 7963a27f90de40ed900bcff7f0771ab9 | swift      | object-store |</span><br><span class="line">| 9aa244907a4e428b8a8c996e086a518f | placement  | placement    |</span><br><span class="line">| a967ff55c55143439e899cbd0530af6b | neutron    | network      |</span><br><span class="line">| c06cc37f779a4163b147fca99ec3cc33 | keystone   | identity     |</span><br><span class="line">| c8a39eec1cac44719bbb15b883d62735 | glance     | image        |</span><br><span class="line">| d8871026f6cc4c00b25bcfcf6ae05064 | ceilometer | metering     |</span><br><span class="line">| f2b5cd4ee2344f389e6eb1e9a8fbccfc | gnocchi    | metric       |</span><br><span class="line">| fccbbd2dbf3141acbe8a2291df1b67a1 | cinderv3   | volumev3     |</span><br><span class="line">+----------------------------------+------------+--------------+</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看catalog</span></span><br><span class="line">$ openstack catalog list </span><br><span class="line">+------------+--------------+----------------------------------------------------------------------------+</span><br><span class="line">| Name       | Type         | Endpoints                                                                  |</span><br><span class="line">+------------+--------------+----------------------------------------------------------------------------+</span><br><span class="line">| aodh       | alarming     | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8042                                       |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8042                                         |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8042                                          |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| cinderv2   | volumev2     | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8776/v2/400487fe18a541f1a3c4c462b4ff9fa6   |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8776/v2/400487fe18a541f1a3c4c462b4ff9fa6      |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8776/v2/400487fe18a541f1a3c4c462b4ff9fa6     |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| nova       | compute      | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8774/v2.1/400487fe18a541f1a3c4c462b4ff9fa6    |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8774/v2.1/400487fe18a541f1a3c4c462b4ff9fa6 |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8774/v2.1/400487fe18a541f1a3c4c462b4ff9fa6   |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| cinder     | volume       | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8776/v1/400487fe18a541f1a3c4c462b4ff9fa6   |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8776/v1/400487fe18a541f1a3c4c462b4ff9fa6     |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8776/v1/400487fe18a541f1a3c4c462b4ff9fa6      |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| swift      | object-store | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8080/v1/AUTH_400487fe18a541f1a3c4c462b4ff9fa6|</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8080/v1/AUTH_400487fe18a541f1a3c4c462b4ff9fa6 |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8080/v1/AUTH_400487fe18a541f1a3c462b4ff9fa6|</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| placement  | placement    | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8778/placement                               |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8778/placement                                |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8778/placement                             |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| neutron    | network      | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:9696                                         |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:9696                                       |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:9696                                          |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| keystone   | identity     | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:35357/v3                                      |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:5000/v3                                      |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:5000/v3                                    |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| glance     | image        | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:9292                                         |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:9292                                       |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:9292                                          |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| ceilometer | metering     | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8777                                          |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8777                                       |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8777                                         |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| gnocchi    | metric       | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8041                                         |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8041                                       |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8041                                          |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">| cinderv3   | volumev3     | RegionOne                                                                  |</span><br><span class="line">|            |              |   public: http://172.16.8.100:8776/v3/400487fe18a541f1a3c4c462b4ff9fa6     |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   internal: http://172.16.8.100:8776/v3/400487fe18a541f1a3c4c462b4ff9fa6   |</span><br><span class="line">|            |              | RegionOne                                                                  |</span><br><span class="line">|            |              |   admin: http://172.16.8.100:8776/v3/400487fe18a541f1a3c4c462b4ff9fa6      |</span><br><span class="line">|            |              |                                                                            |</span><br><span class="line">+------------+--------------+----------------------------------------------------------------------------+</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是通过packstack快速安装openstack&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;系统：CentOS 7 最小
      
    
    </summary>
    
      <category term="通过packstack快速安装openstack" scheme="http://yoursite.com/categories/%E9%80%9A%E8%BF%87packstack%E5%BF%AB%E9%80%9F%E5%AE%89%E8%A3%85openstack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="PackStack" scheme="http://yoursite.com/tags/PackStack/"/>
    
  </entry>
  
  <entry>
    <title>通过devstack安装部署OpenStack</title>
    <link href="http://yoursite.com/2018/03/10/openstack-devstack-installation/"/>
    <id>http://yoursite.com/2018/03/10/openstack-devstack-installation/</id>
    <published>2018-03-09T16:00:00.000Z</published>
    <updated>2018-03-16T08:35:12.071Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是通过devstack安装部署OpenStack</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>系统：CentOS 7 最小安装版</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/selinux/config  </span><br><span class="line"><span class="comment">#SELINUX=enforcing       //注释掉  </span></span><br><span class="line">SELINUX=disabled         //增加  </span><br><span class="line"></span><br><span class="line">$ <span class="built_in">cd</span> /home  </span><br><span class="line">$ git <span class="built_in">clone</span> http://git.trystack.cn/openstack-dev/devstack.git -b stable/ocata</span><br></pre></td></tr></table></figure><p>注意一定要使用 -b stable/ocata 指定devstack版本（与要安装的openstack版本一致），否则基本上不会安装成功！</p><h2 id="创建stack用户"><a href="#创建stack用户" class="headerlink" title="创建stack用户"></a>创建stack用户</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /home/devstack/tools  </span><br><span class="line">$ ./create-stack-user.sh  </span><br><span class="line">$ chown -R stack:stack /home/devstack  </span><br><span class="line">$ chmod 777 /opt/stack -R  </span><br><span class="line">$ chmod 777 /var/run/openvswitch/db.sock  </span><br><span class="line">$ su stack</span><br></pre></td></tr></table></figure><h2 id="创建local-conf文件"><a href="#创建local-conf文件" class="headerlink" title="创建local.conf文件"></a>创建local.conf文件</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">[[<span class="built_in">local</span>|localrc]]  </span><br><span class="line">GIT_BASE=http://git.trystack.cn  </span><br><span class="line">NOVNC_REPO=http://git.trystack.cn/kanaka/noVNC.git  </span><br><span class="line">SPICE_REPO=http://git.trystack.cn/git/spice/spice-html5.git  </span><br><span class="line">RECLONE=True  </span><br><span class="line">DOWNLOAD_DEFAULT_IMAGES=False  </span><br><span class="line">IMAGE_URLS=http://download.cirros-cloud.net/0.3.4/cirros-0.3.4-x86_64-disk.img  </span><br><span class="line">ADMIN_PASSWORD=openstack  </span><br><span class="line">DATABASE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span>  </span><br><span class="line">RABBIT_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span>  </span><br><span class="line">SERVICE_PASSWORD=<span class="variable">$ADMIN_PASSWORD</span>  </span><br><span class="line">HOST_IP=192.168.0.120  </span><br><span class="line">LOGFILE=/opt/stack/logs/stack.sh.log  </span><br><span class="line">LOGDAYS=2  </span><br><span class="line">LOG_COLOR=True  </span><br><span class="line">HORIZON_BRANCH=stable/ocata  </span><br><span class="line">KEYSTONE_BRANCH=stable/ocata  </span><br><span class="line">NOVA_BRANCH=stable/ocata  </span><br><span class="line">NEUTRON_BRANCH=stable/ocata  </span><br><span class="line">GLANCE_BRANCH=stable/ocata  </span><br><span class="line">CINDER_BRANCH=stable/ocata  </span><br><span class="line">KEYSTONE_TOKEN_FORMAT=UUID  </span><br><span class="line"><span class="comment"># Enabling Neutron (network) Service  </span></span><br><span class="line">disable_service n-net  </span><br><span class="line">enable_service q-svc  </span><br><span class="line">enable_service q-agt  </span><br><span class="line">enable_service q-dhcp  </span><br><span class="line">enable_service q-l3  </span><br><span class="line">enable_service q-meta  </span><br><span class="line">enable_service q-metering  </span><br><span class="line">enable_service neutron  </span><br><span class="line"><span class="comment"># VLAN configuration  </span></span><br><span class="line">Q_PLUGIN=ml2  </span><br><span class="line">ENABLE_TENANT_VLANS=True  </span><br><span class="line">TENANT_VLAN_RANGE=1100:2999  </span><br><span class="line">SWIFT_HASH=66a3d6b56c1f479c8b4e70ab5c2000f5  </span><br><span class="line">SWIFT_REPLICAS=1  </span><br><span class="line">SWIFT_DATA_DIR=<span class="variable">$DEST</span>/data</span><br></pre></td></tr></table></figure><h2 id="stackrc文件配置修改"><a href="#stackrc文件配置修改" class="headerlink" title="stackrc文件配置修改"></a>stackrc文件配置修改</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只需修改一处，改成如下即可，使用https而不是用git，因为git被阻塞了，不能访问下载编译中需要的文件  </span></span><br><span class="line"><span class="comment"># Base GIT Repo URL  </span></span><br><span class="line"><span class="comment"># Another option is https://git.openstack.org  </span></span><br><span class="line"><span class="comment">#GIT_BASE=$&#123;GIT_BASE:-git://git.openstack.org&#125;  </span></span><br><span class="line">GIT_BASE=<span class="variable">$&#123;GIT_BASE:-https://git.openstack.org&#125;</span></span><br></pre></td></tr></table></figure><h2 id="开始安装"><a href="#开始安装" class="headerlink" title="开始安装"></a>开始安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[stack@localhost devstack]$ ./stack.sh  </span><br><span class="line">（注意：除首次安装外，每次执行./stack.sh前都要执行./unstack.sh,否则很容易报错，安装失败）</span><br></pre></td></tr></table></figure><h2 id="遇到错误"><a href="#遇到错误" class="headerlink" title="遇到错误"></a>遇到错误</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">2018-03-04 06:40:36.101 | Collecting libvirt-python===2.5.0 (from -c /opt/stack/requirements/upper-constraints.txt (line 188))  </span><br><span class="line">2018-03-04 06:40:42.129 |   Downloading http://pypi.doubanio.com/packages/42/e2/40956b304fc69ba4947929398bdaf116c5e4f8a4db1dd57060a45ca84ff1/libvirt-python-2.5.0.tar.gz (172kB)  </span><br><span class="line">2018-03-04 06:40:43.172 | Building wheels <span class="keyword">for</span> collected packages: libvirt-python  </span><br><span class="line">2018-03-04 06:40:43.175 |   Running setup.py bdist_wheel <span class="keyword">for</span> libvirt-python: started  </span><br><span class="line">2018-03-04 06:40:43.841 |   Running setup.py bdist_wheel <span class="keyword">for</span> libvirt-python: finished with status <span class="string">'error'</span>  </span><br><span class="line">2018-03-04 06:40:43.841 |   Complete output from <span class="built_in">command</span> /usr/bin/python2 -u -c <span class="string">"import setuptools, tokenize;__file__='/tmp/pip-build-KkW7rf/libvirt-python/setup.py';f=getattr(tokenize, 'open', open)(__file__);code=f.read().replace('\r\n', '\n');f.close();exec(compile(code, __file__, 'exec'))"</span> bdist_wheel -d /tmp/tmpoNOPZYpip-wheel- --python-tag cp27:  </span><br><span class="line">2018-03-04 06:40:43.841 |   running bdist_wheel  </span><br><span class="line">2018-03-04 06:40:43.841 |   running build</span><br></pre></td></tr></table></figure><p>参考:</p><ul><li><a href="https://stackoverflow.com/questions/42098592/libvirt-python-install-on-mac-error" target="_blank" rel="noopener">地址1</a></li><li><a href="http://blog.csdn.net/lyjshen/article/details/69467444" target="_blank" rel="noopener">地址2</a></li></ul><p>解决:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ pip install  https://libvirt.org/sources/python/libvirt-python-3.0.0.tar.gz  </span><br><span class="line">$ vi /opt/stack/requirements/global-requirements.txt  </span><br><span class="line">$ vi /opt/stack/requirements/upper-constraints.txt</span><br></pre></td></tr></table></figure><p>上面两个文件将libvirt-python修改为3.2.0<br>然后执行</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./unstack.sh  </span><br><span class="line">$ ./stack.sh</span><br></pre></td></tr></table></figure><h2 id="安装成功界面"><a href="#安装成功界面" class="headerlink" title="安装成功界面"></a>安装成功界面</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">This is your host IP address: 192.168.0.120</span><br><span class="line">This is your host IPv6 address: ::1</span><br><span class="line">Horizon is now available at http://192.168.0.120/dashboard</span><br><span class="line">Keystone is serving at http://192.168.0.120/identity/</span><br><span class="line">The default users are: admin and demo</span><br><span class="line">The password: openstack</span><br><span class="line">DevStack Version: ocata</span><br><span class="line">Change: 66827dda40737f482b00580b52bc2865c605dcae Bump NOVA_READY_TIMEOUT to 120s 2018-02-23 16:58:30 -0500</span><br><span class="line">OS Version: CentOS 7.4.1708 Core</span><br></pre></td></tr></table></figure><p><img src="/images/openstack-devstack.png" alt="openstack-devstack"></p><h3 id="关闭防火墙"><a href="#关闭防火墙" class="headerlink" title="关闭防火墙"></a>关闭防火墙</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ systemctl stop firewalld.service  </span><br><span class="line">$ systemctl <span class="built_in">disable</span> firewalld.service  </span><br><span class="line">$ cat /etc/sysconfig/iptables  </span><br><span class="line"><span class="comment"># sample configuration for iptables service  </span></span><br><span class="line"><span class="comment"># you can edit this manually or use system-config-firewall  </span></span><br><span class="line"><span class="comment"># please do not ask us to add additional ports/services to this default configuration  </span></span><br><span class="line">*filter  </span><br><span class="line">:INPUT ACCEPT [0:0]  </span><br><span class="line">:FORWARD ACCEPT [0:0]  </span><br><span class="line">:OUTPUT ACCEPT [0:0]  </span><br><span class="line"><span class="comment">#-A INPUT -m state --state RELATED,ESTABLISHED -j ACCEPT  </span></span><br><span class="line"><span class="comment">#-A INPUT -p icmp -j ACCEPT  </span></span><br><span class="line"><span class="comment">#-A INPUT -i lo -j ACCEPT  </span></span><br><span class="line"><span class="comment">#-A INPUT -p tcp -m state --state NEW -m tcp --dport 22 -j ACCEPT  </span></span><br><span class="line"><span class="comment">#-A INPUT -j REJECT --reject-with icmp-host-prohibited  </span></span><br><span class="line"><span class="comment">#-A FORWARD -j REJECT --reject-with icmp-host-prohibited  </span></span><br><span class="line">COMMIT  </span><br><span class="line">$ systemctl restart iptables.service  </span><br><span class="line">$ systemctl <span class="built_in">enable</span> iptables.service</span><br></pre></td></tr></table></figure><p><a href="http://192.168.0.120/dashboard登录，用户名为admin，密码为openstack" target="_blank" rel="noopener">http://192.168.0.120/dashboard登录，用户名为admin，密码为openstack</a></p><p><img src="/images/openstack-devstack-1.png" alt="openstack-devstack-1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是通过devstack安装部署OpenStack&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;系统：CentOS 7 最小安装版&lt;
      
    
    </summary>
    
      <category term="通过devstack安装部署OpenStack" scheme="http://yoursite.com/categories/%E9%80%9A%E8%BF%87devstack%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2OpenStack/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="DevStack" scheme="http://yoursite.com/tags/DevStack/"/>
    
  </entry>
  
  <entry>
    <title>OpenStack安装部署</title>
    <link href="http://yoursite.com/2018/03/09/openstack-install-introduction/"/>
    <id>http://yoursite.com/2018/03/09/openstack-install-introduction/</id>
    <published>2018-03-08T16:00:00.000Z</published>
    <updated>2018-03-16T08:08:30.753Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是openstack安装部署私有云详细图文教程</p><p>本文主要分享的是云计算、openstack的使用、私有云平台建设、云服务器云硬盘的构建和使用。从基本概念入手到私有云建设，信息量非常大。对于openstack的安装部署都是从官方文档中一步步的介绍，内容非常详细。</p><h2 id="云计算基本概念"><a href="#云计算基本概念" class="headerlink" title="云计算基本概念"></a>云计算基本概念</h2><p>云计算（cloud computing）是基于互联网的相关服务的增加、使用和交付模式，通常涉及通过互联网来提供动态易扩展且经常是虚拟化的资源。云是网络、互联网的一种比喻说法。过去在图中往往用云来表示电信网，后来也用来表示互联网和底层基础设施的抽象。因此，云计算甚至可以让你体验每秒10万亿次的运算能力，拥有这么强大的计算能力可以模拟核爆炸、预测气候变化和市场发展趋势。用户通过电脑、笔记本、手机等方式接入数据中心，按自己的需求进行运算。</p><h3 id="云计算分类"><a href="#云计算分类" class="headerlink" title="云计算分类"></a>云计算分类</h3><p>简单来说，就是把应用程序和数据都放在由大量服务器组成的云中，用户需要什么只要购买相应服务并使用即可。</p><p>云计算分为私有云、公有云、混合云。</p><p>云计算的服务分类：基础设施即服务、平台即服务、软件即服务。</p><ul><li><p>超大规模。“云”具有相当的规模，Google云计算已经拥有100多万台服务器，亚马逊、IBM、微软和Yahoo等公司的“云”均拥有几十万台服务器。“云”能赋予用户前所未有的计算能力。</p></li><li><p>虚拟化。云计算支持用户在任意位置使用各种终端获取服务。所请求的资源来自“云”,而不是固定的有形的实体。应用在“云”中某处运行，但实际上用户无需了解应用运行的具体位置，只需要一台笔记本或一个PDA,就可以通过网络服务来获取各种能力超强的服务。</p></li><li><p>高可靠性。“云”使用了数据多副本容错、计算节点同构可互换等措施来保障服务的高可靠性，使用云计算比使用本地计算机更加可靠。</p></li><li><p>通用性。云计算不针对特定的应用，在“云”的支撑下可以构造出于变万化的应用，同一片“云”可以同时支撑不同的应用运行。</p></li><li><p>高可伸缩性。“云”的规模可以动态伸缩，满足应用和用户规模增长的需要。</p></li><li><p>按需服务。“云”是一个庞大的资源池，用户按需购买，像自来水、电和煤气那样计费。</p></li><li><p>极其廉价。“云”的特殊容错措施使得可以采用极其廉价的节点来构成云;“云”的自动化管理使数据中心管理成本大幅降低;“云”的公用性和通用性使资源的利用率大幅提升;“云”设施可以建在电力资源丰富的地区，从而大幅降低能源成本。</p></li></ul><h3 id="OpenStack基本概念"><a href="#OpenStack基本概念" class="headerlink" title="OpenStack基本概念"></a>OpenStack基本概念</h3><p>OpenStack是一个开源的云计算管理平台项目，由几个主要的组件组合起来完成具体工作。OpenStack支持几乎所有类型的云环境，项目目标是提供实施简单、可大规模扩展、丰富、标准统一的云计算管理平台。OpenStack通过各种互补的服务提供了基础设施即服务（IaaS）的解决方案，每个服务提供API以进行集成。</p><p>常见组件：</p><ul><li>Dashboard(web项目)</li><li>Keystone(权限管理)</li><li>Nova(管理虚拟机)</li><li>Nova-network(管理网络流量和ip)</li><li>Glance(镜像管理项目)</li><li>Cinder(磁盘管理)</li><li>Swift(网盘，对象)</li></ul><p>基础的系统图<br><img src="/images/openstack-1.png" alt="openstack-1"></p><h2 id="Openstack安装部署"><a href="#Openstack安装部署" class="headerlink" title="Openstack安装部署"></a>Openstack安装部署</h2><p>下面来说一说openstack的安装部署。首先说明openstack的整体安装非常复杂，文末提供官方安装文档下载，我就是根据这个英文的官方文档来安装部署的。以下内容是参照官方文档“openstack-install-guide-yum-icehouse.pdf”来的。</p><p>首先需要准备3台linux的机器。我这里选用的是centos6.6的64位的系统。配置IP地址，关闭防火墙，修改主机名。</p><ul><li><p>Controller Node: 1 processor, 2 GB memory, and 5 GB storage</p></li><li><p>Compute01: 1 processor, 512 MB memory, and 5 GB storage</p></li><li><p>Compute02 Node: 1 processor, 2 GB memory, and 10 GB storage</p></li></ul><p>我这里controller的ip地址是:192.168.44.147</p><p>computer01：192.168.44.148</p><p>computer02：192.168.44.149</p><p>基本环境配置</p><p>以下内容是在controller机器上配置的。</p><h3 id="数据库安装"><a href="#数据库安装" class="headerlink" title="数据库安装"></a>数据库安装</h3><p>因为需要一个数据库，所以这里选用了mysql数据库。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install mysql mysql-server MySQL-python</span><br></pre></td></tr></table></figure><p>编辑<code>/etc/my.cnf</code>文件，在里面添加如下内容，主要意思就是设置编码为utf-8。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">default-storage-engine = innodb</span><br><span class="line">innodb_file_per_table</span><br><span class="line">collation-server = utf8_general_ci</span><br><span class="line">init-connect = <span class="string">'SET NAMES utf8'</span></span><br><span class="line">character-set-server = utf8</span><br></pre></td></tr></table></figure></p><p>然后启动服务，开机启动。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ service mysqld start</span><br><span class="line">$ chkconfig mysqld on</span><br><span class="line">$ mysql_install_db</span><br><span class="line">$ mysql_secure_installation</span><br></pre></td></tr></table></figure></p><p>赋权，使其可以远程登录。<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">mysql &gt; GRANT ALL PRIVILEGES ON *.* TO <span class="string">'root'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'password'</span>;</span><br></pre></td></tr></table></figure></p><h2 id="OpenStack基本包安装"><a href="#OpenStack基本包安装" class="headerlink" title="OpenStack基本包安装"></a>OpenStack基本包安装</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ yum install yum-plugin-priorities</span><br><span class="line">$ yum install http://repos.fedorapeople.org/repos/openstack/openstackicehouse/rdo-release-icehouse-3.noarch.rpm</span><br><span class="line">$ yum install http://dl.fedoraproject.org/pub/epel/6/x86_64/epel-release-6-8.noarch.rpm</span><br><span class="line">$ yum install openstack-utils</span><br><span class="line">$ yum install openstack-selinux</span><br></pre></td></tr></table></figure><h3 id="安装消息队列"><a href="#安装消息队列" class="headerlink" title="安装消息队列"></a>安装消息队列</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install qpid-cpp-server</span><br></pre></td></tr></table></figure><p>启动服务<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ service qpidd start</span><br><span class="line">$ chkconfig qpidd on</span><br></pre></td></tr></table></figure></p><h3 id="权限认证服务-keystone"><a href="#权限认证服务-keystone" class="headerlink" title="权限认证服务(keystone)"></a>权限认证服务(keystone)</h3><p>安装权限认证服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install openstack-keystone python-keystoneclient -y</span><br></pre></td></tr></table></figure><p>创建用户，写入到配置文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/keystone/keystone.conf database connection mysql://keystone:KEYSTONE_DBPASS@controller/keystone</span><br></pre></td></tr></table></figure><p>创建keystone数据库表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p</span><br><span class="line">mysql&gt; CREATE DATABASE keystone;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'localhost'</span> \</span><br><span class="line">IDENTIFIED BY <span class="string">'KEYSTONE_DBPASS'</span>;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON keystone.* TO <span class="string">'keystone'</span>@<span class="string">'%'</span> \</span><br><span class="line">IDENTIFIED BY <span class="string">'KEYSTONE_DBPASS'</span>;</span><br><span class="line">mysql&gt; <span class="built_in">exit</span></span><br></pre></td></tr></table></figure><p>自动生成表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ su -s /bin/sh -c <span class="string">"keystone-manage db_sync"</span> keystone</span><br></pre></td></tr></table></figure><p>设置用户环境变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ ADMIN_TOKEN=$(openssl rand -hex 10)</span><br><span class="line">$ <span class="built_in">echo</span> <span class="variable">$ADMIN_TOKEN</span></span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/keystone/keystone.conf DEFAULT \</span><br><span class="line">$ admin_token <span class="variable">$ADMIN_TOKEN</span></span><br><span class="line"></span><br><span class="line">$ keystone-manage pki_setup --keystone-user keystone --keystone-group keystone</span><br><span class="line">$ chown -R keystone:keystone /etc/keystone/ssl</span><br><span class="line">$ chmod -R o-rwx /etc/keystone/ssl</span><br></pre></td></tr></table></figure><p>启动keystone服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ service openstack-keystone start</span><br><span class="line">$ chkconfig openstack-keystone on</span><br></pre></td></tr></table></figure><p>将admin_token设置到环境变量中去。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> OS_SERVICE_TOKEN=<span class="variable">$ADMIN_TOKEN</span></span><br><span class="line">$ <span class="built_in">export</span> OS_SERVICE_ENDPOINT=http://controller:35357/v2.0</span><br></pre></td></tr></table></figure><p>创建管理员用户，默认的用户名为admin,密码为ADMIN_PASS,你可以自定义修改：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">[root@controller keystone]<span class="comment"># keystone user-create --name=admin --pass=ADMIN_PASS --email=ADMIN_EMAIL</span></span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">| email | ADMIN_EMAIL|</span><br><span class="line">| enabled | True |</span><br><span class="line">|id| 332f5ecb18c64bf687b154facd629fb6 |</span><br><span class="line">| name | admin |</span><br><span class="line">| username | admin |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone role-create --name=admin</span></span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">|id| ab5da2c50d7443efb0514e84c2b4adb7 |</span><br><span class="line">| name | admin |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone tenant-create --name=admin --description="Admin Tenant"</span></span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| description | Admin Tenant |</span><br><span class="line">| enabled | True |</span><br><span class="line">| id | b89723a38b154eeea6416a83b293b0ad |</span><br><span class="line">| name| admin |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone user-role-add --user=admin --tenant=admin --role=admin</span></span><br><span class="line">[root@controller keystone]<span class="comment"># keystone user-role-add --user=admin --role=_member_ --tenant=admin</span></span><br><span class="line">[root@controller keystone]<span class="comment">#</span></span><br></pre></td></tr></table></figure></p><p>创建一个权限认证服务，因为我这里的主机名是controller，所以下面有 <a href="http://controller:35357/v2.0" target="_blank" rel="noopener">http://controller:35357/v2.0</a> 这些信息，可以自定义修改：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">[root@controller keystone]<span class="comment"># keystone service-create --name=keystone --type=identity \</span></span><br><span class="line">&gt; --description=<span class="string">"OpenStack Identity"</span></span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| description |OpenStack Identity|</span><br><span class="line">| enabled | True |</span><br><span class="line">| id | 4bfd5f39b86c45edb92d5c0488987dd8 |</span><br><span class="line">| name| keystone |</span><br><span class="line">| <span class="built_in">type</span>| identity |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone endpoint-create \</span></span><br><span class="line">&gt; --service-id=$(keystone service-list | awk <span class="string">'/ identity / &#123;print $2&#125;'</span>) \</span><br><span class="line">&gt; --publicurl=http://controller:5000/v2.0 \</span><br><span class="line">&gt; --internalurl=http://controller:5000/v2.0 \</span><br><span class="line">&gt; --adminurl=http://controller:35357/v2.0</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| adminurl | http://controller:35357/v2.0 |</span><br><span class="line">| id | 0350b72901a141498fe48304b13a1b98 |</span><br><span class="line">| internalurl | http://controller:5000/v2.0|</span><br><span class="line">| publicurl | http://controller:5000/v2.0|</span><br><span class="line">|region |regionOne |</span><br><span class="line">| service_id | 4bfd5f39b86c45edb92d5c0488987dd8 |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone user-create --name=demo --pass=DEMO_PASS --email=DEMO_EMAIL</span></span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">| email |DEMO_EMAIL|</span><br><span class="line">| enabled | True |</span><br><span class="line">|id| b44a9a1462d74f4e928b3b339289a2ae |</span><br><span class="line">| name | demo |</span><br><span class="line">| username | demo |</span><br><span class="line">+----------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone tenant-create --name=demo --description="Demo Tenant"</span></span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| description | Demo Tenant|</span><br><span class="line">| enabled | True |</span><br><span class="line">| id | 7bd79509ee184a2098ca0644679b60a8 |</span><br><span class="line">| name| demo |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">[root@controller keystone]<span class="comment"># keystone user-role-add --user=demo --role=_member_ --tenant=demo</span></span><br><span class="line">[root@controller keystone]<span class="comment">#</span></span><br></pre></td></tr></table></figure></p><p>为了方便我们取到token，我们可以vi admin-openrc.sh,然后添加如下内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">export</span> OS_USERNAME=admin</span><br><span class="line">$ <span class="built_in">export</span> OS_PASSWORD=ADMIN_PASS</span><br><span class="line">$ <span class="built_in">export</span> OS_TENANT_NAME=admin</span><br><span class="line">$ <span class="built_in">export</span> OS_AUTH_URL=http://controller:35357/v2.0</span><br></pre></td></tr></table></figure><p>在每次关机重启之后都要重新执行下面的命令，让环境变量起作用。否则会报”Expecting an auth URL via either –os-auth-url or env[OS_AUTH_URL]”的错误：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">source</span> admin-openrc.sh</span><br></pre></td></tr></table></figure><p>我们可以使用下面的命令查看keystone目前的用户有哪些：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ keystone user-list</span><br></pre></td></tr></table></figure><h3 id="配置镜像服务-glance"><a href="#配置镜像服务-glance" class="headerlink" title="配置镜像服务(glance)"></a>配置镜像服务(glance)</h3><p>在controller服务器中安装服务:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ yum install openstack-glance python-glanceclient</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf database connection mysql://glance:GLANCE_DBPASS@controller/glance</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf database connection mysql://glance:GLANCE_DBPASS@controller/glance</span><br></pre></td></tr></table></figure><p>在mysql数据库中创建glance数据库：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -u root -p</span><br><span class="line">mysql&gt; CREATE DATABASE glance;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'GLANCE_DBPASS'</span>;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON glance.* TO <span class="string">'glance'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'GLANCE_DBPASS'</span>;</span><br><span class="line">mysql&gt; flush privileges;</span><br></pre></td></tr></table></figure><p>自动生成表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ su -s /bin/sh -c <span class="string">"glance-manage db_sync"</span> glance</span><br></pre></td></tr></table></figure><p>在keystone上创建用户：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ keystone user-create --name=glance --pass=GLANCE_PASS --email=glance@example.com</span><br><span class="line">$ keystone user-role-add --user=glance --tenant=service --role=admin</span><br></pre></td></tr></table></figure><p>配置授权服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken admin_user glance</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf keystone_authtoken admin_password GLANCE_PASS</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-api.conf paste_deploy flavor keystone</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken admin_user glance</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf keystone_authtoken admin_password GLANCE_PASS</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/glance/glance-registry.conf paste_deploy flavor keystone</span><br></pre></td></tr></table></figure></p><p>启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ service openstack-glance-api start</span><br><span class="line">$ service openstack-glance-registry start</span><br><span class="line">$ chkconfig openstack-glance-api on</span><br><span class="line">$ chkconfig openstack-glance-registry on</span><br></pre></td></tr></table></figure><p>创建服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">$ keystone service-create --name=glance --<span class="built_in">type</span>=image --description=<span class="string">"OpenStack Image Service"</span></span><br><span class="line">$ keystone endpoint-create --service-id=$(keystone service-list | awk <span class="string">'/ image / &#123;print $2&#125;'</span>) --publicurl=http://controller:9292 --internalurl=http://controller:9292 --adminurl=http://controller:9292</span><br><span class="line"></span><br><span class="line">$ keystone service-create --name=glance --<span class="built_in">type</span>=image --description=<span class="string">"OpenStack Image Service"</span></span><br><span class="line">/usr/lib64/python2.6/site-packages/Crypto/Util/number.py:57: PowmInsecureWarning: Not using mpz_powm_sec. You should rebuild using libgmp &gt;= 5 to avoid timing attack vulnerability.</span><br><span class="line"> _warn(<span class="string">"Not using mpz_powm_sec. You should rebuild using libgmp &gt;= 5 to avoid timing attack vulnerability."</span>, PowmInsecureWarning)</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| description | OpenStack Image Service |</span><br><span class="line">| enabled | True |</span><br><span class="line">| id | a45f77cfc12c42d19a45a6ea12cdfc51 |</span><br><span class="line">| name| glance |</span><br><span class="line">| <span class="built_in">type</span>| image |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">[root@controller ~]<span class="comment"># keystone endpoint-create \</span></span><br><span class="line">&gt; --service-id=$(keystone service-list | awk <span class="string">'/ image / &#123;print $2&#125;'</span>) \</span><br><span class="line">&gt; --publicurl=http://controller:9292 \</span><br><span class="line">&gt; --internalurl=http://controller:9292 \</span><br><span class="line">&gt; --adminurl=http://controller:9292</span><br><span class="line"></span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| Property | Value |</span><br><span class="line">+-------------+----------------------------------+</span><br><span class="line">| adminurl | http://controller:9292 |</span><br><span class="line">| id | 75deb2d2f85e4b7b88fe18bf5fca1a87 |</span><br><span class="line">| internalurl | http://controller:9292 |</span><br><span class="line">| publicurl | http://controller:9292 |</span><br><span class="line">|region |regionOne |</span><br><span class="line">| service_id | a45f77cfc12c42d19a45a6ea12cdfc51 |</span><br><span class="line">+-------------+----------------------------------+</span><br></pre></td></tr></table></figure><p>创建镜像，先将下载好的cirros-0.3.2-x86_64-disk.img放置在/root目录下，然后执行下面的命令：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ glance image-create --name <span class="string">"cirros-0.3.2-x86_64"</span> --disk-format qcow2 --container-format bare --is-public True --progress &lt; cirros-0.3.2-x86_64-disk.img</span><br></pre></td></tr></table></figure><p>查看镜像列表：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ glance image-list</span><br></pre></td></tr></table></figure><p><img src="/images/openstack-image-list.jpg" alt="openstack-image-list"></p><p>在前面我们已经对kvm虚拟化有所了解，所以我们现在知道qcow2就是一个镜像文件。</p><h3 id="服务器管理（Nova"><a href="#服务器管理（Nova" class="headerlink" title="服务器管理（Nova)"></a>服务器管理（Nova)</h3><p>对于虚拟机管理我们需要从controller和computer01进行配置。</p><p>先来看controller的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">$ yum install openstack-nova-api openstack-nova-cert openstack-nova-conductor openstack-nova-console openstack-nova-novncproxy openstack-nova-scheduler python-novaclient</span><br><span class="line">$ mysql -u root -p</span><br><span class="line">mysql&gt; CREATE DATABASE nova;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'localhost'</span> \</span><br><span class="line">IDENTIFIED BY <span class="string">'NOVA_DBPASS'</span>;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON nova.* TO <span class="string">'nova'</span>@<span class="string">'%'</span> \</span><br><span class="line">IDENTIFIED BY <span class="string">'NOVA_DBPASS'</span>;</span><br><span class="line"></span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf database connection mysql://nova:NOVA_DBPASS@controller/nova</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT rpc_backend qpid</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT qpid_hostname controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT my_ip 192.168.44.147</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT vncserver_listen 192.168.216.210</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT vncserver_proxyclient_address 192.168.44.147</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT auth_strategy keystone</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_user nova</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_password NOVA_PASS</span><br><span class="line">$ keystone user-create --name=nova --pass=NOVA_PASS --email=nova@example.com</span><br><span class="line">$ keystone user-role-add --user=nova --tenant=service --role=admin</span><br><span class="line">$ keystone service-create --name=nova --<span class="built_in">type</span>=compute --description=<span class="string">"OpenStack Compute"</span></span><br><span class="line">$ keystone endpoint-create --service-id=$(keystone service-list | awk <span class="string">'/ compute / &#123;print $2&#125;'</span>) --publicurl=http://controller:8774/v2/%\(tenant_id\)s --internalurl=http://controller:8774/v2/%\(tenant_id\)s --adminurl=http://controller:8774/v2/%\(tenant_id\)s</span><br></pre></td></tr></table></figure><p>然后是computer01的配置：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">$ yum install openstack-nova-compute</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf database connection mysql://nova:NOVA_DBPASS@controller/nova</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT auth_strategy keystone</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_user nova</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf keystone_authtoken admin_password NOVA_PASS</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT rpc_backend qpid</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT qpid_hostname controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT my_ip 192.168.44.148</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT vnc_enabled True</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT vncserver_listen 0.0.0.0</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT vncserver_proxyclient_address 192.168.44.148</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT novncproxy_base_url http://controller:6080/vnc_auto.html</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT glance_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf libvirt virt_type kvm</span><br></pre></td></tr></table></figure><p>启动服务：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ service libvirtd start</span><br><span class="line">$ service messagebus start</span><br><span class="line">$ service openstack-nova-compute start</span><br><span class="line">$ chkconfig libvirtd on</span><br><span class="line">$ chkconfig messagebus on</span><br><span class="line">$ chkconfig openstack-nova-compute on</span><br></pre></td></tr></table></figure><h3 id="网络服务配置"><a href="#网络服务配置" class="headerlink" title="网络服务配置"></a>网络服务配置</h3><p>在controller端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT network_api_class nova.network.api.API</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT security_group_api nova</span><br></pre></td></tr></table></figure><p>在computer01端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">$ yum install openstack-nova-network openstack-nova-api</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT network_api_class nova.network.api.API</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT security_group_api nova</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT network_manager nova.network.manager.FlatDHCPManager</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT firewall_driver nova.virt.libvirt.firewall.IptablesFirewallDriver</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT network_size 254</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT allow_same_net_traffic False</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT multi_host True</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT send_arp_for_ha True</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT share_dhcp_address True</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT force_dhcp_release True</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT flat_network_bridge br100</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT flat_interface eth1</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/nova/nova.conf DEFAULT public_interface eth0</span><br><span class="line">$ nova network-create demo-net --bridge br100 --multi-host T --fixed-range-v4 88.8.8.16/28</span><br></pre></td></tr></table></figure><p>然后使用nova net-list来查看：<br><img src="/images/openstack-nova-net-list.png" alt="openstack-nova-net-list"></p><h2 id="创建虚拟机"><a href="#创建虚拟机" class="headerlink" title="创建虚拟机"></a>创建虚拟机</h2><h3 id="配置ssh密码登录："><a href="#配置ssh密码登录：" class="headerlink" title="配置ssh密码登录："></a>配置ssh密码登录：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ ssh-keygen</span><br></pre></td></tr></table></figure><h3 id="增加公钥到openstack环境中："><a href="#增加公钥到openstack环境中：" class="headerlink" title="增加公钥到openstack环境中："></a>增加公钥到openstack环境中：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nova keypair-add --pub-key ~/.ssh/id_rsa.pub demo-key</span><br></pre></td></tr></table></figure><h3 id="验证是否配置成功："><a href="#验证是否配置成功：" class="headerlink" title="验证是否配置成功："></a>验证是否配置成功：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ nova keypair-list</span><br><span class="line">$ nova flavor-list</span><br></pre></td></tr></table></figure><h3 id="创建实例："><a href="#创建实例：" class="headerlink" title="创建实例："></a>创建实例：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64 --nic netid=DEMO_NET_ID --security-group default --key-name demo-key demo-instance1</span><br></pre></td></tr></table></figure><p>说明：</p><p>DEMO_NET_ID：指的是nova net-list的ID。</p><p>demo-instance1指的是虚拟机的名字。</p><p>例如我的成功执行如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nova boot --flavor m1.tiny --image cirros-0.3.2-x86_64 --nic net-id=55fc305f-570f-4d4f-89d0-ce303e589f20 --security-group default --key-name demo-key tfjt</span><br></pre></td></tr></table></figure><h3 id="使用nova-list进行查看："><a href="#使用nova-list进行查看：" class="headerlink" title="使用nova list进行查看："></a>使用nova list进行查看：</h3><p><img src="/images/openstack-nova-list.png" alt="openstack-nova-list"><br>从这里我们看到我这上面有一个ip地址192.168.44.17.这个地址就是浮动IP。</p><h3 id="配置浮动IP"><a href="#配置浮动IP" class="headerlink" title="配置浮动IP"></a>配置浮动IP</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 浮动ip</span></span><br><span class="line">$ nova-manage floating create --ip_range=192.168.44.16/28</span><br><span class="line"></span><br><span class="line"><span class="comment">## 查看可用地址</span></span><br><span class="line">$ nova-manage floating list</span><br><span class="line">$ nova floating-ip-create</span><br><span class="line"></span><br><span class="line"><span class="comment">## 给创建的虚拟机绑定浮动ip，这个7bc0086…就是我之前创建的虚拟机的ID。后面接上IP地址即可。</span></span><br><span class="line">$ nova add-floating-ip 7bc00086-1870-4367-9f05-666d5067ccff 192.168.44.17</span><br><span class="line"></span><br><span class="line"><span class="comment">## 监听</span></span><br><span class="line">$ cpdump -n -i eth0 icmp</span><br></pre></td></tr></table></figure><p>在controller上：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ nova secgroup-add-rule default icmp -1 -1 0.0.0.0/0</span><br><span class="line">$ nova secgroup-add-rule default tcp 22 22 0.0.0.0/0</span><br></pre></td></tr></table></figure><h3 id="使用下面的命令可以输出一个url地址："><a href="#使用下面的命令可以输出一个url地址：" class="headerlink" title="使用下面的命令可以输出一个url地址："></a>使用下面的命令可以输出一个url地址：</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ nova get-vnc-console tfjt novnc</span><br></pre></td></tr></table></figure><h3 id="我们可以在浏览器中进行访问。"><a href="#我们可以在浏览器中进行访问。" class="headerlink" title="我们可以在浏览器中进行访问。"></a>我们可以在浏览器中进行访问。</h3><p><img src="/images/openstack-vnc.png" alt="openstack-vnc"><br>到这里为止，我们的云服务器就算完成了。可以在浏览器上访问我们的云服务器，怎么样是不是很激动。</p><h2 id="界面服务（dashboard）"><a href="#界面服务（dashboard）" class="headerlink" title="界面服务（dashboard）"></a>界面服务（dashboard）</h2><p>上面我们可以看到很多操作是在终端进行的，那么肯定是有界面版的，所以我们在computer02中进行配置。</p><h3 id="安装服务"><a href="#安装服务" class="headerlink" title="安装服务"></a>安装服务</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install memcached python-memcached mod_wsgi openstack-dashboard</span><br></pre></td></tr></table></figure><h3 id="文件配置"><a href="#文件配置" class="headerlink" title="文件配置"></a>文件配置</h3><p>在controller中</p><p>配置/etc/openstack-dashboard/local_settings。修改为如下内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">ALLOWED_HOSTS = [<span class="string">'*'</span>]  </span><br><span class="line">  </span><br><span class="line">  </span><br><span class="line">CACHES = &#123;  </span><br><span class="line"><span class="string">'default'</span>: &#123;  </span><br><span class="line"><span class="string">'BACKEND'</span> : <span class="string">'django.core.cache.backends.memcached.MemcachedCache'</span>,  </span><br><span class="line"><span class="string">'LOCATION'</span> : <span class="string">'127.0.0.1:11211'</span>  </span><br><span class="line">&#125;  </span><br><span class="line">&#125;  </span><br><span class="line">  </span><br><span class="line">OPENSTACK_HOST = <span class="string">"controller"</span></span><br></pre></td></tr></table></figure><h3 id="连接"><a href="#连接" class="headerlink" title="连接"></a>连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ setsebool -P httpd_can_network_connect on</span><br><span class="line">$ service httpd start</span><br><span class="line">$ service memcached start</span><br><span class="line">$ chkconfig httpd on</span><br><span class="line">$ chkconfig memcached on</span><br></pre></td></tr></table></figure><h3 id="浏览器中访问"><a href="#浏览器中访问" class="headerlink" title="浏览器中访问"></a>浏览器中访问</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ http://192.168.44.147/dashboard/project/</span><br></pre></td></tr></table></figure><p><img src="/images/openstack-dashboard-1.png" alt="openstack-dashboard-1"></p><p>用户名就是我们之前配置的admin,密码是ADMIN_PASS</p><p>在这个界面版里面的各种操作我这里就不细说了，可以自己去体验。<br><img src="/images/openstack-dashboard-2.png" alt="openstack-dashboard-2"></p><p>在这里可以看到我们的云服务器实例。<br><img src="/images/openstack-dashboard-3.png" alt="openstack-dashboard-3"></p><h2 id="云硬盘服务（cinder"><a href="#云硬盘服务（cinder" class="headerlink" title="云硬盘服务（cinder)"></a>云硬盘服务（cinder)</h2><p>了解过阿里云或者腾讯云的就知道，里面都有一个云硬盘服务，可以给我们的机器增加磁盘。</p><p>在controller端：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">$ yum stall openstack-cinde</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf database connection mysql://cinder:CINDER_DBPASS@controller/cinder</span><br><span class="line">mysql&gt; CREATE DATABASE cinder;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'localhost'</span> IDENTIFIED BY <span class="string">'CINDER_DBPASS'</span>;</span><br><span class="line">mysql&gt; GRANT ALL PRIVILEGES ON cinder.* TO <span class="string">'cinder'</span>@<span class="string">'%'</span> IDENTIFIED BY <span class="string">'CINDER_DBPASS'</span>;</span><br></pre></td></tr></table></figure><p>创建数据库表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ su -s /bin/sh -c <span class="string">"cinder-manage db sync"</span> cinder</span><br></pre></td></tr></table></figure></p><p>创建用户并赋予角色：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ keystone user-create --name=cinder --pass=CINDER_PASS--email=cinder@example.com</span><br><span class="line">$ keystone user-role-add --user=cinder --tenant=service --role=admin</span><br></pre></td></tr></table></figure></p><p>权限控制配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT auth_strategy keystone</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_user cinder</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_password CINDER_PASS</span><br></pre></td></tr></table></figure></p><p>消息队列配置：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT rpc_backend qpid</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT qpid_hostname controller</span><br><span class="line"></span><br><span class="line">$ keystone service-create --name=cinder --<span class="built_in">type</span>=volume --description=<span class="string">"OpenStack Block Storage"</span></span><br><span class="line">$ keystone endpoint-create --service-id=$(keystone service-list | awk <span class="string">'/ volume / &#123;print $2&#125;'</span>) --publicurl=http://controller:8776/v1/%\(tenant_id\)s --internalurl=http://controller:8776/v1/%\(tenant_id\)s --adminurl=http://controller:8776/v1/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">$ keystone service-create --name=cinderv2 --<span class="built_in">type</span>=volumev2 --description=<span class="string">"OpenStack Block Storage v2"</span></span><br><span class="line">$ keystone endpoint-create --service-id=$(keystone service-list | awk <span class="string">'/ volumev2 / &#123;print $2&#125;'</span>) --publicurl=http://controller:8776/v2/%\(tenant_id\)s --internalurl=http://controller:8776/v2/%\(tenant_id\)s --adminurl=http://controller:8776/v2/%\(tenant_id\)s</span><br><span class="line"></span><br><span class="line">$ service openstack-cinder-api start</span><br><span class="line">$ service openstack-cinder-scheduler start</span><br><span class="line">$ chkconfig openstack-cinder-api on</span><br><span class="line">$ chkconfig openstack-cinder-scheduler on</span><br><span class="line"></span><br><span class="line">$ cinder service-list</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT qpid_hostname controller</span><br><span class="line"></span><br><span class="line">$ cinder --debug list</span><br><span class="line">$ keystone user-create --name=cinder --pass=CINDER_PASS --email=cinder@example.com</span><br><span class="line"></span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT my_ip 192.168.44.147</span><br></pre></td></tr></table></figure></p><p>在node端（computer02)</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">$ pvcreate /dev/sdb</span><br><span class="line">$ vgcreate cinder-volumes /dev/sdb</span><br><span class="line">$ yum install openstack-cinder scsi-target-utils</span><br><span class="line"></span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT auth_strategy keystone</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_uri http://controller:5000</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_protocol http</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken auth_port 35357</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_user cinder</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_tenant_name service</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf keystone_authtoken admin_password CINDER_PASS</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT rpc_backend qpid</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT qpid_hostname controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf database connection mysql://cinder:CINDER_DBPASS@controller/cinde</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT my_ip MANAGEMENT_INTERFACE_IP_ADDRESS</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT glance_host controller</span><br><span class="line">$ openstack-config --<span class="built_in">set</span> /etc/cinder/cinder.conf DEFAULT iscsi_helper tgtadm</span><br></pre></td></tr></table></figure><p>配置文件中：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ vi /etc/tgt/targets.conf</span><br><span class="line">include /etc/cinder/volumes/*</span><br></pre></td></tr></table></figure><p>启动服务：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ service openstack-cinder-volume start</span><br><span class="line">$ service tgtd start</span><br><span class="line">$ chkconfig openstack-cinder-volume on</span><br><span class="line">$ chkconfig tgtd on</span><br></pre></td></tr></table></figure></p><p>创建磁盘：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cinder create --display-name myVolume 1</span><br></pre></td></tr></table></figure></p><p>查看磁盘列表：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ cinder list</span><br></pre></td></tr></table></figure></p><p>然后再界面端查看云硬盘的配置并进行挂载：<br><img src="/images/openstack-dashboard-4.png" alt="openstack-dashboard-4"></p><p>在云服务器上挂载磁盘：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line">$ mkfs.ext3 /dev/vdb1</span><br><span class="line"></span><br><span class="line">$ mkdir /mnt/<span class="built_in">test</span></span><br><span class="line">$ mount /dev/vdb1 /mnt/<span class="built_in">test</span></span><br><span class="line">$ mount</span><br><span class="line"></span><br><span class="line">$ fdisk /dev/vdb</span><br><span class="line">Device contains neither a valid DOS partition table, nor Sun, SGI or OSF disklabel</span><br><span class="line">Building a new DOS disklabel with disk identifier 0x64817ea3.</span><br><span class="line">Changes will remain <span class="keyword">in</span> memory only, until you decide to write them.</span><br><span class="line">After that, of course, the previous content won‘t be recoverable.</span><br><span class="line"></span><br><span class="line">Warning: invalid flag 0x0000 of partition table 4 will be corrected by w(rite)</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): n</span><br><span class="line">Partition <span class="built_in">type</span>:</span><br><span class="line"> p primary (0 primary, 0 extended, 4 free)</span><br><span class="line"> e extended</span><br><span class="line">Select (default p): p</span><br><span class="line">Partition number (1-4, default 1): 1</span><br><span class="line">First sector (2048-2097151, default 2048): +1G</span><br><span class="line">Value out of range.</span><br><span class="line">First sector (2048-2097151, default 2048): +512M</span><br><span class="line">Last sector, +sectors or +size&#123;K,M,G&#125; (1048576-2097151, default 2097151): </span><br><span class="line">Using default value 2097151</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): p</span><br><span class="line"></span><br><span class="line">Disk /dev/vdb: 1073 MB, 1073741824 bytes</span><br><span class="line">16 heads, 63 sectors/track, 2080 cylinders, total 2097152 sectors</span><br><span class="line">Units = sectors of 1 * 512 = 512 bytes</span><br><span class="line">Sector size (logical/physical): 512 bytes / 512 bytes</span><br><span class="line">I/O size (minimum/optimal): 512 bytes / 512 bytes</span><br><span class="line">Disk identifier: 0x64817ea3</span><br><span class="line"></span><br><span class="line"> Device Boot Start End Blocks Id System</span><br><span class="line">/dev/vdb1 1048576 2097151 524288 83 Linux</span><br><span class="line"></span><br><span class="line">Command (m <span class="keyword">for</span> <span class="built_in">help</span>): w</span><br><span class="line">The partition table has been altered!</span><br><span class="line"></span><br><span class="line">Calling ioctl() to re-read partition table.</span><br><span class="line">Syncing disks.</span><br><span class="line">$ df -h</span><br></pre></td></tr></table></figure></p><p><img src="/images/openstack-dashboard-5.png" alt="openstack-dashboard-5"></p><p>总结：整个操作流程非常复杂，我也是花了几天才慢慢搭建好的，这其实也就是个基础，给的其实是一个思路，后面需要怎样的扩展可以根据自己的业务需要进行不同的设置。对于不明白的地方可以查阅官方文档。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是openstack安装部署私有云详细图文教程&lt;/p&gt;
&lt;p&gt;本文主要分享的是云计算、openstack的使用、私有云平台建设、云服务器云硬盘的构建和使用。从基本概念入手到私有云建设，信息量非常大。对于openstack的安装部署都是从官方文档中一步步的介绍，内
      
    
    </summary>
    
      <category term="OpenStack安装部署教程" scheme="http://yoursite.com/categories/OpenStack%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="OpenStack" scheme="http://yoursite.com/tags/OpenStack/"/>
    
      <category term="Installation" scheme="http://yoursite.com/tags/Installation/"/>
    
  </entry>
  
  <entry>
    <title>open-falcon安装部署教程</title>
    <link href="http://yoursite.com/2018/03/09/open-falcon-install/"/>
    <id>http://yoursite.com/2018/03/09/open-falcon-install/</id>
    <published>2018-03-08T16:00:00.000Z</published>
    <updated>2018-03-16T06:46:26.716Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是open-falcon安装部署教程</p><h2 id="环境准备"><a href="#环境准备" class="headerlink" title="环境准备"></a>环境准备</h2><ul><li>系统：CentOS 7 最小安装版</li></ul><h2 id="安装redis"><a href="#安装redis" class="headerlink" title="安装redis"></a>安装redis</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y redis</span><br></pre></td></tr></table></figure><h2 id="安装mysql"><a href="#安装mysql" class="headerlink" title="安装mysql"></a>安装mysql</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ yum install -y mysql-server</span><br></pre></td></tr></table></figure><p><strong>注意，请确保redis和MySQL已启动。</strong></p><h2 id="初始化MySQL表结构"><a href="#初始化MySQL表结构" class="headerlink" title="初始化MySQL表结构"></a>初始化MySQL表结构</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> /tmp/ &amp;&amp; git <span class="built_in">clone</span> https://github.com/open-falcon/falcon-plus.git </span><br><span class="line">$ <span class="built_in">cd</span> /tmp/falcon-plus/scripts/mysql/db_schema/</span><br><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 1_uic-db-schema.sql</span><br><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 2_portal-db-schema.sql</span><br><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 3_dashboard-db-schema.sql</span><br><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 4_graph-db-schema.sql</span><br><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 5_alarms-db-schema.sql</span><br><span class="line">$ rm -rf /tmp/falcon-plus/</span><br></pre></td></tr></table></figure><p><strong>如果你是从v0.1.0升级到当前版本v0.2.0，那么只需要执行如下命令：</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ mysql -h 127.0.0.1 -u root -p &lt; 5_alarms-db-schema.sql</span><br></pre></td></tr></table></figure><h2 id="安装go环境"><a href="#安装go环境" class="headerlink" title="安装go环境"></a>安装go环境</h2><p>go-v1.10下载地址：</p><ul><li><p><a href="https://dl.google.com/go/go1.10.linux-amd64.tar.gz" target="_blank" rel="noopener">linux64位下载地址</a></p></li><li><p><a href="https://dl.google.com/go/go1.10.linux-386.tar.gz" target="_blank" rel="noopener">linux32位下载地址</a></p></li><li><p><a href="https://dl.google.com/go/go1.10.windows-amd64.zip" target="_blank" rel="noopener">windows64位编译包下载地址</a></p></li><li><p><a href="https://dl.google.com/go/go1.10.windows-amd64.msi" target="_blank" rel="noopener">windows64位安装包下载地址</a></p></li><li><p><a href="https://dl.google.com/go/go1.10.windows-386.zip" target="_blank" rel="noopener">windows32位编译包下载地址</a></p></li><li><p><a href="https://dl.google.com/go/go1.10.windows-386.msi" target="_blank" rel="noopener">windows32位安装包下载地址</a></p></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 下载linux64位压缩包</span></span><br><span class="line">$ wget https://dl.google.com/go/go1.10.linux-amd64.tar.gz</span><br><span class="line">$ tar -C /usr/<span class="built_in">local</span> -xzf go1.10.linux-amd64.tar.gz</span><br><span class="line">$ <span class="built_in">echo</span> <span class="string">"export PATH=<span class="variable">$PATH</span>:/usr/local/go/bin"</span> &gt;&gt; /etc/profile</span><br><span class="line">$ <span class="built_in">source</span> /etc/profile</span><br><span class="line">$ go version</span><br><span class="line">go version go1.10 linux/amd64</span><br></pre></td></tr></table></figure><p>go环境部署完毕。</p><h2 id="下载编译好的二进制版本"><a href="#下载编译好的二进制版本" class="headerlink" title="下载编译好的二进制版本"></a>下载编译好的二进制版本</h2><p><a href="https://github.com/open-falcon/falcon-plus/releases/download/v0.2.1/open-falcon-v0.2.1.tar.gz" target="_blank" rel="noopener">官方编译好的v0.2.1版本</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ mkdir -p ~/open-falcon</span><br><span class="line">$ <span class="built_in">cd</span> ~/open-falcon</span><br><span class="line">$ wget https://github.com/open-falcon/falcon-plus/releases/download/v0.2.1/open-falcon-v0.2.1.tar.gz</span><br><span class="line">$ tar -xzvf open-falcon-v0.2.1.tar.gz</span><br></pre></td></tr></table></figure><h2 id="首先确认配置文件中数据库账号密码与实际相同，否则需要修改配置文件"><a href="#首先确认配置文件中数据库账号密码与实际相同，否则需要修改配置文件" class="headerlink" title="首先确认配置文件中数据库账号密码与实际相同，否则需要修改配置文件"></a>首先确认配置文件中数据库账号密码与实际相同，否则需要修改配置文件</h2><h3 id="修改配置文件cfg-json"><a href="#修改配置文件cfg-json" class="headerlink" title="修改配置文件cfg.json"></a>修改配置文件cfg.json</h3><p>猜测部分模块依赖连接数据库，因为如果不修改配置文件，aggregator模块会出现无法启动，graph、hbs、nodata、api、alarm模块会出现开启不报错但是状态为开启失败的情况。</p><p>如果需要每个模块都能正常启动，需要将上面模块的cfg.json的数据库信息进行修改。根据本教程的配置，需要修改配置文件所在的目录： </p><table><thead><tr><th>模块</th><th style="text-align:center">配置文件所在路径</th></tr></thead><tbody><tr><td>aggregator</td><td style="text-align:center">~/open-falcon/aggregator/config/cfg.json</td></tr><tr><td>graph</td><td style="text-align:center">~/open-falcon/graph/config/cfg.json</td></tr><tr><td>hbs</td><td style="text-align:center">~/open-falcon/hbs/config/cfg.json</td></tr><tr><td>nodata</td><td style="text-align:center">~/open-falcon/nodata/config/cfg.json</td></tr><tr><td>api</td><td style="text-align:center">~/open-falcon/api/config/cfg.json</td></tr><tr><td>alarm</td><td style="text-align:center">~/open-falcon/alarm/config/cfg.json</td></tr></tbody></table><p><strong>1）修改aggregator的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/aggregator/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-aggregator-cfg.png" alt="open-falcon-aggregator-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。</p><p><strong>2）修改graph的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/graph/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-graph-cfg.png" alt="open-falcon-graph-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。</p><p><strong>3）修改hbs的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/hbs/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-hbs-cfg.png" alt="open-falcon-hbs-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。</p><p><strong>4）修改nodata的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/nodata/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-nodata-cfg.png" alt="open-falcon-nodata-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。</p><p><strong>5）修改api的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/api/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-api-cfg.png" alt="open-falcon-api-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。</p><p><strong>6）修改alarm的配置文件</strong></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ vim ~/open-falcon/alarm/config/cfg.json</span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-alarm-cfg.png" alt="open-falcon-alarm-cfg"></p><p>mysql的root密码为空，则去掉“password”，若不为空，则用root密码替换“password”。 </p><h2 id="启动后端"><a href="#启动后端" class="headerlink" title="启动后端"></a>启动后端</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">$ <span class="built_in">cd</span> ~/open-falcon</span><br><span class="line">$ ./open-falcon start</span><br><span class="line">[falcon-graph] 25103</span><br><span class="line">[falcon-hbs] 25113</span><br><span class="line">[falcon-judge] 25122</span><br><span class="line">[falcon-transfer] 25129</span><br><span class="line">[falcon-nodata] 25136</span><br><span class="line">[falcon-aggregator] 25142</span><br><span class="line">[falcon-agent] 25151</span><br><span class="line">[falcon-gateway] 25158</span><br><span class="line">[falcon-api] 25165</span><br><span class="line">[falcon-alarm] 25174</span><br><span class="line"></span><br><span class="line"><span class="comment">## 检查任务情况</span></span><br><span class="line">$ ./open-falcon check</span><br><span class="line">        falcon-graph         UP           25103 </span><br><span class="line">          falcon-hbs         UP           25113 </span><br><span class="line">        falcon-judge         UP           25122 </span><br><span class="line">     falcon-transfer         UP           25129 </span><br><span class="line">       falcon-nodata         UP           25136 </span><br><span class="line">   falcon-aggregator         UP           25142 </span><br><span class="line">        falcon-agent         UP           25151 </span><br><span class="line">      falcon-gateway         UP           25158 </span><br><span class="line">          falcon-api         UP           25165 </span><br><span class="line">        falcon-alarm         UP           25174 </span><br><span class="line"></span><br><span class="line"><span class="comment">## 更多工具用法</span></span><br><span class="line"><span class="comment"># ./open-falcon [start|stop|restart|check|monitor|reload] module</span></span><br><span class="line"><span class="comment"># For debugging , You can check ~/open-falcon/$moduleName/log/logs/xxx.log</span></span><br></pre></td></tr></table></figure><h2 id="部署前端"><a href="#部署前端" class="headerlink" title="部署前端"></a>部署前端</h2><h3 id="获取前端代码"><a href="#获取前端代码" class="headerlink" title="获取前端代码"></a>获取前端代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 先安装依赖</span></span><br><span class="line">$ yum install -y python-virtualenv</span><br><span class="line">$ yum install -y python-devel</span><br><span class="line">$ yum install -y openldap-devel</span><br><span class="line">$ yum install -y mysql-devel</span><br><span class="line">$ yum install -y git</span><br><span class="line">$ yum groupinstall <span class="string">"Development tools"</span> -y</span><br><span class="line"></span><br><span class="line"><span class="comment">## 下载前端源码</span></span><br><span class="line">$ <span class="built_in">cd</span> ~/open-falcon</span><br><span class="line">$ git <span class="built_in">clone</span> https://github.com/open-falcon/dashboard.git</span><br><span class="line"></span><br><span class="line"><span class="comment">## 进入前端文件夹用pip安装对应依赖</span></span><br><span class="line">$ <span class="built_in">cd</span> ./dashboard</span><br><span class="line">$ virtualenv ./env</span><br><span class="line">$ ./env/bin/pip install -r pip_requirements.txt</span><br></pre></td></tr></table></figure><h3 id="修改配置"><a href="#修改配置" class="headerlink" title="修改配置"></a>修改配置</h3><p>根据本次记录的配置，dashboard的配置文件在<code>~/open-falcon/dashboard/rrd/config.py</code>，需要根据实际情况对内部配置进行修改。</p><p>按照上述的数据库账号密码进行修改</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## API_ADDR 表示后端api组件的地址</span></span><br><span class="line">API_ADDR = <span class="string">"http://127.0.0.1:8080/api/v1"</span> </span><br><span class="line"></span><br><span class="line"><span class="comment">## 根据实际情况，修改PORTAL_DB_*, 默认用户名为root，默认密码为""</span></span><br><span class="line"><span class="comment">## 根据实际情况，修改ALARM_DB_*, 默认用户名为root，默认密码为""</span></span><br></pre></td></tr></table></figure><p><img src="/images/open-falcon-dashboard-cfg.png" alt="open-falcon-dashboard-cfg"></p><p>设置对应的IP地址，提供访问入口，并配置数据库对应账号密码</p><h2 id="开启8081端口"><a href="#开启8081端口" class="headerlink" title="开启8081端口"></a>开启8081端口</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CentOS 7</span></span><br><span class="line">$ firewall-cmd --add-port=8081/tcp --permanent</span><br><span class="line">$ firewall-cmd --reload</span><br><span class="line"></span><br><span class="line"><span class="comment"># CentOS 6</span></span><br><span class="line">$ iptables -I INPUT -p tcp --dport 8081 -j ACCEPT</span><br></pre></td></tr></table></figure><h3 id="以开发者模式启动"><a href="#以开发者模式启动" class="headerlink" title="以开发者模式启动"></a>以开发者模式启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ ./env/bin/python wsgi.py</span><br><span class="line"></span><br><span class="line"><span class="comment">## open http://127.0.0.1:8081 in your browser.</span></span><br></pre></td></tr></table></figure><h3 id="在生产环境启动"><a href="#在生产环境启动" class="headerlink" title="在生产环境启动"></a>在生产环境启动</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">$ bash control start</span><br><span class="line"></span><br><span class="line"><span class="comment">## open http://127.0.0.1:8081 in your browser.</span></span><br></pre></td></tr></table></figure><h3 id="停止dashboard运行"><a href="#停止dashboard运行" class="headerlink" title="停止dashboard运行"></a>停止dashboard运行</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bash control stop</span><br></pre></td></tr></table></figure><h3 id="查看日志"><a href="#查看日志" class="headerlink" title="查看日志"></a>查看日志</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bash control tail</span><br></pre></td></tr></table></figure><h2 id="dashbord用户管理"><a href="#dashbord用户管理" class="headerlink" title="dashbord用户管理"></a>dashbord用户管理</h2><p>dashbord没有默认创建任何账号包括管理账号，需要你通过页面进行注册账号。<br>想拥有管理全局的超级管理员账号，需要手动注册用户名为root的账号（第一个帐号名称为root的用户会被自动设置为超级管理员）。<br>超级管理员可以给普通用户分配权限管理。</p><p><strong>小提示：</strong><br>注册账号能够被任何打开dashboard页面的人注册，所以当给相关的人注册完账号后，需要去关闭注册账号功能。只需要去修改api组件的配置文件cfg.json，将signup_disable配置项修改为true，重启api即可。当需要给人开账号的时候，再将配置选项改回去，用完再关掉即可。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是open-falcon安装部署教程&lt;/p&gt;
&lt;h2 id=&quot;环境准备&quot;&gt;&lt;a href=&quot;#环境准备&quot; class=&quot;headerlink&quot; title=&quot;环境准备&quot;&gt;&lt;/a&gt;环境准备&lt;/h2&gt;&lt;ul&gt;
&lt;li&gt;系统：CentOS 7 最小安装版&lt;/li&gt;
&lt;
      
    
    </summary>
    
      <category term="open-falcon安装部署教程" scheme="http://yoursite.com/categories/open-falcon%E5%AE%89%E8%A3%85%E9%83%A8%E7%BD%B2%E6%95%99%E7%A8%8B/"/>
    
    
      <category term="centos" scheme="http://yoursite.com/tags/centos/"/>
    
      <category term="open-falcon" scheme="http://yoursite.com/tags/open-falcon/"/>
    
  </entry>
  
  <entry>
    <title>supervisor 安装、配置、常用命令</title>
    <link href="http://yoursite.com/2018/03/08/supervisor/"/>
    <id>http://yoursite.com/2018/03/08/supervisor/</id>
    <published>2018-03-07T16:00:00.000Z</published>
    <updated>2018-03-15T08:55:15.972Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是supervisor 安装、配置、常用命令</p><h2 id="简述"><a href="#简述" class="headerlink" title="简述"></a>简述</h2><p>Supervisor是用Python写的一款应用程监控管理工具，能够启动，停止，重启死进程，提供web管理界面，XML-RPC接口及事件监听。通常我们写了一些脚本都不会带有daemon功能，而是加&amp;或者nohub，screen什么的丢到后台去运行，同时使用corntab定时检测脚本是否存活，以便重新运行脚本。使用Supervisor可以将这些脚本，程序转为守护进程，自动重启它们；还可以监控机器的进程运行状况，输出警报等。</p><p>Supervisor只能运行于Python 2.x的环境，但子进程可以为其他任意程序，比如Python 3，PHP等。这里使用pip来安装</p><h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><ul><li><p>CentOS系统下：<code>$ yum install supervisor</code>，通过这种方式安装后，自动设置为开机启动</p></li><li><p>也可以通过<code>pip install supervisor</code>进行安装，但是需要手动启动，然后手动设置为开机启动</p></li></ul><h2 id="Supervisor-配置"><a href="#Supervisor-配置" class="headerlink" title="Supervisor 配置"></a>Supervisor 配置</h2><p>Supervisor 是一个 C/S 模型的程序，<code>supervisord</code>是<code>server</code>端，<code>supervisorctl</code>是<code>client</code>端。</p><h3 id="supervisord"><a href="#supervisord" class="headerlink" title="supervisord"></a>supervisord</h3><p>下面介绍 supervisord 配置方法。supervisord 的配置文件默认位于<code>/etc/supervisord.conf</code>，内容如下（;后面为注释）：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">; supervisor config file</span><br><span class="line"></span><br><span class="line">[unix_http_server]</span><br><span class="line">file=/var/run/supervisor.sock   ; (the path to the socket file) UNIX socket 文件，supervisorctl 会使用</span><br><span class="line">chmod=0700                       ; sockef file mode (default 0700) socket 文件的 mode，默认是 0700</span><br><span class="line"></span><br><span class="line">[supervisord]</span><br><span class="line">logfile=/var/<span class="built_in">log</span>/supervisor/supervisord.log ; (main <span class="built_in">log</span> file;default <span class="variable">$CWD</span>/supervisord.log) 日志文件，默认是 <span class="variable">$CWD</span>/supervisord.log</span><br><span class="line">pidfile=/var/run/supervisord.pid ; (supervisord pidfile;default supervisord.pid) pid 文件</span><br><span class="line">childlogdir=/var/<span class="built_in">log</span>/supervisor            ; (<span class="string">'AUTO'</span> child <span class="built_in">log</span> dir, default <span class="variable">$TEMP</span>)</span><br><span class="line"></span><br><span class="line">; the below section must remain <span class="keyword">in</span> the config file <span class="keyword">for</span> RPC</span><br><span class="line">; (supervisorctl/web interface) to work, additional interfaces may be</span><br><span class="line">; added by defining them <span class="keyword">in</span> separate rpcinterface: sections</span><br><span class="line">[rpcinterface:supervisor]</span><br><span class="line">supervisor.rpcinterface_factory = supervisor.rpcinterface:make_main_rpcinterface</span><br><span class="line"></span><br><span class="line">[supervisorctl]</span><br><span class="line">serverurl=unix:///var/run/supervisor.sock ; use a unix:// URL  <span class="keyword">for</span> a unix socket 通过 UNIX socket 连接 supervisord，路径与 unix_http_server 部分的 file 一致</span><br><span class="line"></span><br><span class="line">; 在增添需要管理的进程的配置文件时，推荐写到 `/etc/supervisor/conf.d/` 目录下，所以 `include` 项，就需要像如下配置。</span><br><span class="line">; 包含其他的配置文件</span><br><span class="line">[include]</span><br><span class="line">files = /etc/supervisor/conf.d/*.conf ; 引入 `/etc/supervisor/conf.d/` 下的 `.conf` 文件</span><br></pre></td></tr></table></figure><h3 id="program-配置"><a href="#program-配置" class="headerlink" title="program 配置"></a>program 配置</h3><p>program 的配置文件就写在，<code>supervisord</code> 配置中<code>include</code>项的路径下：<code>/etc/supervisor/conf.d/</code>，然后<code>program</code>的配置文件命名规则推荐：<code>app_name.conf</code></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">[program:app] ; 程序名称，在 supervisorctl 中通过这个值来对程序进行一系列的操作</span><br><span class="line">autorestart=True      ; 程序异常退出后自动重启</span><br><span class="line">autostart=True        ; 在 supervisord 启动的时候也自动启动</span><br><span class="line">redirect_stderr=True  ; 把 stderr 重定向到 stdout，默认 <span class="literal">false</span></span><br><span class="line">environment=PATH=<span class="string">"/home/app_env/bin"</span>  ; 可以通过 environment 来添加需要的环境变量，一种常见的用法是使用指定的 virtualenv 环境</span><br><span class="line"><span class="built_in">command</span>=python server.py  ; 启动命令，与手动在命令行启动的命令是一样的</span><br><span class="line">user=ubuntu           ; 用哪个用户启动</span><br><span class="line">directory=/home/app/  ; 程序的启动目录</span><br></pre></td></tr></table></figure><p><strong>需要注意：</strong></p><ul><li><p>用 supervisord 管理时，gunicorn 的 daemon 选项需要设置为 False</p></li><li><p>如果启动命令需要包含<code>workon</code>，修改environment参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">environment=PATH="/home/username/.virtualenvs/myproject/bin"</span><br></pre></td></tr></table></figure></li></ul><h2 id="supervisorctl-操作"><a href="#supervisorctl-操作" class="headerlink" title="supervisorctl 操作"></a>supervisorctl 操作</h2><p>supervisorctl 是 supervisord 的命令行客户端工具，使用的配置和 supervisord 一样，这里就不再说了。下面，主要介绍 supervisorctl 操作的常用命令：</p><p>输入命令 <code>supervisorctl</code> 进入 supervisorctl 的 shell 交互界面（还是纯命令行），就可以在下面输入命令了：</p><ul><li>help # 查看帮助</li><li>status # 查看程序状态</li><li>stop program_name # 关闭 指定的程序</li><li>start program_name # 启动 指定的程序</li><li>restart program_name # 重启 指定的程序</li><li>tail -f program_name # 查看 该程序的日志</li><li>update # 重启配置文件修改过的程序（修改了配置，通过这个命令加载新的配置)</li></ul><p>也可以直接通过 shell 命令操作：</p><ul><li>supervisorctl status</li><li>supervisorctl update<br>…</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是supervisor 安装、配置、常用命令&lt;/p&gt;
&lt;h2 id=&quot;简述&quot;&gt;&lt;a href=&quot;#简述&quot; class=&quot;headerlink&quot; title=&quot;简述&quot;&gt;&lt;/a&gt;简述&lt;/h2&gt;&lt;p&gt;Supervisor是用Python写的一款应用程监控管理工具，能够
      
    
    </summary>
    
      <category term="supervisor 安装、配置、常用命令" scheme="http://yoursite.com/categories/supervisor-%E5%AE%89%E8%A3%85%E3%80%81%E9%85%8D%E7%BD%AE%E3%80%81%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
    
    
      <category term="supervisor" scheme="http://yoursite.com/tags/supervisor/"/>
    
  </entry>
  
  <entry>
    <title>比特币如何挖矿（挖矿原理）-工作量证明</title>
    <link href="http://yoursite.com/2018/03/03/blockchain-pow/"/>
    <id>http://yoursite.com/2018/03/03/blockchain-pow/</id>
    <published>2018-03-02T16:00:00.000Z</published>
    <updated>2018-03-07T08:34:30.193Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是比特币如何挖矿（挖矿原理）-工作量证明</p><p>在<a href="https://polaris0112.github.io/2018/03/02/blockchain-accounting/" target="_blank" rel="noopener">区块链记账原理</a>一篇，我们了解到记账是把交易记录、交易时间、账本序号、上一个Hash值等信息计算Hash打包的过程。<br>我们知道所有的计算和存贮是需要消耗计算机资源的，既然要付出成本，那节点为什么还要参与记账呢？在中本聪（比特币之父）的设计里，完成记账的节点可以获得系统给与的一定数量的比特币奖励，这个奖励的过程也就是比特币的发行过程，因此大家形象的把记账称为“挖矿”，本文将详细讨论这个过程。</p><h2 id="记账工作"><a href="#记账工作" class="headerlink" title="记账工作"></a>记账工作</h2><p>由于记账是有奖励的，每次记账都可以给自己凭空增加一定数量的个比特币（当前是12.5比特币，博文写作时每个比特币是4万人民币以上，大家可以算算多少钱），因此就出现大家争相记账，大家一起记账就会引起问题：出现记账不一致的问题，比特币系统引入工作量证明来解决这个问题，规则如下：</p><ul><li>一段时间内（10分钟左右，具体时间会与密码学难题难度相互影响）只有一人可以记账成功</li><li>通过解决密码学难题（即工作量证明）竞争获得唯一记账权</li><li>其他节点复制记账结果</li></ul><p>不过在进行工作量证明之前，记账节点会做进行如下准备工作：</p><ul><li>收集广播中还没有被记录账本的原始交易信息</li><li>检查每个交易信息中付款地址有没有足够的余额</li><li>验证交易是否有正确的签名</li><li>把验证通过的交易信息进行打包记录</li><li>添加一个奖励交易：给自己的地址增加12.5比特币</li></ul><p>如果节点争夺记账权成功的话，就可以得到12.5比特币的奖励。</p><h2 id="工作量证明"><a href="#工作量证明" class="headerlink" title="工作量证明"></a>工作量证明</h2><p><a href="https://polaris0112.github.io/2018/03/02/blockchain-accounting/" target="_blank" rel="noopener">区块链记账原理</a>我们了解到，每次记账的时候会把上一个块的Hash值和当前的账页信息一起作为原始信息进行Hash。<br>如果仅仅是这样，显然每个人都可以很轻松的完成记账。<br>为了保证10分钟左右只有一个人可以记账，就必须要提高记账的难度，使得Hash的结果必须以若干个0开头。同是为了满足这个条件，在进行Hash时引入一个随机数变量。</p><p>用伪代码表示一下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 没有难度时为：Hash(上一个Hash值，交易记录集) = 456635BCD</span></span><br><span class="line">Hash(上一个Hash值，交易记录集，随机数) = 0000aFD635BCD</span><br></pre></td></tr></table></figure><p>我们知道改变Hash的原始信息的任何一部分，Hash值也会随之不断的变化，因此在运算Hash时，不断的改变随机数的值，总可以找的一个随机数使的Hash的结果以若干个0开头（下文把这个过程称为猜谜），率先找到随机数的节点就获得此次记账的唯一记账权。</p><h3 id="计算量分析"><a href="#计算量分析" class="headerlink" title="计算量分析"></a>计算量分析</h3><p>（这部分可选阅读）我们简单分析下记账难度有多大，<br>Hash值是由数字和大小写字母构成的字符串，每一位有62种可能性（可能为26个大写字母、26个小写字母，10个数字中任一个），假设任何一个字符出现的概率是均等的，那么第一位为0的概率是1/62（其他位出现什么字符先不管），理论上需要尝试62次Hash运算才会出现一次第一位为0的情况，如果前两2位为0，就得尝试62的平方次Hash运算，以n个0开头就需要尝试62的n次方次运算。我们结合当前实际区块#493050信息来看看：</p><p><img src="/images/blockchain-6.jpg" alt="blockchain-6"></p><p>注：<a href="https://blockchain.info" target="_blank" rel="noopener">数据来源</a><br>我们可以看到Hash值以18个0开头，理论上需要尝试62的18次方次，这个数是非常非常巨大的，我已经算不清楚了，应该是亿亿级别以上了。如此大的计算量需要投入大量的计算设备、电力等，<br>目前应该没有单矿工独立参与挖矿了，基本都是由矿工联合起来组成矿池进行挖矿（矿池里的矿工按算力百分比来分收益）。</p><p>从经济的角度讲，只有挖矿还有收益（比特币价格不断上涨也让收益变大），就会有新的矿工加入，从而加剧竞争，提高算力难度，挖矿就需要耗费更多的运算和电力，相互作用引起最终成本会接近收益。</p><p>题外话：国内由于电力成本较低，相对收益更高，中国的算力占整个网络的一半以上</p><h2 id="验证"><a href="#验证" class="headerlink" title="验证"></a>验证</h2><p>在节点成功找到满足的Hash值之后，会马上对全网进行广播打包区块，网络的节点收到广播打包区块，会立刻对其进行验证。</p><p>如果验证通过，则表明已经有节点成功解迷，自己就不再竞争当前区块打包，而是选择接受这个区块，记录到自己的账本中，然后进行下一个区块的竞争猜谜。<br>网络中只有最快解谜的区块，才会添加的账本中，其他的节点进行复制，这样就保证了整个账本的唯一性。</p><p>假如节点有任何的作弊行为，都会导致网络的节点验证不通过，直接丢弃其打包的区块，这个区块就无法记录到总账本中，作弊的节点耗费的成本就白费了，因此在巨大的挖矿成本下，也使得矿工自觉自愿的遵守比特币系统的共识协议，也就确保了整个系统的安全。</p><p>进阶阅读<a href="https://polaris0112.github.io/2018/03/03/blockchain-merkle/" target="_blank" rel="noopener">比特币区块结构Merkle树及简单支付验证分析</a>，可以详细了解区块结构如何验证交易。</p><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><p>矿工的收益其实不仅仅包含新发行的12.5比特币奖励，同时还有交易费收益（本文忽略一些细节是为了让主干更清晰）。</p><p>有兴趣的同学可以看看图中区块都包含了那些信息，红箭头标示出的是本文涉及的信息。</p><p>本文中有提到共识协议，比特币共识协议主要是由工作量证明和最长链机制 两部分组成，请阅读<a href="https://polaris0112.github.io/2018/03/03/blockchain-selection/" target="_blank" rel="noopener">比特币如何达成共识 - 最长链的选择</a>。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是比特币如何挖矿（挖矿原理）-工作量证明&lt;/p&gt;
&lt;p&gt;在&lt;a href=&quot;https://polaris0112.github.io/2018/03/02/blockchain-accounting/&quot; target=&quot;_blank&quot; rel=&quot;noopene
      
    
    </summary>
    
      <category term="比特币如何挖矿（挖矿原理）-工作量证明" scheme="http://yoursite.com/categories/%E6%AF%94%E7%89%B9%E5%B8%81%E5%A6%82%E4%BD%95%E6%8C%96%E7%9F%BF%EF%BC%88%E6%8C%96%E7%9F%BF%E5%8E%9F%E7%90%86%EF%BC%89-%E5%B7%A5%E4%BD%9C%E9%87%8F%E8%AF%81%E6%98%8E/"/>
    
    
      <category term="Blockchain" scheme="http://yoursite.com/tags/Blockchain/"/>
    
  </entry>
  
  <entry>
    <title>比特币如何达成共识 - 最长链的选择</title>
    <link href="http://yoursite.com/2018/03/03/blockchain-selection/"/>
    <id>http://yoursite.com/2018/03/03/blockchain-selection/</id>
    <published>2018-03-02T16:00:00.000Z</published>
    <updated>2018-03-07T08:36:25.056Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是比特币如何达成共识 - 最长链的选择</p><p>比特币没有中心机构，几乎所有的完整节点都有一份公共总帐本，那么大家如何达成共识：确认哪一份才是公认权威的总账本呢？</p><h2 id="为什么要遵守协议"><a href="#为什么要遵守协议" class="headerlink" title="为什么要遵守协议"></a>为什么要遵守协议</h2><p>这其实是一个经济问题，在经济活动中的每个人都是自私自利的，追求的是利益的最大化，一个节点工作量只有在其他的节点认同其是有效的（打包的新区块，其他的节点只有验证通过才会加入到区块链中，并在网络上传播），才能够过得收益，<br>而只有遵守规则才会得到其他的节点认同。<br>因此，基于逐利，节点就会自发的遵守协议。共识就是数以万计的独立节点遵守了简单的规则（通过异步交互）自发形成的。</p><blockquote><p>共识：共同遵守的协议规范</p></blockquote><h2 id="去中心化共识"><a href="#去中心化共识" class="headerlink" title="去中心化共识"></a>去中心化共识</h2><p>在<a href="https://polaris0112.github.io/2018/03/03/blockchain-pow/" target="_blank" rel="noopener">工作量证明</a>一篇，我们了解通过工作量证明来竞争记账，权威的总帐本是怎么达到共识的，没有完全说清楚，今天补上，<br>实际上，比特币的共识由所有节点的4个独立过程相互作用而产生：</p><ul><li>每个节点（挖矿节点）依据标准对每个交易进行独立验证</li><li>挖矿节点通过完成工作量证明，将交易记录独立打包进新区块</li><li>每个节点独立的对新区块进行校验并组装进区块链</li><li>每个节点对区块链进行独立选择，在工作量证明机制下选择累计工作量最大的区块链</li></ul><p>共识最终目的是保证比特币不停的在工作量最大的区块链上运转，工作量最大的区块链就是权威的公共总帐本。</p><h2 id="最长链的选择"><a href="#最长链的选择" class="headerlink" title="最长链的选择"></a>最长链的选择</h2><p>先来一个定义，把累计了最多难度的区块链。在一般情况下，也是包含最多区块的那个链称为<strong>主链</strong><br>每一个（挖矿）节点总是选择并尝试延长主链。</p><h2 id="分叉"><a href="#分叉" class="headerlink" title="分叉"></a>分叉</h2><p>当有两名矿工在几乎在相同的时间内，各自都算得了工作量证明解，便立即传播自己的“获胜”区块到网络中，先是传播给邻近的节点而后传播到整个网络。每个收到有效区块的节点都会将其并入并延长区块链。<br>当这个两个区块传播时，一些节点首先收到#3458A, 一些节点首先收到#3458B，这两个候选区块（通常这两个候选区块会包含几乎相同的交易）都是主链的延伸，分叉就会产生，这时分叉出有竞争关系的两条链，如图：</p><p><img src="/images/blockchain-7.jpg" alt="blockchain-7"></p><p>两个块都收到的节点，会把其中有更多工作量的一条会继续作为主链，另一条作为备用链保存（保存是因为备用链将来可能会超过主链难度称为新主链）。</p><h2 id="分叉解决"><a href="#分叉解决" class="headerlink" title="分叉解决"></a>分叉解决</h2><p>收到#3458A的（挖矿）节点，会立刻以这个区块为父区块来产生新的候选区块，并尝试寻找这个候选区块的工作量证明解。同样地，接受#3458B区块的节点会以这个区块为链的顶点开始生成新块，延长这个链（下面称为B链）。<br>这时总会有一方抢先发现工作量证明解并将其传播出去，假设以#3458B为父区块的工作量证明首先解出，如图：</p><p><img src="/images/blockchain-8.jpg" alt="blockchain-8"></p><p>当原本以#3458A为父区块求解的节点在收到#3458B, #3459B之后，会立刻将B链作为主链（因为#3458A为顶点的链已经不是最长链了）继续挖矿。</p><blockquote><p>节点也有可能先收到#3459B，再收到#3458B，收到#3459B时，会被认为是“孤块“（因为还找不到#3459B的父块#3458B）保存在孤块池中，一旦收到父块#3458B时，节点就会将孤块从孤块池中取出，并且连接到它的父区块，让它作为区块链的一部分。</p></blockquote><p>比特币将区块间隔设计为10分钟，是在更快速的交易确认和更低的分叉概率间作出的妥协。更短的区块产生间隔会让交易确认更快地完成，也会导致更加频繁地区块链分叉。与之相对地，长的间隔会减少分叉数量，却会导致更长的确认时间。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是比特币如何达成共识 - 最长链的选择&lt;/p&gt;
&lt;p&gt;比特币没有中心机构，几乎所有的完整节点都有一份公共总帐本，那么大家如何达成共识：确认哪一份才是公认权威的总账本呢？&lt;/p&gt;
&lt;h2 id=&quot;为什么要遵守协议&quot;&gt;&lt;a href=&quot;#为什么要遵守协议&quot; clas
      
    
    </summary>
    
      <category term="比特币如何达成共识 - 最长链的选择" scheme="http://yoursite.com/categories/%E6%AF%94%E7%89%B9%E5%B8%81%E5%A6%82%E4%BD%95%E8%BE%BE%E6%88%90%E5%85%B1%E8%AF%86-%E6%9C%80%E9%95%BF%E9%93%BE%E7%9A%84%E9%80%89%E6%8B%A9/"/>
    
    
      <category term="Blockchain" scheme="http://yoursite.com/tags/Blockchain/"/>
    
  </entry>
  
  <entry>
    <title>比特币区块结构 Merkle 树及简单支付验证分析</title>
    <link href="http://yoursite.com/2018/03/03/blockchain-merkle/"/>
    <id>http://yoursite.com/2018/03/03/blockchain-merkle/</id>
    <published>2018-03-02T16:00:00.000Z</published>
    <updated>2018-03-07T08:35:51.512Z</updated>
    
    <content type="html"><![CDATA[<p>本帖子记录的是比特币区块结构 Merkle 树及简单支付验证分析</p><p>在比特币网络中，不是每个节点都有能力储存完整的区块链数据，受限于存储空间的的限制，很多节点是以SPV（Simplified Payment Verification简单支付验证）钱包接入比特币网络，通过简单支付验证可以在不必存储完整区块链下对交易进行验证，本文将分析区块结构Merkle树及如何进行交易验证。</p><h2 id="区块结构"><a href="#区块结构" class="headerlink" title="区块结构"></a>区块结构</h2><p>在<a href="https://polaris0112.github.io/2018/03/03/blockchain-pow/" target="_blank" rel="noopener">工作量证明</a>中出现过一个区块信息截图：</p><p><img src="/images/blockchain-6.jpg" alt="blockchain-6"></p><p>细心的同学一定已经在里面发现了很多未讲的其他信息，如：时间戳，版本号，交易次数，二进制哈希树根(Merkle根)等。</p><p>我们来看看一个区块结构到底是怎样的：</p><p><img src="/images/blockchain-9.jpg" alt="blockchain-9"></p><p>如上图（下文称：区块结构图）所示：每个数据区块包含区块头和区块体。<br>区块头封装了当前版本号、前一区块哈希值、当前区块PoW要求的随机数(Nonce)、时间戳、以及Merkle根信息。<br>区块体则包括当前区块经过验证的、 区块创建过程中生成的所有交易记录。这些记录通过 Merkle树的哈希过程生成唯一的Merkle根并记入区块头.</p><blockquote><p>区块哈希值实际上并不包含在区块的数据结构里，其实区块打包时只有区块头被用于计算哈希（从网络被接收时由每个节点计算出来），常说的区块哈希值实际是区块头哈希值，它可以用来唯一、明确地标识一个区块。</p></blockquote><p>区块头是80字节，而平均每个交易至少是250字节，而且平均每个区块包含2000个交易。因此，包含完整交易的区块比区块头的4千倍还要大。<br>SPV节点只下载区块头，不下载包含在每个区块中的交易信息。这样的不含交易信息的区块链，大小只有完整区块链的几千分之1，那SPV节点是如何验证交易的呢？</p><h2 id="哈希验证"><a href="#哈希验证" class="headerlink" title="哈希验证"></a>哈希验证</h2><p>上面先留一个引子，先来回顾下哈希函数，<a href="https://polaris0112.github.io/2018/03/02/blockchain-accounting/" target="_blank" rel="noopener">记账原理</a>我们知道原始信息任何微小的变化都会哈希完全不同的哈希值。</p><h2 id="简单文件验证"><a href="#简单文件验证" class="headerlink" title="简单文件验证"></a>简单文件验证</h2><p>我们通常用哈希来检验下载的文件是否完整，我经常看到这样的下载页面：</p><p><img src="/images/blockchain-10.jpg" alt="blockchain-10"></p><p>可以看到下载链接后面提供了一个MD5（MD5也是一种Hash算法），这样我们可以在下载之后对文件计算MD5，如果MD5与提供的MD5相等，说明文件有没有被损坏，这个验证过程相信大家都能理解。</p><h3 id="多点文件验证-哈希列表"><a href="#多点文件验证-哈希列表" class="headerlink" title="多点文件验证(哈希列表)"></a>多点文件验证(哈希列表)</h3><p>现在复杂度提高一点，在P2P网络中下载时，会把大文件切成小文件，同时从多个机器上下载数据，这个时候怎么验证数据呢？</p><p>以BT下载为例，在下载真正的数据之前，我们会先下载一个哈希列表的（每个下小块计算出一个哈希），如果有一个小块数据在传输过程中损坏了，那我只要重新下载这一个数据块就行了，这时有一个问题就出现了，那么多的哈希，怎么保证它们本身(哈希列表中的哈希值)都是正确地呢？</p><p>答案是把每个小块数据的哈希值拼到一起，然后对这个长字符串在作一次哈希运算，得到哈希列表的根哈希。只要根哈希校对比一样就说明验哈希列表是正确的，再通过哈希列表校验小数据块，如果所有的小数据块验证通过则说明大文件没有被损坏。</p><h3 id="Merkle树"><a href="#Merkle树" class="headerlink" title="Merkle树"></a>Merkle树</h3><p>验证交易的过程和文件验证很相似，可以人为每个交易是一个小数据块，但比特币使用Merkle树的方式进行验证，相对于哈希列表，Merkle树是一种哈希二叉树，它的明显的一个好处是可以单独拿出一个分支来（作为一个小树）对部分数据进行校验，更加高效。</p><p>我们回看下上面的区块结构图，区块体就包含这样一个Merkle树,Merkle树被用来归纳一个区块中的所有交易。</p><p>每个叶子节点是每个交易信息的哈希，往上对相邻的两个哈希合并成字符串再哈希，继续类似的操作直到只剩下顶部的一个节点，即Merkle根，存入区块头。</p><p>因为Merkle树是二叉树，所以它需要偶数个叶子节点。如果仅有奇数个交易需要归纳，那最后的交易就会被复制一份以构成偶数个叶子节点，这种偶数个叶子节点的树也被称为平衡树。</p><h3 id="简化支付验证"><a href="#简化支付验证" class="headerlink" title="简化支付验证"></a>简化支付验证</h3><p>SPV节点不保存所有交易也不会下载整个区块，仅仅保存区块头，我们来看看它是如何对交易数据进行验证的。</p><p>假如要验证区块结构图中交易6，SPV节点会通过向相邻节点索要（通过Merkleblock消息）包括从交易6哈希值沿Merkle树上溯至区块头根哈希处的哈希序列 (即哈希节点6, 5, 56, 78, 5678, 1234 1~8 - 称为认证路径) 来确认交易的存在性和正确性。（在N个交易组成的区块中确认任一交易只需要计算log2(N)个字节的哈希值，非常快速高效）</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本帖子记录的是比特币区块结构 Merkle 树及简单支付验证分析&lt;/p&gt;
&lt;p&gt;在比特币网络中，不是每个节点都有能力储存完整的区块链数据，受限于存储空间的的限制，很多节点是以SPV（Simplified Payment Verification简单支付验证）钱包接入比特币网
      
    
    </summary>
    
      <category term="比特币区块结构 Merkle 树及简单支付验证分析" scheme="http://yoursite.com/categories/%E6%AF%94%E7%89%B9%E5%B8%81%E5%8C%BA%E5%9D%97%E7%BB%93%E6%9E%84-Merkle-%E6%A0%91%E5%8F%8A%E7%AE%80%E5%8D%95%E6%94%AF%E4%BB%98%E9%AA%8C%E8%AF%81%E5%88%86%E6%9E%90/"/>
    
    
      <category term="Blockchain" scheme="http://yoursite.com/tags/Blockchain/"/>
    
  </entry>
  
</feed>
